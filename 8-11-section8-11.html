<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="8.11 Different slopes and different intercepts | Intermediate Statistics with R" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="gpeterson406/Greenwood_Book" />

<meta name="author" content="Mark C Greenwood" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="8.11 Different slopes and different intercepts | Intermediate Statistics with R">

<title>8.11 Different slopes and different intercepts | Intermediate Statistics with R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li class="has-sub"><a href="1-chapter1.html#chapter1"><span class="toc-section-number">1</span> Preface</a><ul>
<li><a href="1-1-section1-1.html#section1-1"><span class="toc-section-number">1.1</span> Overview of methods</a></li>
<li><a href="1-2-section1-2.html#section1-2"><span class="toc-section-number">1.2</span> Getting started in R</a></li>
<li><a href="1-3-section1-3.html#section1-3"><span class="toc-section-number">1.3</span> Basic summary statistics, histograms, and boxplots using R</a></li>
<li><a href="1-4-section1-4.html#section1-4"><span class="toc-section-number">1.4</span> Chapter summary</a></li>
<li><a href="1-5-section1-5.html#section1-5"><span class="toc-section-number">1.5</span> Summary of important R code</a></li>
<li><a href="1-6-section1-6.html#section1-6"><span class="toc-section-number">1.6</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="2-chapter2.html#chapter2"><span class="toc-section-number">2</span> (R)e-Introduction to statistics</a><ul>
<li><a href="2-1-section2-1.html#section2-1"><span class="toc-section-number">2.1</span> Histograms, boxplots, and density curves</a></li>
<li><a href="2-2-section2-2.html#section2-2"><span class="toc-section-number">2.2</span> Pirate-plots</a></li>
<li><a href="2-3-section2-3.html#section2-3"><span class="toc-section-number">2.3</span> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li><a href="2-4-section2-4.html#section2-4"><span class="toc-section-number">2.4</span> Permutation testing for the two sample mean situation</a></li>
<li><a href="2-5-section2-5.html#section2-5"><span class="toc-section-number">2.5</span> Hypothesis testing (general)</a></li>
<li><a href="2-6-section2-6.html#section2-6"><span class="toc-section-number">2.6</span> Connecting randomization (nonparametric) and parametric tests</a></li>
<li><a href="2-7-section2-7.html#section2-7"><span class="toc-section-number">2.7</span> Second example of permutation tests</a></li>
<li><a href="2-8-section2-8.html#section2-8"><span class="toc-section-number">2.8</span> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li><a href="2-9-section2-9.html#section2-9"><span class="toc-section-number">2.9</span> Confidence intervals and bootstrapping</a></li>
<li><a href="2-10-section2-10.html#section2-10"><span class="toc-section-number">2.10</span> Bootstrap confidence intervals for difference in GPAs</a></li>
<li><a href="2-11-section2-11.html#section2-11"><span class="toc-section-number">2.11</span> Chapter summary</a></li>
<li><a href="2-12-section2-12.html#section2-12"><span class="toc-section-number">2.12</span> Summary of important R code</a></li>
<li><a href="2-13-section2-13.html#section2-13"><span class="toc-section-number">2.13</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="3-chapter3.html#chapter3"><span class="toc-section-number">3</span> One-Way ANOVA</a><ul>
<li><a href="3-1-section3-1.html#section3-1"><span class="toc-section-number">3.1</span> Situation</a></li>
<li><a href="3-2-section3-2.html#section3-2"><span class="toc-section-number">3.2</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li><a href="3-3-section3-3.html#section3-3"><span class="toc-section-number">3.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li><a href="3-4-section3-4.html#section3-4"><span class="toc-section-number">3.4</span> ANOVA model diagnostics including QQ-plots</a></li>
<li><a href="3-5-section3-5.html#section3-5"><span class="toc-section-number">3.5</span> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li><a href="3-6-section3-6.html#section3-6"><span class="toc-section-number">3.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li><a href="3-7-section3-7.html#section3-7"><span class="toc-section-number">3.7</span> Pair-wise comparisons for the Overtake data</a></li>
<li><a href="3-8-section3-8.html#section3-8"><span class="toc-section-number">3.8</span> Chapter summary</a></li>
<li><a href="3-9-section3-9.html#section3-9"><span class="toc-section-number">3.9</span> Summary of important R code</a></li>
<li><a href="3-10-section3-10.html#section3-10"><span class="toc-section-number">3.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="4-chapter4.html#chapter4"><span class="toc-section-number">4</span> Two-Way ANOVA</a><ul>
<li><a href="4-1-section4-1.html#section4-1"><span class="toc-section-number">4.1</span> Situation</a></li>
<li><a href="4-2-section4-2.html#section4-2"><span class="toc-section-number">4.2</span> Designing a two-way experiment and visualizing results</a></li>
<li><a href="4-3-section4-3.html#section4-3"><span class="toc-section-number">4.3</span> Two-Way ANOVA models and hypothesis tests</a></li>
<li><a href="4-4-section4-4.html#section4-4"><span class="toc-section-number">4.4</span> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li><a href="4-5-section4-5.html#section4-5"><span class="toc-section-number">4.5</span> Observational study example: The Psychology of Debt</a></li>
<li><a href="4-6-section4-6.html#section4-6"><span class="toc-section-number">4.6</span> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li><a href="4-7-section4-7.html#section4-7"><span class="toc-section-number">4.7</span> Chapter summary</a></li>
<li><a href="4-8-section4-8.html#section4-8"><span class="toc-section-number">4.8</span> Summary of important R code</a></li>
<li><a href="4-9-section4-9.html#section4-9"><span class="toc-section-number">4.9</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="5-chapter5.html#chapter5"><span class="toc-section-number">5</span> Chi-square tests</a><ul>
<li><a href="5-1-section5-1.html#section5-1"><span class="toc-section-number">5.1</span> Situation, contingency tables, and tableplots</a></li>
<li><a href="5-2-section5-2.html#section5-2"><span class="toc-section-number">5.2</span> Homogeneity test hypotheses</a></li>
<li><a href="5-3-section5-3.html#section5-3"><span class="toc-section-number">5.3</span> Independence test hypotheses</a></li>
<li><a href="5-4-section5-4.html#section5-4"><span class="toc-section-number">5.4</span> Models for R by C tables</a></li>
<li><a href="5-5-section5-5.html#section5-5"><span class="toc-section-number">5.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-6-section5-6.html#section5-6"><span class="toc-section-number">5.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-7-section5-7.html#section5-7"><span class="toc-section-number">5.7</span> Examining residuals for the source of differences</a></li>
<li><a href="5-8-section5-8.html#section5-8"><span class="toc-section-number">5.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li><a href="5-9-section5-9.html#section5-9"><span class="toc-section-number">5.9</span> Political party and voting results: Complete analysis</a></li>
<li><a href="5-10-section5-10.html#section5-10"><span class="toc-section-number">5.10</span> Is cheating and lying related in students?</a></li>
<li><a href="5-11-section5-11.html#section5-11"><span class="toc-section-number">5.11</span> Analyzing a stratified random sample of California schools</a></li>
<li><a href="5-12-section5-12.html#section5-12"><span class="toc-section-number">5.12</span> Chapter summary</a></li>
<li><a href="5-13-section5-13.html#section5-13"><span class="toc-section-number">5.13</span> Summary of important R commands</a></li>
<li><a href="5-14-section5-14.html#section5-14"><span class="toc-section-number">5.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="6-chapter6.html#chapter6"><span class="toc-section-number">6</span> Correlation and Simple Linear Regression</a><ul>
<li><a href="6-1-section6-1.html#section6-1"><span class="toc-section-number">6.1</span> Relationships between two quantitative variables</a></li>
<li><a href="6-2-section6-2.html#section6-2"><span class="toc-section-number">6.2</span> Estimating the correlation coefficient</a></li>
<li><a href="6-3-section6-3.html#section6-3"><span class="toc-section-number">6.3</span> Relationships between variables by groups</a></li>
<li><a href="6-4-section6-4.html#section6-4"><span class="toc-section-number">6.4</span> Inference for the correlation coefficient</a></li>
<li><a href="6-5-section6-5.html#section6-5"><span class="toc-section-number">6.5</span> Are tree diameters related to tree heights?</a></li>
<li><a href="6-6-section6-6.html#section6-6"><span class="toc-section-number">6.6</span> Describing relationships with a regression model</a></li>
<li><a href="6-7-section6-7.html#section6-7"><span class="toc-section-number">6.7</span> Least Squares Estimation</a></li>
<li><a href="6-8-section6-8.html#section6-8"><span class="toc-section-number">6.8</span> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li><a href="6-9-section6-9.html#section6-9"><span class="toc-section-number">6.9</span> Outliers: leverage and influence</a></li>
<li><a href="6-10-section6-10.html#section6-10"><span class="toc-section-number">6.10</span> Residual diagnostics – setting the stage for inference</a></li>
<li><a href="6-11-section6-11.html#section6-11"><span class="toc-section-number">6.11</span> Old Faithful discharge and waiting times</a></li>
<li><a href="6-12-section6-12.html#section6-12"><span class="toc-section-number">6.12</span> Chapter summary</a></li>
<li><a href="6-13-section6-13.html#section6-13"><span class="toc-section-number">6.13</span> Summary of important R code</a></li>
<li><a href="6-14-section6-14.html#section6-14"><span class="toc-section-number">6.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="7-chapter7.html#chapter7"><span class="toc-section-number">7</span> Simple linear regression inference</a><ul>
<li><a href="7-1-section7-1.html#section7-1"><span class="toc-section-number">7.1</span> Model</a></li>
<li><a href="7-2-section7-2.html#section7-2"><span class="toc-section-number">7.2</span> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li><a href="7-3-section7-3.html#section7-3"><span class="toc-section-number">7.3</span> Bozeman temperature trend</a></li>
<li><a href="7-4-section7-4.html#section7-4"><span class="toc-section-number">7.4</span> Randomization-based inferences for the slope coefficient</a></li>
<li><a href="7-5-section7-5.html#section7-5"><span class="toc-section-number">7.5</span> Transformations part I: Linearizing relationships</a></li>
<li><a href="7-6-section7-6.html#section7-6"><span class="toc-section-number">7.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li><a href="7-7-section7-7.html#section7-7"><span class="toc-section-number">7.7</span> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li><a href="7-8-section7-8.html#section7-8"><span class="toc-section-number">7.8</span> Chapter summary</a></li>
<li><a href="7-9-section7-9.html#section7-9"><span class="toc-section-number">7.9</span> Summary of important R code</a></li>
<li><a href="7-10-section7-10.html#section7-10"><span class="toc-section-number">7.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="8-chapter8.html#chapter8"><span class="toc-section-number">8</span> Multiple linear regression</a><ul>
<li><a href="8-1-section8-1.html#section8-1"><span class="toc-section-number">8.1</span> Going from SLR to MLR</a></li>
<li><a href="8-2-section8-2.html#section8-2"><span class="toc-section-number">8.2</span> Validity conditions in MLR</a></li>
<li><a href="8-3-section8-3.html#section8-3"><span class="toc-section-number">8.3</span> Interpretation of MLR terms</a></li>
<li><a href="8-4-section8-4.html#section8-4"><span class="toc-section-number">8.4</span> Comparing multiple regression models</a></li>
<li><a href="8-5-section8-5.html#section8-5"><span class="toc-section-number">8.5</span> General recommendations for MLR interpretations and VIFs</a></li>
<li><a href="8-6-section8-6.html#section8-6"><span class="toc-section-number">8.6</span> MLR inference: Parameter inferences using the t-distribution</a></li>
<li><a href="8-7-section8-7.html#section8-7"><span class="toc-section-number">8.7</span> Overall F-test in multiple linear regression</a></li>
<li><a href="8-8-section8-8.html#section8-8"><span class="toc-section-number">8.8</span> Case study: First year college GPA and SATs</a></li>
<li><a href="8-9-section8-9.html#section8-9"><span class="toc-section-number">8.9</span> Different intercepts for different groups: MLR with indicator variables</a></li>
<li><a href="8-10-section8-10.html#section8-10"><span class="toc-section-number">8.10</span> Additive MLR with more than two groups: Headache example</a></li>
<li><a href="8-11-section8-11.html#section8-11"><span class="toc-section-number">8.11</span> Different slopes and different intercepts</a></li>
<li><a href="8-12-section8-12.html#section8-12"><span class="toc-section-number">8.12</span> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li><a href="8-13-section8-13.html#section8-13"><span class="toc-section-number">8.13</span> AICs for model selection</a></li>
<li><a href="8-14-section8-14.html#section8-14"><span class="toc-section-number">8.14</span> Case study: Forced expiratory volume model selection using AICs</a></li>
<li><a href="8-15-section8-15.html#section8-15"><span class="toc-section-number">8.15</span> Chapter summary</a></li>
<li><a href="8-16-section8-16.html#section8-16"><span class="toc-section-number">8.16</span> Summary of important R code</a></li>
<li><a href="8-17-section8-17.html#section8-17"><span class="toc-section-number">8.17</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="9-chapter9.html#chapter9"><span class="toc-section-number">9</span> Case studies</a><ul>
<li><a href="9-1-section9-1.html#section9-1"><span class="toc-section-number">9.1</span> Overview of material covered</a></li>
<li><a href="9-2-section9-2.html#section9-2"><span class="toc-section-number">9.2</span> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li><a href="9-3-section9-3.html#section9-3"><span class="toc-section-number">9.3</span> Ants learn to rely on more informative attributes during decision-making</a></li>
<li><a href="9-4-section9-4.html#section9-4"><span class="toc-section-number">9.4</span> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li><a href="9-5-section9-5.html#section9-5"><span class="toc-section-number">9.5</span> What do didgeridoos really do about sleepiness?</a></li>
<li><a href="9-6-section9-6.html#section9-6"><span class="toc-section-number">9.6</span> General summary</a></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="section8-11" class="section level2">
<h2><span class="header-section-number">8.11</span> Different slopes and different intercepts</h2>
<p>Sometimes researchers are specifically interested in whether the slopes vary
across groups or the regression lines in the scatterplot for the different
groups may not look parallel or it may just be hard to tell visually if there
really is a difference in the slopes.

Unless you are <strong>very sure</strong> that there
is not an interaction between the grouping variable and the quantitative
predictor, you should<a href="#fn131" class="footnote-ref" id="fnref131"><sup>131</sup></a> start by fitting a model containing an
interaction and then see if you can drop it.


It may be the case that you end up
with the simpler additive model from the previous sections, but you don’t want
to assume the same slope across groups unless you are absolutely sure that is
the case.

This should remind you a bit of the discussions of the additive and
interaction models in the Two-way ANOVA material. The models, concerns, and
techniques are very similar, but with the quantitative variable replacing one
of the two categorical variables. As always, the scatterplot is a good first
step to understanding whether we need the extra complexity that these models
require.</p>
<p>A new example provides motivation for the consideration of different
slopes and
intercepts. A study was performed to address whether the relationship between
nonverbal IQs and
reading accuracy differs between dyslexic and non-dyslexic students. Two groups
of students were identified, one group of <em>dyslexic</em> students was identified
first (19 students) and then a group of gender and age similar student matches
were identified (25 students) for a total sample size of <span class="math inline">\(n=44\)</span>, provided in the
<code>dyslexic3</code> data set from the <code>smdata</code> package <span class="citation">(Merkle and Smithson <a href="#ref-R-smdata" role="doc-biblioref">2018</a>)</span>.

This type of study design is an attempt to “balance” the data from the two
groups on some important characteristics to make the comparisons of the groups
as fair as possible.

The researchers attempted to balance the characteristics
of the subjects in the two groups so that if they found different results for
the two groups, they could attribute it to the main difference they used to
create the groups – dyslexia or not. This design, <strong><em>case-control</em></strong> or
<strong>case-comparison</strong> where each subject with a trait is matched to one or more
subjects in the “control” group would hopefully reduce confounding from other
factors and then allow stronger conclusions in situations where it is
impossible to randomly
assign treatments to subjects.

We still would avoid using “causal” language but
this design is about as good as you can get when you are unable to randomly
assign levels to subjects.</p>
<p>Using these data, we can explore the relationship between nonverbal
IQ scores and reading accuracy, with reading accuracy measured as a proportion
correct. The fact that there is an
upper limit to the response variable attained by many students will cause
complications below, but we can still learn something from our attempts to
analyze these data using an MLR model. The scatterplot in
Figure <a href="8-11-section8-11.html#fig:Figure8-25">2.176</a> seems to indicate some clear differences in the
<em>IQ</em> vs <em>reading score</em> relationship between the <em>dys</em>=0 (non-dyslexic) and
<em>dys</em>=1 (dyslexic) students (code below makes these levels more explicit in the data set). Note that the IQ is standardized to have mean 0
and standard deviation of 1 which means that
a 1 unit change in IQ score is a 1 SD change and that the <em>y</em>-intercept (for
<span class="math inline">\(x=0\)</span>) is right in the center of the plot and actually interesting<a href="#fn132" class="footnote-ref" id="fnref132"><sup>132</sup></a>.</p>
<p>(ref:fig8-25) Scatterplot for reading score versus nonverbal IQ by dyslexia
group.</p>
<div class="figure"><span id="fig:Figure8-25"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-25-1.png" alt="(ref:fig8-25)" width="576" />
<p class="caption">
Figure 2.176: (ref:fig8-25)
</p>
</div>
<div class="sourceCode" id="cb792"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb792-1" title="1"><span class="kw">library</span>(smdata)</a>
<a class="sourceLine" id="cb792-2" title="2"><span class="kw">data</span>(<span class="st">&quot;dyslexic3&quot;</span>)</a>
<a class="sourceLine" id="cb792-3" title="3">dyslexic3 &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(dyslexic3)</a>
<a class="sourceLine" id="cb792-4" title="4">dyslexic3<span class="op">$</span>dys &lt;-<span class="st"> </span><span class="kw">factor</span>(dyslexic3<span class="op">$</span>dys)</a>
<a class="sourceLine" id="cb792-5" title="5"><span class="kw">levels</span>(dyslexic3<span class="op">$</span>dys) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;no&quot;</span>, <span class="st">&quot;yes&quot;</span>)</a>
<a class="sourceLine" id="cb792-6" title="6"><span class="kw">scatterplot</span>(score<span class="op">~</span>ziq<span class="op">|</span>dys, <span class="dt">xlab=</span><span class="st">&quot;Standardized nonverbal IQ scores&quot;</span>,</a>
<a class="sourceLine" id="cb792-7" title="7">            <span class="dt">ylab=</span><span class="st">&quot;Reading score&quot;</span>, <span class="dt">data=</span>dyslexic3, <span class="dt">smooth=</span>F,</a>
<a class="sourceLine" id="cb792-8" title="8">            <span class="dt">main=</span><span class="st">&quot;Plot of IQ vs Reading by dyslexia status&quot;</span>, <span class="dt">col=</span><span class="kw">viridis</span>(<span class="dv">7</span>)[<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">6</span>)])</a></code></pre></div>
<p>To allow for both different y-intercepts and slope coefficients on the
quantitative predictor, we need to include a “modification” of the slope
coefficient. This is performed using an <strong><em>interaction</em></strong> between the two
predictor variables where we allow the impacts of one variable
(slopes) to change based on the levels of another variable (grouping variable).

The formula notation is <code>y~x*group</code>, remembering that this also includes the
<strong><em>main effects</em></strong> (the additive variable components) as well as the
interaction coefficients; this is similar to what we discussed in the Two-Way ANOVA interaction model.

We can start with the general model for a two-level categorical variable with an
interaction, which is</p>
<p><span class="math display">\[y_i=\beta_0 + \beta_1x_i +\beta_2I_{\text{CatName},i} +
{\color{red}{\boldsymbol{\beta_3I_{\text{CatName},i}x_i}}}+\varepsilon_i,\]</span></p>
<p>where the new component involves the product of both the indicator and the quantitative
predictor variable. The <span class="math inline">\(\color{red}{\boldsymbol{\beta_3}}\)</span> coefficient will be
found in a row of output with <strong>both</strong> variable names in it (with the indicator
level name) with a colon between them (something like <code>x:grouplevel</code>). As
always, the best way to understand any
model involving indicators is to plug in 0s or 1s for the indicator variable(s)
and simplify the equations. </p>
<ul>
<li><p>For any observation in the baseline group <span class="math inline">\(I_{\text{CatName},i}=0\)</span>, so</p>
<p><span class="math display">\[y_i=\beta_0+\beta_1x_i+\beta_2I_{\text{CatName},i}+
  {\color{red}{\boldsymbol{\beta_3I_{\text{CatName},i}x_i}}}+\varepsilon_i\]</span></p>
<p>simplifies quickly to:</p>
<p><span class="math display">\[y_i=\beta_0+\beta_1x_i+\varepsilon_i\ .\]</span></p>
<ul>
<li>So the baseline group’s model involves the initial intercept and
quantitative slope coefficient.</li>
</ul></li>
<li><p>For any observation in the second category <span class="math inline">\(I_{\text{CatName},i}=1\)</span>, so</p>
<p><span class="math display">\[y_i=\beta_0+\beta_1x_i+\beta_2I_{\text{CatName},i}+
  {\color{red}{\boldsymbol{\beta_3I_{\text{CatName},i}x_i}}}+\varepsilon_i\]</span></p>
<p>is</p>
<p><span class="math display">\[y_i=\beta_0+\beta_1x_i+\beta_2*1+
  {\color{red}{\boldsymbol{\beta_3*1*x_i}}}+\varepsilon_i\]</span></p>
<p>which “simplifies” to</p>
<p><span class="math display">\[y_i = (\beta_0+\beta_2) + (\beta_1+{\color{red}{\boldsymbol{\beta_3}}})x_i
  +\varepsilon_i,\]</span></p>
<p>by combining like terms.</p>
<ul>
<li>For the second category, the model contains a modified y-intercept,
now <span class="math inline">\(\beta_0+\beta_2\)</span>, <strong>and</strong> a modified slope coefficient, now
<span class="math inline">\(\beta_1+\color{red}{\boldsymbol{\beta_3}}\)</span>.</li>
</ul></li>
</ul>
<p>We can make this more concrete by applying this to the dyslexia data with
<code>dys</code> as a categorical variable for dyslexia status of subjects (levels of <em>no</em> and <em>yes</em>) and <code>ziq</code> the standardized IQ. The model is estimated as:</p>
<div class="sourceCode" id="cb793"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb793-1" title="1">dys_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score<span class="op">~</span>ziq<span class="op">*</span>dys, <span class="dt">data=</span>dyslexic3)</a>
<a class="sourceLine" id="cb793-2" title="2"><span class="kw">summary</span>(dys_model)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ ziq * dys, data = dyslexic3)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.26362 -0.04152  0.01682  0.06790  0.17740 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  0.87586    0.02391  36.628  &lt; 2e-16
## ziq          0.05827    0.02535   2.299   0.0268
## dysyes      -0.27951    0.03827  -7.304 7.11e-09
## ziq:dysyes  -0.07285    0.03821  -1.907   0.0638
## 
## Residual standard error: 0.1017 on 40 degrees of freedom
## Multiple R-squared:  0.712,  Adjusted R-squared:  0.6904 
## F-statistic: 32.96 on 3 and 40 DF,  p-value: 6.743e-11</code></pre>
<p>The estimated model can be written as</p>
<p><span class="math display">\[\widehat{\text{Score}}_i=0.876+0.058\cdot\text{ZIQ}_i - 0.280I_{\text{yes},i}
-{\color{red}{\boldsymbol{0.073}}}I_{\text{yes},i}\cdot\text{ZIQ}_i\]</span></p>
<p>and simplified for the two groups as:</p>
<ul>
<li><p>For the baseline (non-dyslexic, <span class="math inline">\(I_{\text{yes},i}=0\)</span>) students:</p>
<p><span class="math display">\[\widehat{\text{Score}}_i=0.876+0.058\cdot\text{ZIQ}_i\ .\]</span></p></li>
<li><p>For the deviation (dyslexic, <span class="math inline">\(I_{\text{yes},i}=1\)</span>) students:</p>
<p><span class="math display">\[\begin{array}{rl}
  \widehat{\text{Score}}_i&amp;=0.876+0.058\cdot\text{ZIQ}_i - 0.280*1-
  0.073*1\cdot\text{ZIQ}_i \\
  &amp;=(0.876- 0.280) + (0.058-0.073)\cdot\text{ZIQ}_i, \\
  \end{array}\]</span></p>
<p>which simplifies finally to:</p>
<p><span class="math display">\[\widehat{\text{Score}}_i=0.596-0.015\cdot\text{ZIQ}_i\ .\]</span></p></li>
<li><p>So the slope switched from 0.058 in the non-dyslexic students to -0.015 in
the dyslexic students. The interpretations of these coefficients are outlined
below:</p>
<ul>
<li><p>For the non-dyslexic students: For a 1 SD increase in verbal IQ score,
we estimate, on average, the reading score to go up by 0.058
“points”.</p></li>
<li><p>For the dyslexic students: For a 1 SD increase in verbal IQ score, we
estimate, on average, the reading score to change by -0.015
“points”.</p></li>
</ul></li>
</ul>
<p>So, an expected pattern of results emerges for the non-dyslexic students.
Those with higher IQs tend to have
higher reading accuracy; this does not mean higher IQ’s cause more accurate
reading because random assignment of IQ is not possible. However, for the
dyslexic students, the relationship is not what one would might expect. It is
slightly negative, showing that higher verbal IQ’s are related to lower reading
accuracy. What we conclude from this is that we should not expect higher IQ’s
to show higher performance on a test like this.</p>
<p>Checking the assumptions is always recommended before getting focused
on the inferences in the model. When interactions are present, you should not use VIFs as they are naturally inflated because the same variable is re-used in multiple parts of the model to create the interaction components. Checking the multicollinearity in the related additive model can be performed to understand shared information in the variables used in interactions. When fitting models with multiple groups, it is possible to see “groups” in the
fitted values (x-axis in Residuals vs Fitted and Scale-Location plots) and that is not a problem – it is a feature of these models.



You
should look for issues in the residuals for each group but the residuals should
overall still be normally distributed and have the same variability everywhere.
It is a bit hard to see issues in Figure <a href="8-11-section8-11.html#fig:Figure8-26">2.177</a> because of the
group differences,
but note the line of residuals for the higher fitted values. This is an
artifact of the upper threshold in the reading accuracy test used. As in the
first year of college GPA, these observations were <strong><em>censored</em></strong> – their
true score was outside the range of values we could observe – and so we did not
really get a measure of how good these students were since a lot of their
abilities were higher than the test could detect and they all binned up at the
same value of getting all the questions correct. The relationship in this group might be even stronger if we could
really observe differences in the highest level readers. We should treat the
results for the non-dyslexic group with caution even though they are clearly
scoring on average higher and have a different slope than the results for the
dyslexic students. The QQ-plot suggests a slightly long left tail but this deviation is not too far from what might happen if we simulated from a normal distribution, so is not clear evidence of a violation of the normality assumption. The influence diagnostics do not suggest any
influential points because no points have Cook’s D over 0.5. </p>

<div class="sourceCode" id="cb795"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb795-1" title="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>), <span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">0</span>))</a>
<a class="sourceLine" id="cb795-2" title="2"><span class="kw">plot</span>(dys_model,</a>
<a class="sourceLine" id="cb795-3" title="3">     <span class="dt">sub.caption=</span><span class="st">&quot;Plot of diagnostics for Dyslexia Interaction model&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a></code></pre></div>
<div class="figure"><span id="fig:Figure8-26"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-26-1.png" alt="Diagnostic plots for interaction model for reading scores." width="960" />
<p class="caption">
Figure 2.177: Diagnostic plots for interaction model for reading scores.
</p>
</div>
<p>For these models, we have relaxed an earlier assumption that data were collected
from only one group. In
fact, we are doing specific research that is focused on questions about the
differences between groups. However, these models still make assumptions that,
within a specific group, the relationships are linear between the predictor and response variables. They also assume that
the variability in the residuals is the same for all observations. Sometimes it
can be difficult to check the assumptions by looking at the overall diagnostic
plots and it may be easier to go back to the original scatterplot or plot the
residuals vs fitted values by group to fully assess the results.

Figure <a href="8-11-section8-11.html#fig:Figure8-27">2.178</a>
shows a scatterplot of the residuals vs the quantitative explanatory variable
by the groups. The variability in the residuals is a bit larger in the
non-dyslexic group, possibly suggesting that variability in the reading test is
higher for higher scoring individuals even though we couldn’t observe all of
that variability because there were so many perfect scores in this group.</p>
<p>(ref:fig8-27) Plot of Residuals vs Fitted from interaction dyslexia data model
with groups indicated.</p>
<div class="sourceCode" id="cb796"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb796-1" title="1"><span class="kw">scatterplot</span>(<span class="kw">residuals</span>(dys_model)<span class="op">~</span><span class="kw">fitted</span>(dys_model)<span class="op">|</span>dys,</a>
<a class="sourceLine" id="cb796-2" title="2">            <span class="dt">data=</span>dyslexic3, <span class="dt">smooth=</span>F, <span class="dt">col=</span><span class="kw">viridis</span>(<span class="dv">7</span>)[<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">6</span>)])</a></code></pre></div>
<div class="figure"><span id="fig:Figure8-27"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-27-1.png" alt="(ref:fig8-27)" width="576" />
<p class="caption">
Figure 2.178: (ref:fig8-27)
</p>
</div>
<p>If we feel comfortable enough with the assumptions to trust the inferences here
(this might be dangerous), then we can consider what some of the model inferences
provide us in this situation. For example, the test for
<span class="math inline">\(H_0: {\color{red}{\boldsymbol{\beta_3}}}=0\)</span> vs
<span class="math inline">\(H_A: {\color{red}{\boldsymbol{\beta_3}}}\ne 0\)</span> provides an interesting comparison.
Under the null hypothesis, the two groups would have the same slope so it
provides an
opportunity to directly consider whether the relationship (via the slope) is
different between the groups in their respective populations. We find
<span class="math inline">\(t=-1.907\)</span> which, if the assumptions are true, follows a <span class="math inline">\(t(40)\)</span>-distribution
under the null hypothesis. This test
statistic has a corresponding p-value of 0.0638. So it provides some evidence against the null hypothesis of no difference in the slopes between the two groups but it isn’t strong evidence against it. There are serious issues (like getting
the wrong idea about directions of relationships) if we ignore a potentially
important
interaction and some statisticians would recommend retaining interactions even
if the evidence is only moderate for its inclusion in the model.

For the
original research question of whether the relationships differ for the two
groups, we only have marginal evidence to support that result. Possibly with a
larger sample size or a reading test that only a few students could get 100%
on, the researchers might have detected a more pronounced difference in the
slopes for the two groups.</p>
<p>In the presence of a categorical by quantitative interaction, term-plots can
be generated that plot the results for each group on the same display or on separate facets for each level of the categorical variable. The first version is useful for comparing the different lines and the second version is useful to add the partial residuals and get a final exploration of model assumptions and ranges of values where predictor variables were observed in each group.

The term-plots basically provide a plot of the “simplified” SLR models for each group. In
Figure <a href="8-11-section8-11.html#fig:Figure8-28">2.179</a> we can see noticeable differences in the slopes
and intercepts. Note that
testing for differences in intercepts between groups is not very interesting
when there are different slopes because if you change the slope, you have to
change the intercept. The plot shows that there are clear differences in the
means even though we don’t have a test to directly assess that in this
complicated of a model<a href="#fn133" class="footnote-ref" id="fnref133"><sup>133</sup></a>. Figure Figure <a href="8-11-section8-11.html#fig:Figure8-28X">2.180</a> splits the plots up and adds partial residuals to the plots. The impact on the estimated model for the perfect scores in the non-dyslexic subjects is very prominent as well as the difference in the relationships between the two variables in the two groups.</p>

<div class="sourceCode" id="cb797"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb797-1" title="1"><span class="kw">plot</span>(<span class="kw">allEffects</span>(dys_model), <span class="dt">ci.style=</span><span class="st">&quot;bands&quot;</span>, <span class="dt">multiline=</span>T, <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="dt">grid=</span>T)</a></code></pre></div>
<div class="figure"><span id="fig:Figure8-28"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-28-1.png" alt="Term-plots for interaction model for reading scores using the multiline=T option to overlay the results for the two groups on one plot." width="576" />
<p class="caption">
Figure 2.179: Term-plots for interaction model for reading scores using the <code>multiline=T</code> option to overlay the results for the two groups on one plot.
</p>
</div>

<div class="sourceCode" id="cb798"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb798-1" title="1"><span class="kw">plot</span>(<span class="kw">allEffects</span>(dys_model, <span class="dt">residuals=</span>T), <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="dt">grid=</span>T)</a></code></pre></div>
<div class="figure"><span id="fig:Figure8-28X"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-28X-1.png" alt="Term-plots for interaction model for reading scores with partial residuals and the results for the two groups in different panels of the plot." width="576" />
<p class="caption">
Figure 2.180: Term-plots for interaction model for reading scores with partial residuals and the results for the two groups in different panels of the plot.
</p>
</div>
<p>It certainly appears in the plots that IQ has a different impact on the mean
score in the two groups (even though the p-value only provided marginal
evidence in support of the interaction). To
reinforce the potential dangers of forcing the same slope for both groups,
consider the additive model for these data. Again, this just shifts one group
off the other one, but both have the same slope. The following model summary
and term-plots (Figure <a href="8-11-section8-11.html#fig:Figure8-29">2.181</a>) suggest the potentially
dangerous conclusion that
can come from assuming a common slope when that might not be the case.</p>


<div class="sourceCode" id="cb799"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb799-1" title="1">dys_modelR &lt;-<span class="st"> </span><span class="kw">lm</span>(score<span class="op">~</span>ziq<span class="op">+</span>dys, <span class="dt">data=</span>dyslexic3)</a>
<a class="sourceLine" id="cb799-2" title="2"><span class="kw">summary</span>(dys_modelR)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ ziq + dys, data = dyslexic3)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.26062 -0.05565  0.02932  0.07577  0.13217 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  0.89178    0.02312  38.580  &lt; 2e-16
## ziq          0.02620    0.01957   1.339    0.188
## dysyes      -0.26879    0.03905  -6.883 2.41e-08
## 
## Residual standard error: 0.1049 on 41 degrees of freedom
## Multiple R-squared:  0.6858, Adjusted R-squared:  0.6705 
## F-statistic: 44.75 on 2 and 41 DF,  p-value: 4.917e-11</code></pre>
<div class="sourceCode" id="cb801"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb801-1" title="1"><span class="kw">plot</span>(<span class="kw">allEffects</span>(dys_modelR, <span class="dt">residuals=</span>T))</a></code></pre></div>
<div class="figure"><span id="fig:Figure8-29"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-29-1.png" alt="Term-plots for additive model for reading scores." width="576" />
<p class="caption">
Figure 2.181: Term-plots for additive model for reading scores.
</p>
</div>
<p>This model provides little evidence against the null hypothesis that IQ is not linearly related to reading score for all
students (<span class="math inline">\(t_{41}=1.34\)</span>, p-value=0.188), adjust for dyslexia status, but strong evidence against the null hypothesis of no difference in the true
y-intercepts (<span class="math inline">\(t_{41}=-6.88\)</span>, p-value <span class="math inline">\(&lt;0.00001\)</span>) after adjusting for the verbal IQ score.</p>
<p>Since the IQ term has a large p-value, we could drop it from the
model – leaving a model that only includes the grouping variable:</p>

<div class="sourceCode" id="cb802"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb802-1" title="1">dys_modelR2 &lt;-<span class="st"> </span><span class="kw">lm</span>(score<span class="op">~</span>dys, <span class="dt">data=</span>dyslexic3)</a>
<a class="sourceLine" id="cb802-2" title="2"><span class="kw">summary</span>(dys_modelR2)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ dys, data = dyslexic3)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.25818 -0.04510  0.02514  0.09520  0.09694 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  0.90480    0.02117  42.737   &lt;2e-16
## dysyes      -0.29892    0.03222  -9.278    1e-11
## 
## Residual standard error: 0.1059 on 42 degrees of freedom
## Multiple R-squared:  0.6721, Adjusted R-squared:  0.6643 
## F-statistic: 86.08 on 1 and 42 DF,  p-value: 1e-11</code></pre>
<div class="sourceCode" id="cb804"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb804-1" title="1"><span class="kw">plot</span>(<span class="kw">allEffects</span>(dys_modelR2, <span class="dt">residuals=</span>T))</a></code></pre></div>
<div class="figure"><span id="fig:Figure8-30"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-30-1.png" alt="Term-plot for dyslexia status only model for reading scores." width="576" />
<p class="caption">
Figure 2.182: Term-plot for dyslexia status only model for reading scores.
</p>
</div>
<p>These results, including the term-plot in Figure <a href="8-11-section8-11.html#fig:Figure8-30">2.182</a>, show
that there is evidence of a difference in the mean reading scores
between the two groups and maybe that is all these data really say… This is the
logical outcome if we decide that the interaction is not important <em>in this data set</em>. In general, if the interaction is dropped, the interaction model can be
reduced to considering an additive model with the categorical and quantitative
predictor variables.


Either or both of those variables could also be considered
for removal, usually starting with the variable with the larger p-value,
leaving a string of ever-simpler models possible if large p-values are continually
encountered<a href="#fn134" class="footnote-ref" id="fnref134"><sup>134</sup></a>.</p>
<p>It is useful to note that the last model has returned us to the first
model we encountered in Chapter <a href="2-chapter2.html#chapter2">2</a> where we were just comparing
the means for two groups.
However, the researchers probably were not seeking to make the discovery that
dyslexic students have a tougher time than non-dyslexic students on a reading
test but sometimes that is all that the data support. The key part of this
sequence of decisions was how much evidence you think a p-value of 0.06
contains…</p>
<p>For more than two categories in a categorical variable, the model
contains more
indicators to keep track of but uses the same ideas. We have to deal with
modifying the intercept and slope coefficients for <strong>every</strong> deviation group so the task is onerous but relatively repetitive. 
The general model is:</p>
<p><span class="math display">\[\begin{array}{rl}
y_i=\beta_0 &amp;+ \beta_1x_i +\beta_2I_{\text{Level }2,i}+\beta_3I_{\text{Level }3,i}
+\cdots+\beta_JI_{\text{Level }J,i} \\
&amp;+\beta_{J+1}I_{\text{Level }2,i}\:x_i+\beta_{J+2}I_{\text{Level }3,i}\:x_i
+\cdots+\beta_{2J-1}I_{\text{Level }J,i}\:x_i +\varepsilon_i.\ 
\end{array}\]</span></p>
<p>Specific to the audible tolerance/headache data that had four groups. The model
with an interaction present is</p>
<p><span class="math display">\[\begin{array}{rl}
\text{du2}_i = \beta_0 &amp; + \beta_1\cdot\text{du1}_i + \beta_2I_{T1,i} +
\beta_3I_{T2,i} + \beta_4I_{\text{T3},i} \\
&amp;+ \beta_5I_{T1,i}\cdot\text{du1}_i + \beta_6I_{T2,i}\cdot\text{du1}_i
+ \beta_7I_{\text{T3},i}\cdot\text{du1}_i+\varepsilon_i.\ 
\end{array}\]</span></p>
<p>Based on the following output, the estimated general regression model is</p>
<p><span class="math display">\[\begin{array}{rl}
\widehat{\text{du2}}_i = 0.241 &amp;+ 0.839\cdot\text{du1}_i + 1.091I_{T1,i} + 0.855I_{T2,i} +0.775I_{T3,i} \\
&amp; - 0.106I_{T1,i}\cdot\text{du1}_i - 0.040I_{T2,i}\cdot\text{du1}_i
+ 0.093I_{T3,i}\cdot\text{du1}_i.\ 
\end{array}\]</span></p>
<p>Then we could work out the specific equation for <strong>each group</strong> with
replacing their indicator variable in two places with 1s and the rest of
the indicators with 0. For example, for the <em>T1</em> group:</p>
<p><span class="math display">\[\begin{array}{rll}
\widehat{\text{du2}}_i &amp;= 0.241 &amp;+ 0.839\cdot\text{du1}_i + 1.091\cdot1 + 0.855\cdot0 +0.775\cdot0 \\
&amp;&amp;- 0.106\cdot1\cdot\text{du1}_i - 0.040\cdot0\cdot\text{du1}_i
+ 0.093\cdot0\cdot\text{du1}_i \\
\widehat{\text{du2}}_i&amp;=0.241&amp;+0.839\cdot\text{du1}_i + 1.091 - 0.106\cdot\text{du1}_i \\
\widehat{\text{du2}}_i&amp;=1.332 &amp;+ 0.733\cdot\text{du1}_i.\ 
\end{array}\]</span></p>

<div class="figure"><span id="fig:Figure8-31"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-31-1.png" alt="Term-plot for decibel tolerance interaction model with partial residuals (version 1)." width="672" />
<p class="caption">
Figure 2.183: Term-plot for decibel tolerance interaction model with partial residuals (version 1).
</p>
</div>

<div class="sourceCode" id="cb805"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb805-1" title="1">head2 &lt;-<span class="st"> </span><span class="kw">lm</span>(du2<span class="op">~</span>du1<span class="op">*</span>treatment, <span class="dt">data=</span>Headache)</a>
<a class="sourceLine" id="cb805-2" title="2"><span class="kw">summary</span>(head2)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = du2 ~ du1 * treatment, data = Headache)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.8072 -1.0969 -0.3285  0.8192 10.6039 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)      0.24073    0.68331   0.352    0.725
## du1              0.83923    0.10289   8.157 1.93e-12
## treatmentT1      1.09084    0.95020   1.148    0.254
## treatmentT2      0.85524    1.14770   0.745    0.458
## treatmentT3      0.77471    0.97370   0.796    0.428
## du1:treatmentT1 -0.10604    0.14326  -0.740    0.461
## du1:treatmentT2 -0.03981    0.17658  -0.225    0.822
## du1:treatmentT3  0.09300    0.13590   0.684    0.496
## 
## Residual standard error: 2.148 on 90 degrees of freedom
## Multiple R-squared:  0.7573, Adjusted R-squared:  0.7384 
## F-statistic: 40.12 on 7 and 90 DF,  p-value: &lt; 2.2e-16</code></pre>

<p>Or we can let the term-plots (Figures <a href="8-11-section8-11.html#fig:Figure8-31">2.183</a> and
<a href="8-11-section8-11.html#fig:Figure8-32">2.184</a>) show us all four different simplified models. Here we
can see that all the slopes “look” to be pretty similar. When the interaction
model is fit and the results “look” like the additive model, there is a good
chance that we will be able to avoid all this complication and just use the
additive model without missing anything interesting.

There are two different
options for displaying interaction models. Version 1 (Figure
<a href="8-11-section8-11.html#fig:Figure8-31">2.183</a>) has a
different panel for each level of the categorical variable and Version 2
(Figure <a href="8-11-section8-11.html#fig:Figure8-32">2.184</a>) puts all the lines on the same plot. In this
case, neither
version shows much of a difference and Version 2 overlaps so much that you
can’t see all the groups. In these situations, it can be useful to make the term-plots with <code>multiline=T</code> and <code>multiline=F</code> and select the version that captures the results best.</p>
<p>(ref:fig8-32) Term-plot for decibel tolerance interaction model (version 2).
This plot is not printed in color because it is impossible to distinguish the
four groups whether in color or black and white.</p>
<div class="figure"><span id="fig:Figure8-32"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-32-1.png" alt="(ref:fig8-32)" width="576" />
<p class="caption">
Figure 2.184: (ref:fig8-32)
</p>
</div>
<div class="sourceCode" id="cb807"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb807-1" title="1"><span class="kw">plot</span>(<span class="kw">allEffects</span>(head2, <span class="dt">residuals=</span>T))</a>
<a class="sourceLine" id="cb807-2" title="2"><span class="kw">plot</span>(<span class="kw">allEffects</span>(head2), <span class="dt">multiline=</span>T, <span class="dt">ci.style=</span><span class="st">&quot;bands&quot;</span>)</a></code></pre></div>
<p>In situations with more than 2 levels, the <span class="math inline">\(t\)</span>-tests for the interaction or
changing y-intercepts are not informative for deciding if you really need
different slopes or intercepts for all the groups.

They only tell
you if a specific group is potentially different from the baseline group and
the choice of the baseline is arbitrary. To assess whether we really need to
have varying slopes or intercepts with more than two groups we need to develop
<span class="math inline">\(F\)</span>-tests for the interaction part of the model.</p>



</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-smdata">
<p>Merkle, Ed, and Michael Smithson. 2018. <em>Smdata: Data to Accompany Smithson &amp; Merkle, 2013</em>. <a href="https://CRAN.R-project.org/package=smdata">https://CRAN.R-project.org/package=smdata</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="131">
<li id="fn131"><p>The strength of this recommendation drops when you have many predictors as you can’t do this for every variable, but the concern remains about an assumption of no interaction whenever you fit models without them. In more complex situations, think about variables that are most likely to interact in their impacts on the response based on the situation being studied and try to explore those.<a href="8-11-section8-11.html#fnref131" class="footnote-back">↩</a></p></li>
<li id="fn132"><p>Standardizing quantitative predictor variables is popular in social sciences, often where the response variable is also standardized. In those situations, they generate what are called “standardized betas” (<a href="https://en.wikipedia.org/wiki/Standardized_coefficient" class="uri">https://en.wikipedia.org/wiki/Standardized_coefficient</a>) that estimate the change in SDs in the response for a 1 SD increase in the explanatory variable.<a href="8-11-section8-11.html#fnref132" class="footnote-back">↩</a></p></li>
<li id="fn133"><p>There is a way to test for a difference in the two lines at a particular <span class="math inline">\(x\)</span> value but it is beyond the scope of this material.<a href="8-11-section8-11.html#fnref133" class="footnote-back">↩</a></p></li>
<li id="fn134"><p>This is an example of what is called “step down” testing for model
refinement which is a commonly used technique for arriving at a final model to
describe response variables. Note that each step in the process should be
reported, not just the final model that only has variables with small p-values
remaining in it.<a href="8-11-section8-11.html#fnref134" class="footnote-back">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="8-10-section8-10.html"><button class="btn btn-default">Previous</button></a>
<a href="8-12-section8-12.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
