<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="7.4 Comparing multiple regression models | Intermediate Statistics with R" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="gpeterson406/Greenwood_Book" />

<meta name="author" content="Mark C Greenwood" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="7.4 Comparing multiple regression models | Intermediate Statistics with R">

<title>7.4 Comparing multiple regression models | Intermediate Statistics with R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li class="has-sub"><a href="acknowledgments.html#acknowledgments">Acknowledgments</a><ul>
<li><a href="0-1-section1-5.html#section1-5"><span class="toc-section-number">0.1</span> Summary of important R code</a></li>
</ul></li>
<li class="has-sub"><a href="1-chapter2.html#chapter2"><span class="toc-section-number">1</span> (R)e-Introduction to statistics</a><ul>
<li><a href="1-1-section2-1.html#section2-1"><span class="toc-section-number">1.1</span> Histograms, boxplots, and density curves</a></li>
<li><a href="1-2-section2-2.html#section2-2"><span class="toc-section-number">1.2</span> Pirate-plots</a></li>
<li><a href="1-3-section2-3.html#section2-3"><span class="toc-section-number">1.3</span> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li><a href="1-4-section2-4.html#section2-4"><span class="toc-section-number">1.4</span> Permutation testing for the two sample mean situation</a></li>
<li><a href="1-5-section2-5.html#section2-5"><span class="toc-section-number">1.5</span> Hypothesis testing (general)</a></li>
<li><a href="1-6-section2-6.html#section2-6"><span class="toc-section-number">1.6</span> Connecting randomization (nonparametric) and parametric tests</a></li>
<li><a href="1-7-section2-7.html#section2-7"><span class="toc-section-number">1.7</span> Second example of permutation tests</a></li>
<li><a href="1-8-section2-8.html#section2-8"><span class="toc-section-number">1.8</span> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li><a href="1-9-section2-9.html#section2-9"><span class="toc-section-number">1.9</span> Confidence intervals and bootstrapping</a></li>
<li><a href="1-10-section2-10.html#section2-10"><span class="toc-section-number">1.10</span> Bootstrap confidence intervals for difference in GPAs</a></li>
<li><a href="1-11-section2-11.html#section2-11"><span class="toc-section-number">1.11</span> Chapter summary</a></li>
<li><a href="1-12-section2-12.html#section2-12"><span class="toc-section-number">1.12</span> Summary of important R code</a></li>
<li><a href="1-13-section2-13.html#section2-13"><span class="toc-section-number">1.13</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="2-chapter3.html#chapter3"><span class="toc-section-number">2</span> One-Way ANOVA</a><ul>
<li><a href="2-1-section3-1.html#section3-1"><span class="toc-section-number">2.1</span> Situation</a></li>
<li><a href="2-2-section3-2.html#section3-2"><span class="toc-section-number">2.2</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li><a href="2-3-section3-3.html#section3-3"><span class="toc-section-number">2.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li><a href="2-4-section3-4.html#section3-4"><span class="toc-section-number">2.4</span> ANOVA model diagnostics including QQ-plots</a></li>
<li><a href="2-5-section3-5.html#section3-5"><span class="toc-section-number">2.5</span> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li><a href="2-6-section3-6.html#section3-6"><span class="toc-section-number">2.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li><a href="2-7-section3-7.html#section3-7"><span class="toc-section-number">2.7</span> Pair-wise comparisons for the Overtake data</a></li>
<li><a href="2-8-section3-8.html#section3-8"><span class="toc-section-number">2.8</span> Chapter summary</a></li>
<li><a href="2-9-section3-9.html#section3-9"><span class="toc-section-number">2.9</span> Summary of important R code</a></li>
<li><a href="2-10-section3-10.html#section3-10"><span class="toc-section-number">2.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="3-chapter4.html#chapter4"><span class="toc-section-number">3</span> Two-Way ANOVA</a><ul>
<li><a href="3-1-section4-1.html#section4-1"><span class="toc-section-number">3.1</span> Situation</a></li>
<li><a href="3-2-section4-2.html#section4-2"><span class="toc-section-number">3.2</span> Designing a two-way experiment and visualizing results</a></li>
<li><a href="3-3-section4-3.html#section4-3"><span class="toc-section-number">3.3</span> Two-Way ANOVA models and hypothesis tests</a></li>
<li><a href="3-4-section4-4.html#section4-4"><span class="toc-section-number">3.4</span> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li><a href="3-5-section4-5.html#section4-5"><span class="toc-section-number">3.5</span> Observational study example: The Psychology of Debt</a></li>
<li><a href="3-6-section4-6.html#section4-6"><span class="toc-section-number">3.6</span> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li><a href="3-7-section4-7.html#section4-7"><span class="toc-section-number">3.7</span> Chapter summary</a></li>
<li><a href="3-8-section4-8.html#section4-8"><span class="toc-section-number">3.8</span> Summary of important R code</a></li>
<li><a href="3-9-section4-9.html#section4-9"><span class="toc-section-number">3.9</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="4-chapter5.html#chapter5"><span class="toc-section-number">4</span> Chi-square tests</a><ul>
<li><a href="4-1-section5-1.html#section5-1"><span class="toc-section-number">4.1</span> Situation, contingency tables, and tableplots</a></li>
<li><a href="4-2-section5-2.html#section5-2"><span class="toc-section-number">4.2</span> Homogeneity test hypotheses</a></li>
<li><a href="4-3-section5-3.html#section5-3"><span class="toc-section-number">4.3</span> Independence test hypotheses</a></li>
<li><a href="4-4-section5-4.html#section5-4"><span class="toc-section-number">4.4</span> Models for R by C tables</a></li>
<li><a href="4-5-section5-5.html#section5-5"><span class="toc-section-number">4.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="4-6-section5-6.html#section5-6"><span class="toc-section-number">4.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="4-7-section5-7.html#section5-7"><span class="toc-section-number">4.7</span> Examining residuals for the source of differences</a></li>
<li><a href="4-8-section5-8.html#section5-8"><span class="toc-section-number">4.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li><a href="4-9-section5-9.html#section5-9"><span class="toc-section-number">4.9</span> Political party and voting results: Complete analysis</a></li>
<li><a href="4-10-section5-10.html#section5-10"><span class="toc-section-number">4.10</span> Is cheating and lying related in students?</a></li>
<li><a href="4-11-section5-11.html#section5-11"><span class="toc-section-number">4.11</span> Analyzing a stratified random sample of California schools</a></li>
<li><a href="4-12-section5-12.html#section5-12"><span class="toc-section-number">4.12</span> Chapter summary</a></li>
<li><a href="4-13-section5-13.html#section5-13"><span class="toc-section-number">4.13</span> Summary of important R commands</a></li>
<li><a href="4-14-section5-14.html#section5-14"><span class="toc-section-number">4.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="5-chapter6.html#chapter6"><span class="toc-section-number">5</span> Correlation and Simple Linear Regression</a><ul>
<li><a href="5-1-section6-1.html#section6-1"><span class="toc-section-number">5.1</span> Relationships between two quantitative variables</a></li>
<li><a href="5-2-section6-2.html#section6-2"><span class="toc-section-number">5.2</span> Estimating the correlation coefficient</a></li>
<li><a href="5-3-section6-3.html#section6-3"><span class="toc-section-number">5.3</span> Relationships between variables by groups</a></li>
<li><a href="5-4-section6-4.html#section6-4"><span class="toc-section-number">5.4</span> Inference for the correlation coefficient</a></li>
<li><a href="5-5-section6-5.html#section6-5"><span class="toc-section-number">5.5</span> Are tree diameters related to tree heights?</a></li>
<li><a href="5-6-section6-6.html#section6-6"><span class="toc-section-number">5.6</span> Describing relationships with a regression model</a></li>
<li><a href="5-7-section6-7.html#section6-7"><span class="toc-section-number">5.7</span> Least Squares Estimation</a></li>
<li><a href="5-8-section6-8.html#section6-8"><span class="toc-section-number">5.8</span> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li><a href="5-9-section6-9.html#section6-9"><span class="toc-section-number">5.9</span> Outliers: leverage and influence</a></li>
<li><a href="5-10-section6-10.html#section6-10"><span class="toc-section-number">5.10</span> Residual diagnostics – setting the stage for inference</a></li>
<li><a href="5-11-section6-11.html#section6-11"><span class="toc-section-number">5.11</span> Old Faithful discharge and waiting times</a></li>
<li><a href="5-12-section6-12.html#section6-12"><span class="toc-section-number">5.12</span> Chapter summary</a></li>
<li><a href="5-13-section6-13.html#section6-13"><span class="toc-section-number">5.13</span> Summary of important R code</a></li>
<li><a href="5-14-section6-14.html#section6-14"><span class="toc-section-number">5.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="6-chapter7.html#chapter7"><span class="toc-section-number">6</span> Simple linear regression inference</a><ul>
<li><a href="6-1-section7-1.html#section7-1"><span class="toc-section-number">6.1</span> Model</a></li>
<li><a href="6-2-section7-2.html#section7-2"><span class="toc-section-number">6.2</span> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li><a href="6-3-section7-3.html#section7-3"><span class="toc-section-number">6.3</span> Bozeman temperature trend</a></li>
<li><a href="6-4-section7-4.html#section7-4"><span class="toc-section-number">6.4</span> Randomization-based inferences for the slope coefficient</a></li>
<li><a href="6-5-section7-5.html#section7-5"><span class="toc-section-number">6.5</span> Transformations part I: Linearizing relationships</a></li>
<li><a href="6-6-section7-6.html#section7-6"><span class="toc-section-number">6.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li><a href="6-7-section7-7.html#section7-7"><span class="toc-section-number">6.7</span> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li><a href="6-8-section7-8.html#section7-8"><span class="toc-section-number">6.8</span> Chapter summary</a></li>
<li><a href="6-9-section7-9.html#section7-9"><span class="toc-section-number">6.9</span> Summary of important R code</a></li>
<li><a href="6-10-section7-10.html#section7-10"><span class="toc-section-number">6.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="7-chapter8.html#chapter8"><span class="toc-section-number">7</span> Multiple linear regression</a><ul>
<li><a href="7-1-section8-1.html#section8-1"><span class="toc-section-number">7.1</span> Going from SLR to MLR</a></li>
<li><a href="7-2-section8-2.html#section8-2"><span class="toc-section-number">7.2</span> Validity conditions in MLR</a></li>
<li><a href="7-3-section8-3.html#section8-3"><span class="toc-section-number">7.3</span> Interpretation of MLR terms</a></li>
<li><a href="7-4-section8-4.html#section8-4"><span class="toc-section-number">7.4</span> Comparing multiple regression models</a></li>
<li><a href="7-5-section8-5.html#section8-5"><span class="toc-section-number">7.5</span> General recommendations for MLR interpretations and VIFs</a></li>
<li><a href="7-6-section8-6.html#section8-6"><span class="toc-section-number">7.6</span> MLR inference: Parameter inferences using the t-distribution</a></li>
<li><a href="7-7-section8-7.html#section8-7"><span class="toc-section-number">7.7</span> Overall F-test in multiple linear regression</a></li>
<li><a href="7-8-section8-8.html#section8-8"><span class="toc-section-number">7.8</span> Case study: First year college GPA and SATs</a></li>
<li><a href="7-9-section8-9.html#section8-9"><span class="toc-section-number">7.9</span> Different intercepts for different groups: MLR with indicator variables</a></li>
<li><a href="7-10-section8-10.html#section8-10"><span class="toc-section-number">7.10</span> Additive MLR with more than two groups: Headache example</a></li>
<li><a href="7-11-section8-11.html#section8-11"><span class="toc-section-number">7.11</span> Different slopes and different intercepts</a></li>
<li><a href="7-12-section8-12.html#section8-12"><span class="toc-section-number">7.12</span> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li><a href="7-13-section8-13.html#section8-13"><span class="toc-section-number">7.13</span> AICs for model selection</a></li>
<li><a href="7-14-section8-14.html#section8-14"><span class="toc-section-number">7.14</span> Case study: Forced expiratory volume model selection using AICs</a></li>
<li><a href="7-15-section8-15.html#section8-15"><span class="toc-section-number">7.15</span> Chapter summary</a></li>
<li><a href="7-16-section8-16.html#section8-16"><span class="toc-section-number">7.16</span> Summary of important R code</a></li>
<li><a href="7-17-section8-17.html#section8-17"><span class="toc-section-number">7.17</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="8-chapter9.html#chapter9"><span class="toc-section-number">8</span> Case studies</a><ul>
<li><a href="8-1-section9-1.html#section9-1"><span class="toc-section-number">8.1</span> Overview of material covered</a></li>
<li><a href="8-2-section9-2.html#section9-2"><span class="toc-section-number">8.2</span> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li><a href="8-3-section9-3.html#section9-3"><span class="toc-section-number">8.3</span> Ants learn to rely on more informative attributes during decision-making</a></li>
<li><a href="8-4-section9-4.html#section9-4"><span class="toc-section-number">8.4</span> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li><a href="8-5-section9-5.html#section9-5"><span class="toc-section-number">8.5</span> What do didgeridoos really do about sleepiness?</a></li>
<li><a href="8-6-section9-6.html#section9-6"><span class="toc-section-number">8.6</span> General summary</a></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="section8-4" class="section level2">
<h2><span class="header-section-number">7.4</span> Comparing multiple regression models</h2>
<p>With more than one variable, we now have many potential models that we could
consider.

We could include only one of the predictors, all of them, or
combinations of sets of the
variables. For example, maybe the model that includes <em>Elevation</em> does not “need”
both <em>Min.Temp</em> and <em>Max.Temp</em>? Or maybe the model isn’t improved over an SLR
with just <em>Elevation</em> as a predictor. Or maybe none of the predictors are
“useful”? In this section,
we discuss some general model comparison issues and a metric that can be used
to pick among a suite of different models (often called a set of <strong><em>candidate
models</em></strong> to reflect that they are all potentially interesting and we need to
compare them and possibly pick one). </p>
<p>It is certainly possible the researchers may have an <em>a priori</em>
reason to only consider a single model. For example, in a designed experiment
where combinations of, say, three different predictors are randomly assigned,
the initial model with all three predictors may be sufficient to address all the
research questions of interest. One advantage in these situations is that the
variable combinations can be created to prevent multicollinearity among the
predictors and avoid that complication in interpretations. However, this is
more the exception than the rule. Usually, there are competing predictors or
questions about whether some predictors matter more than others. This type of
research always introduces the potential for multicollinearity to complicate the
interpretation of each predictor in the presence of others. Because of this,
multiple models are often considered, where “unimportant” variables are dropped
from the model. The assessment of “importance” using p-values will be discussed
in Section <a href="7-6-section8-6.html#section8-6">7.6</a>, but for now we will consider other reasons to pick
one model over another. </p>
<p>There are some general reasons to choose a particular model:</p>
<ol style="list-style-type: decimal">
<li><p>Diagnostics are better with one model compared to others.</p></li>
<li><p>One model predicts/explains the responses better than the others
(<strong><em>R</em></strong><sup>2</sup>).</p></li>
<li><p><em>a priori</em> reasons to “use” a particular model, for example in a designed
experiment or it includes variable(s) whose slopes need to be estimated to find
interesting results (even if the variables are not “important” in the model).</p></li>
<li><p>Model selection “criteria” suggest one model is better than the others<a href="#fn118" class="footnote-ref" id="fnref118"><sup>118</sup></a>.</p></li>
</ol>
<p>It is OK to consider multiple reasons to select a model but it is dangerous to
“shop” for a model across many possible models – a practice which is sometimes
called <strong><em>data-dredging</em></strong> and leads to a high chance of spurious results from a
single model that is usually reported based on this type of exploration. Just
like in other discussions of
multiple testing issues previously, if you explore many versions of a model,
maybe only keeping the best ones, this is very different from picking one model
(and tests) <em>a priori</em> and just exploring that result.</p>
<p>As in SLR, we can use the <strong><em>R</em></strong><sup>2</sup> (the <strong><em>coefficient of
determination</em></strong>)  to measure the percentage of the variation in the
response variable that the model explains. In MLR, it is important to remember
that <strong><em>R</em></strong><sup>2</sup> is now an overall
measure for the model and not specific to a single variable. It is comparable to
other models including those fit with only a single predictor (SLR). So to
meet criterion (2), we could simply find the model with the largest
<strong><em>R</em></strong><sup>2</sup> value, finding the model that explains the most variation in
the responses.
Unfortunately for this idea, when you add more “stuff” to a regression model
(even “unimportant” predictors), the <strong><em>R</em></strong><sup>2</sup> will always go up. This
can be seen by considering</p>
<p><span class="math display">\[R^2 = \frac{\text{SS}_{\text{regression}}}{\text{SS}_{\text{total}}}\ 
\text{ where }\  \text{SS}_{\text{regression}} = \text{SS}_{\text{total}}
- \text{SS}_{\text{error}}\ 
\text{ and }\  \text{SS}_{\text{error}} = \Sigma(y-\hat{y})^2\ .\]</span></p>
<p>Because adding extra variables to a linear model will only make the fitted
values better, not worse, the <span class="math inline">\(\text{SS}_{\text{error}}\)</span> will always go down
if more predictors are added to the model. If <span class="math inline">\(\text{SS}_{\text{error}}\)</span>
goes down and <span class="math inline">\(\text{SS}_{\text{total}}\)</span> is fixed, then adding extra variables
will always increase <span class="math inline">\(\text{SS}_{\text{regression}}\)</span> and, thus, increase
<strong><em>R</em></strong><sup>2</sup>. This means that <strong><em>R</em></strong><sup>2</sup> is only useful for
selecting models when you are picking between two models of the same size
(same number of predictors). So we mainly use it as a summary of model quality
once we pick a model, not a method of picking among a set of candidate models.
Remember that <strong><em>R</em></strong><sup>2</sup> continues to have the property of being between
0 and 1 (or 0% and 100%) and that value refers to the <strong>proportion (percentage)
of variation in the response explained by the model</strong>, whether we are using it
for SLR or MLR.</p>
<p>However, there is an adjustment to the <strong><em>R</em></strong><sup>2</sup> measure that makes it
useful for selecting among models. The measure is called the <strong><em>adjusted</em></strong>
<strong><em>R</em></strong><sup>2</sup>.  The <span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span> measure adds a
penalty for adding more variables to the model, providing the potential for this
measure to decrease if the extra variables do not really benefit the model. The
measure is calculated as</p>
<p><span class="math display">\[R^2_{\text{adjusted}} = 1 - 
\frac{\text{SS}_{\text{error}}/df_{\text{error}}}{\text{SS}_{\text{total}}/(N-1)}
= 1 - \frac{\text{MS}_{\text{error}}}{\text{MS}_{\text{total}}},\]</span></p>
<p>which incorporates the <em>degrees of freedom</em> for the model via the error
<em>degrees of freedom</em> which go
down as the model complexity increases. This adjustment means that just
adding extra useless variables (variables that do not explain very much extra
variation) do not increase this measure. That makes this measure useful for
model selection since it can help us to stop adding unimportant variables and
find a “good” model among a set of candidates. Like the regular
<strong><em>R</em></strong><sup>2</sup>, larger values are better. The downside to
<span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span> is that it <strong>is no longer a percentage
of variation in the response that is explained by the model</strong>; it can be less
than 0 and so has no interpretable scale. It is just “larger is better”. It
provides
one method for building a model (different from using p-values to drop
unimportant variables as discussed below), by fitting a set of candidate models
containing different variables and then <strong>picking the model with the largest</strong>
<span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span>.

You will want to interpret this new
measure on a percentage scale, but do not do that. It is a just a measure to
help you pick a model and that is all it is!</p>
<p>One other caveat in
model comparison is worth mentioning: make sure you are comparing models for
the same responses. That may sound trivial and usually it is. But when there
are missing values in the data set, especially on some explanatory variables
and not others, it is important to be careful that the <span class="math inline">\(y\text{&#39;s}\)</span> do not
change between models you are comparing. This relates to our <em>Snow Depth</em>
modeling because responses were being removed due to their influential nature.
We can’t compare <strong><em>R</em></strong><sup>2</sup> or <span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span>
for <span class="math inline">\(n=25\)</span> to a model when <span class="math inline">\(n=23\)</span> – it isn’t a fair comparison on
either measure since they based on the total variability which is changing as
the responses used change.</p>
<p>In the MLR (or SLR) model summaries, both the <strong><em>R</em></strong><sup>2</sup> and
<span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span>
are available. Make sure you are able to pick out the correct one. For the
reduced data set (<span class="math inline">\(n=23\)</span>) <em>Snow Depth</em> models, the pertinent part of the model
summary for the model with all three predictors is the last three lines:</p>
<div class="sourceCode" id="cb724"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb724-1" title="1">m6 &lt;-<span class="st"> </span><span class="kw">lm</span>(Snow.Depth<span class="op">~</span>Elevation<span class="op">+</span>Min.Temp<span class="op">+</span>Max.Temp, <span class="dt">data=</span>snotel2[<span class="op">-</span><span class="kw">c</span>(<span class="dv">9</span>,<span class="dv">22</span>),])</a>
<a class="sourceLine" id="cb724-2" title="2"><span class="kw">summary</span>(m6)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Snow.Depth ~ Elevation + Min.Temp + Max.Temp, data = snotel2[-c(9, 
##     22), ])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -14.878  -4.486   0.024   3.996  20.728 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -2.133e+02  7.458e+01  -2.859   0.0100
## Elevation    2.686e-02  4.997e-03   5.374 3.47e-05
## Min.Temp     9.843e-01  1.359e+00   0.724   0.4776
## Max.Temp     1.243e+00  5.452e-01   2.280   0.0343
## 
## Residual standard error: 8.832 on 19 degrees of freedom
## Multiple R-squared:  0.8535, Adjusted R-squared:  0.8304 
## F-statistic:  36.9 on 3 and 19 DF,  p-value: 4.003e-08</code></pre>
<p>There is a value for <span class="math inline">\(\large{\textbf{Multiple R-Squared}} \text{ of } 0.8535\)</span>,
this is the <strong><em>R</em></strong><sup>2</sup> value and suggests that the model with
<em>Elevation</em>, <em>Min</em> and <em>Max</em> temperatures explains 85.4% of the variation in
<em>Snow Depth</em>. The <span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span> is 0.8304 and is
available further to the right labeled as
<span class="math inline">\(\color{red}{\large{\textbf{Adjusted R-Squared}}}\)</span>. We repeated this for a
suite of different models for this same <span class="math inline">\(n=23\)</span> data set and found the following
results in Table <a href="7-4-section8-4.html#tab:Table8-1">1.13</a>. The top <span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span>
model is the model with <em>Elevation</em> and <em>Max.Temp</em>, which beats out the model with
all three variables on <span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span>. Note that the top
<em>R</em><sup>2</sup> model is the model with three predictors, but the most complicated model
will always have that characteristic.</p>

<table style="width:100%;">
<caption><span id="tab:Table8-1">Table 1.13: </span> Model comparisons for Snow Depth data, sorted by model complexity.</caption>
<colgroup>
<col width="33%" />
<col width="10%" />
<col width="11%" />
<col width="21%" />
<col width="21%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>Model</strong>        </th>
<th align="center"><span class="math inline">\(\boldsymbol{K}\)</span></th>
<th align="right"><span class="math inline">\(\boldsymbol{R^2}\)</span></th>
<th align="right"><span class="math inline">\(\boldsymbol{R^2_{\text{adjusted}}}\)</span></th>
<th align="center"><span class="math inline">\(\boldsymbol{R^2_{\text{adjusted}}}\)</span>
<strong>Rank</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">SD <span class="math inline">\(\sim\)</span> Elevation</td>
<td align="center">1</td>
<td align="right">0.8087</td>
<td align="right">0.7996</td>
<td align="center">3</td>
</tr>
<tr class="even">
<td align="left">SD <span class="math inline">\(\sim\)</span> Min.Temp</td>
<td align="center">1</td>
<td align="right">0.6283</td>
<td align="right">0.6106</td>
<td align="center">5</td>
</tr>
<tr class="odd">
<td align="left">SD <span class="math inline">\(\sim\)</span> Max.Temp</td>
<td align="center">1</td>
<td align="right">0.4131</td>
<td align="right">0.3852</td>
<td align="center">7</td>
</tr>
<tr class="even">
<td align="left">SD <span class="math inline">\(\sim\)</span> Elevation + Min.Temp</td>
<td align="center">2</td>
<td align="right">0.8134</td>
<td align="right">0.7948</td>
<td align="center">4</td>
</tr>
<tr class="odd">
<td align="left">SD <span class="math inline">\(\sim\)</span> Elevation + Max.Temp</td>
<td align="center">2</td>
<td align="right">0.8495</td>
<td align="right">0.8344</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="left">SD <span class="math inline">\(\sim\)</span> Min.Temp + Max.Temp</td>
<td align="center">2</td>
<td align="right">0.6308</td>
<td align="right">0.5939</td>
<td align="center">6</td>
</tr>
<tr class="odd">
<td align="left">SD <span class="math inline">\(\sim\)</span> Elevation + Min.Temp
+ Max.Temp</td>
<td align="center">3</td>
<td align="right">0.8535</td>
<td align="right">0.8304</td>
<td align="center">2</td>
</tr>
</tbody>
</table>
<p>The top adjusted <strong><em>R</em></strong><sup>2</sup> model contained <em>Elevation</em> and <em>Max.Temp</em> and has an <strong><em>R</em></strong><sup>2</sup> of
0.8495, so we can say that the model with
<em>Elevation</em> and <em>Maximum Temperature</em> explains 84.95% percent of the variation
in <em>Snow Depth</em> and also that this model was selected based on the
<span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span>. One of the important features of
<span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span> is available in this example – adding
variables often does not always increase its value even though
<strong><em>R</em></strong><sup>2</sup> does increase with <strong>any</strong> addition. In
Section <a href="7-13-section8-13.html#section8-13">7.13</a> we consider a competitor for this model selection
criterion that may “work” a bit better and be extendable into more complicated
modeling situations; that measure is called the <strong><em>AIC</em></strong>.</p>
</div>
<div class="footnotes">
<hr />
<ol start="118">
<li id="fn118"><p>Also
see Section <a href="7-13-section8-13.html#section8-13">7.13</a> for another method of picking among different
models.<a href="7-4-section8-4.html#fnref118" class="footnote-back">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="7-3-section8-3.html"><button class="btn btn-default">Previous</button></a>
<a href="7-5-section8-5.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
