<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="2.4 Permutation testing for the two sample mean situation | Intermediate Statistics with R" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="gpeterson406/Greenwood_Book" />

<meta name="author" content="Mark C Greenwood" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="2.4 Permutation testing for the two sample mean situation | Intermediate Statistics with R">

<title>2.4 Permutation testing for the two sample mean situation | Intermediate Statistics with R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li class="has-sub"><a href="1-chapter1.html#chapter1"><span class="toc-section-number">1</span> Preface</a><ul>
<li><a href="1-1-section1-1.html#section1-1"><span class="toc-section-number">1.1</span> Overview of methods</a></li>
<li><a href="1-2-section1-2.html#section1-2"><span class="toc-section-number">1.2</span> Getting started in R</a></li>
<li><a href="1-3-section1-3.html#section1-3"><span class="toc-section-number">1.3</span> Basic summary statistics, histograms, and boxplots using R</a></li>
<li><a href="1-4-section1-4.html#section1-4"><span class="toc-section-number">1.4</span> Chapter summary</a></li>
<li><a href="1-5-section1-5.html#section1-5"><span class="toc-section-number">1.5</span> Summary of important R code</a></li>
<li><a href="1-6-section1-6.html#section1-6"><span class="toc-section-number">1.6</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="2-chapter2.html#chapter2"><span class="toc-section-number">2</span> (R)e-Introduction to statistics</a><ul>
<li><a href="2-1-section2-1.html#section2-1"><span class="toc-section-number">2.1</span> Histograms, boxplots, and density curves</a></li>
<li><a href="2-2-section2-2.html#section2-2"><span class="toc-section-number">2.2</span> Pirate-plots</a></li>
<li><a href="2-3-section2-3.html#section2-3"><span class="toc-section-number">2.3</span> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li><a href="2-4-section2-4.html#section2-4"><span class="toc-section-number">2.4</span> Permutation testing for the two sample mean situation</a></li>
<li><a href="2-5-section2-5.html#section2-5"><span class="toc-section-number">2.5</span> Hypothesis testing (general)</a></li>
<li><a href="2-6-section2-6.html#section2-6"><span class="toc-section-number">2.6</span> Connecting randomization (nonparametric) and parametric tests</a></li>
<li><a href="2-7-section2-7.html#section2-7"><span class="toc-section-number">2.7</span> Second example of permutation tests</a></li>
<li><a href="2-8-section2-8.html#section2-8"><span class="toc-section-number">2.8</span> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li><a href="2-9-section2-9.html#section2-9"><span class="toc-section-number">2.9</span> Confidence intervals and bootstrapping</a></li>
<li><a href="2-10-section2-10.html#section2-10"><span class="toc-section-number">2.10</span> Bootstrap confidence intervals for difference in GPAs</a></li>
<li><a href="2-11-section2-11.html#section2-11"><span class="toc-section-number">2.11</span> Chapter summary</a></li>
<li><a href="2-12-section2-12.html#section2-12"><span class="toc-section-number">2.12</span> Summary of important R code</a></li>
<li><a href="2-13-section2-13.html#section2-13"><span class="toc-section-number">2.13</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="3-chapter3.html#chapter3"><span class="toc-section-number">3</span> One-Way ANOVA</a><ul>
<li><a href="3-1-section3-1.html#section3-1"><span class="toc-section-number">3.1</span> Situation</a></li>
<li><a href="3-2-section3-2.html#section3-2"><span class="toc-section-number">3.2</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li><a href="3-3-section3-3.html#section3-3"><span class="toc-section-number">3.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li><a href="3-4-section3-4.html#section3-4"><span class="toc-section-number">3.4</span> ANOVA model diagnostics including QQ-plots</a></li>
<li><a href="3-5-section3-5.html#section3-5"><span class="toc-section-number">3.5</span> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li><a href="3-6-section3-6.html#section3-6"><span class="toc-section-number">3.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li><a href="3-7-section3-7.html#section3-7"><span class="toc-section-number">3.7</span> Pair-wise comparisons for the Overtake data</a></li>
<li><a href="3-8-section3-8.html#section3-8"><span class="toc-section-number">3.8</span> Chapter summary</a></li>
<li><a href="3-9-section3-9.html#section3-9"><span class="toc-section-number">3.9</span> Summary of important R code</a></li>
<li><a href="3-10-section3-10.html#section3-10"><span class="toc-section-number">3.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="4-chapter4.html#chapter4"><span class="toc-section-number">4</span> Two-Way ANOVA</a><ul>
<li><a href="4-1-section4-1.html#section4-1"><span class="toc-section-number">4.1</span> Situation</a></li>
<li><a href="4-2-section4-2.html#section4-2"><span class="toc-section-number">4.2</span> Designing a two-way experiment and visualizing results</a></li>
<li><a href="4-3-section4-3.html#section4-3"><span class="toc-section-number">4.3</span> Two-Way ANOVA models and hypothesis tests</a></li>
<li><a href="4-4-section4-4.html#section4-4"><span class="toc-section-number">4.4</span> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li><a href="4-5-section4-5.html#section4-5"><span class="toc-section-number">4.5</span> Observational study example: The Psychology of Debt</a></li>
<li><a href="4-6-section4-6.html#section4-6"><span class="toc-section-number">4.6</span> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li><a href="4-7-section4-7.html#section4-7"><span class="toc-section-number">4.7</span> Chapter summary</a></li>
<li><a href="4-8-section4-8.html#section4-8"><span class="toc-section-number">4.8</span> Summary of important R code</a></li>
<li><a href="4-9-section4-9.html#section4-9"><span class="toc-section-number">4.9</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="5-chapter5.html#chapter5"><span class="toc-section-number">5</span> Chi-square tests</a><ul>
<li><a href="5-1-section5-1.html#section5-1"><span class="toc-section-number">5.1</span> Situation, contingency tables, and tableplots</a></li>
<li><a href="5-2-section5-2.html#section5-2"><span class="toc-section-number">5.2</span> Homogeneity test hypotheses</a></li>
<li><a href="5-3-section5-3.html#section5-3"><span class="toc-section-number">5.3</span> Independence test hypotheses</a></li>
<li><a href="5-4-section5-4.html#section5-4"><span class="toc-section-number">5.4</span> Models for R by C tables</a></li>
<li><a href="5-5-section5-5.html#section5-5"><span class="toc-section-number">5.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-6-section5-6.html#section5-6"><span class="toc-section-number">5.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-7-section5-7.html#section5-7"><span class="toc-section-number">5.7</span> Examining residuals for the source of differences</a></li>
<li><a href="5-8-section5-8.html#section5-8"><span class="toc-section-number">5.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li><a href="5-9-section5-9.html#section5-9"><span class="toc-section-number">5.9</span> Political party and voting results: Complete analysis</a></li>
<li><a href="5-10-section5-10.html#section5-10"><span class="toc-section-number">5.10</span> Is cheating and lying related in students?</a></li>
<li><a href="5-11-section5-11.html#section5-11"><span class="toc-section-number">5.11</span> Analyzing a stratified random sample of California schools</a></li>
<li><a href="5-12-section5-12.html#section5-12"><span class="toc-section-number">5.12</span> Chapter summary</a></li>
<li><a href="5-13-section5-13.html#section5-13"><span class="toc-section-number">5.13</span> Summary of important R commands</a></li>
<li><a href="5-14-section5-14.html#section5-14"><span class="toc-section-number">5.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="6-chapter6.html#chapter6"><span class="toc-section-number">6</span> Correlation and Simple Linear Regression</a><ul>
<li><a href="6-1-section6-1.html#section6-1"><span class="toc-section-number">6.1</span> Relationships between two quantitative variables</a></li>
<li><a href="6-2-section6-2.html#section6-2"><span class="toc-section-number">6.2</span> Estimating the correlation coefficient</a></li>
<li><a href="6-3-section6-3.html#section6-3"><span class="toc-section-number">6.3</span> Relationships between variables by groups</a></li>
<li><a href="6-4-section6-4.html#section6-4"><span class="toc-section-number">6.4</span> Inference for the correlation coefficient</a></li>
<li><a href="6-5-section6-5.html#section6-5"><span class="toc-section-number">6.5</span> Are tree diameters related to tree heights?</a></li>
<li><a href="6-6-section6-6.html#section6-6"><span class="toc-section-number">6.6</span> Describing relationships with a regression model</a></li>
<li><a href="6-7-section6-7.html#section6-7"><span class="toc-section-number">6.7</span> Least Squares Estimation</a></li>
<li><a href="6-8-section6-8.html#section6-8"><span class="toc-section-number">6.8</span> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li><a href="6-9-section6-9.html#section6-9"><span class="toc-section-number">6.9</span> Outliers: leverage and influence</a></li>
<li><a href="6-10-section6-10.html#section6-10"><span class="toc-section-number">6.10</span> Residual diagnostics – setting the stage for inference</a></li>
<li><a href="6-11-section6-11.html#section6-11"><span class="toc-section-number">6.11</span> Old Faithful discharge and waiting times</a></li>
<li><a href="6-12-section6-12.html#section6-12"><span class="toc-section-number">6.12</span> Chapter summary</a></li>
<li><a href="6-13-section6-13.html#section6-13"><span class="toc-section-number">6.13</span> Summary of important R code</a></li>
<li><a href="6-14-section6-14.html#section6-14"><span class="toc-section-number">6.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="7-chapter7.html#chapter7"><span class="toc-section-number">7</span> Simple linear regression inference</a><ul>
<li><a href="7-1-section7-1.html#section7-1"><span class="toc-section-number">7.1</span> Model</a></li>
<li><a href="7-2-section7-2.html#section7-2"><span class="toc-section-number">7.2</span> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li><a href="7-3-section7-3.html#section7-3"><span class="toc-section-number">7.3</span> Bozeman temperature trend</a></li>
<li><a href="7-4-section7-4.html#section7-4"><span class="toc-section-number">7.4</span> Randomization-based inferences for the slope coefficient</a></li>
<li><a href="7-5-section7-5.html#section7-5"><span class="toc-section-number">7.5</span> Transformations part I: Linearizing relationships</a></li>
<li><a href="7-6-section7-6.html#section7-6"><span class="toc-section-number">7.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li><a href="7-7-section7-7.html#section7-7"><span class="toc-section-number">7.7</span> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li><a href="7-8-section7-8.html#section7-8"><span class="toc-section-number">7.8</span> Chapter summary</a></li>
<li><a href="7-9-section7-9.html#section7-9"><span class="toc-section-number">7.9</span> Summary of important R code</a></li>
<li><a href="7-10-section7-10.html#section7-10"><span class="toc-section-number">7.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="8-chapter8.html#chapter8"><span class="toc-section-number">8</span> Multiple linear regression</a><ul>
<li><a href="8-1-section8-1.html#section8-1"><span class="toc-section-number">8.1</span> Going from SLR to MLR</a></li>
<li><a href="8-2-section8-2.html#section8-2"><span class="toc-section-number">8.2</span> Validity conditions in MLR</a></li>
<li><a href="8-3-section8-3.html#section8-3"><span class="toc-section-number">8.3</span> Interpretation of MLR terms</a></li>
<li><a href="8-4-section8-4.html#section8-4"><span class="toc-section-number">8.4</span> Comparing multiple regression models</a></li>
<li><a href="8-5-section8-5.html#section8-5"><span class="toc-section-number">8.5</span> General recommendations for MLR interpretations and VIFs</a></li>
<li><a href="8-6-section8-6.html#section8-6"><span class="toc-section-number">8.6</span> MLR inference: Parameter inferences using the t-distribution</a></li>
<li><a href="8-7-section8-7.html#section8-7"><span class="toc-section-number">8.7</span> Overall F-test in multiple linear regression</a></li>
<li><a href="8-8-section8-8.html#section8-8"><span class="toc-section-number">8.8</span> Case study: First year college GPA and SATs</a></li>
<li><a href="8-9-section8-9.html#section8-9"><span class="toc-section-number">8.9</span> Different intercepts for different groups: MLR with indicator variables</a></li>
<li><a href="8-10-section8-10.html#section8-10"><span class="toc-section-number">8.10</span> Additive MLR with more than two groups: Headache example</a></li>
<li><a href="8-11-section8-11.html#section8-11"><span class="toc-section-number">8.11</span> Different slopes and different intercepts</a></li>
<li><a href="8-12-section8-12.html#section8-12"><span class="toc-section-number">8.12</span> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li><a href="8-13-section8-13.html#section8-13"><span class="toc-section-number">8.13</span> AICs for model selection</a></li>
<li><a href="8-14-section8-14.html#section8-14"><span class="toc-section-number">8.14</span> Case study: Forced expiratory volume model selection using AICs</a></li>
<li><a href="8-15-section8-15.html#section8-15"><span class="toc-section-number">8.15</span> Chapter summary</a></li>
<li><a href="8-16-section8-16.html#section8-16"><span class="toc-section-number">8.16</span> Summary of important R code</a></li>
<li><a href="8-17-section8-17.html#section8-17"><span class="toc-section-number">8.17</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="9-chapter9.html#chapter9"><span class="toc-section-number">9</span> Case studies</a><ul>
<li><a href="9-1-section9-1.html#section9-1"><span class="toc-section-number">9.1</span> Overview of material covered</a></li>
<li><a href="9-2-section9-2.html#section9-2"><span class="toc-section-number">9.2</span> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li><a href="9-3-section9-3.html#section9-3"><span class="toc-section-number">9.3</span> Ants learn to rely on more informative attributes during decision-making</a></li>
<li><a href="9-4-section9-4.html#section9-4"><span class="toc-section-number">9.4</span> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li><a href="9-5-section9-5.html#section9-5"><span class="toc-section-number">9.5</span> What do didgeridoos really do about sleepiness?</a></li>
<li><a href="9-6-section9-6.html#section9-6"><span class="toc-section-number">9.6</span> General summary</a></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="section2-4" class="section level2">
<h2><span class="header-section-number">2.4</span> Permutation testing for the two sample mean situation</h2>
<p>In any testing situation, you must define some function of the observations that
gives us a single number that addresses our question of interest. This quantity
is called a <strong><em>test statistic</em></strong>. These often take on complicated forms and
have names like <span class="math inline">\(t\)</span> or <span class="math inline">\(z\)</span> statistics that relate to their parametric

(named)
distributions so we know where to look up
<strong><em>p-values</em></strong><a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a>. In randomization settings, they can
have simpler forms because we use the data set to find the
distribution of the statistic under the null hypothesis and don’t need to rely on a
named distribution. We will label our test statistic <strong><em>T</em></strong>
(for <strong>T</strong>est statistic) unless the test statistic has a commonly
used name. Since we are interested in comparing the means of the two groups, we
can define</p>
<p><span class="math display">\[T=\bar{x}_\text{commute} - \bar{x}_\text{casual},\]</span></p>
<p>which coincidentally is what the <code>diffmean</code> function and the second coefficient from the <code>lm</code> provided us previously.
We label our <strong><em>observed test statistic</em></strong> (the one from the original data
set) as</p>
<p><span class="math display">\[T_{obs}=\bar{x}_\text{commute} - \bar{x}_\text{casual},\]</span></p>
<p>which happened to be -25.933 cm here. We will compare this result to the results
for the test statistic that we obtain from permuting the group labels. To
denote permuted results, we will add a * to the labels:</p>
<p><span class="math display">\[T^*=\bar{x}_{\text{commute}^*}-\bar{x}_{\text{casual}^*}.\]</span></p>
<p>We then compare the <span class="math inline">\(T_{obs}=\bar{x}_\text{commute} - \bar{x}_\text{casual} = -25.933\)</span>
to the distribution of results that are possible for the permuted results (<span class="math inline">\(T^*\)</span>)
which corresponds to assuming the null hypothesis is true.</p>
<p>We need to consider lots of permutations to do a permutation test.

In contrast to
your introductory statistics course where, if you did this, it was just a click
away, we are going to learn what was going on “under the hood” of the software you were using. Specifically, we
need a <strong><em>for loop</em></strong>  in R to be able to repeatedly generate the permuted data
sets and record <span class="math inline">\(T^*\)</span> for each one. Loops are a basic programming task that make
randomization methods possible as well as potentially simplifying any repetitive
computing task. To write a “for loop”, we need to choose how many times we want
to do the loop (call that <code>B</code>) and decide on a counter to keep track of where
we are at in the loops (call that <code>b</code>, which goes from 1 up to <code>B</code>). The
simplest loop just involves printing out the index, <code>print(b)</code> at each step.
This is our first use of curly braces, { and }, that are used to group the code
we want to repeatedly run as we proceed through the loop. By typing the following
code in a codechunk and then highlighting it all and hitting the run button,
R will go through the loop <em>B</em> = 5 times, printing out the counter:</p>
<pre><code>B &lt;- 5
for (b in (1:B)){
  print(b)
}</code></pre>
<p>Note that when you highlight and run the code, it will look about the same with
“+” printed after the first line to indicate that all the code is connected when
it appears in the console, looking like this:</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb78-1" title="1"><span class="op">&gt;</span><span class="st"> </span><span class="cf">for</span>(b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</a>
<a class="sourceLine" id="cb78-2" title="2"><span class="op">+</span><span class="st">   </span><span class="kw">print</span>(b)</a>
<a class="sourceLine" id="cb78-3" title="3"><span class="op">+</span><span class="st"> </span>}</a></code></pre></div>
<p>When you run these three lines of code (or compile a .Rmd file that contains this), the console will show you the following
output:</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb79-1" title="1">[<span class="dv">1</span>] <span class="dv">1</span></a>
<a class="sourceLine" id="cb79-2" title="2">[<span class="dv">1</span>] <span class="dv">2</span></a>
<a class="sourceLine" id="cb79-3" title="3">[<span class="dv">1</span>] <span class="dv">3</span></a>
<a class="sourceLine" id="cb79-4" title="4">[<span class="dv">1</span>] <span class="dv">4</span></a>
<a class="sourceLine" id="cb79-5" title="5">[<span class="dv">1</span>] <span class="dv">5</span></a></code></pre></div>
<p>Instead of printing the counter, we want to use the loop to repeatedly compute
our test statistic across <em>B</em> random permutations of the observations. The
<code>shuffle</code> function performs permutations of the group labels relative to
responses and the <code>coef(lmP)[2]</code> extracts the estimated difference in the two group means in the permuted
data set. For a single permutation, the combination of shuffling <code>Condition</code> and
finding the difference in the means, storing it in a variable called <code>Ts</code> is:</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb80-1" title="1">lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span><span class="kw">shuffle</span>(Condition), <span class="dt">data=</span>dsample)</a>
<a class="sourceLine" id="cb80-2" title="2">Ts &lt;-<span class="st"> </span><span class="kw">coef</span>(lmP)[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb80-3" title="3">Ts</a></code></pre></div>
<pre><code>## shuffle(Condition)commute 
##                      16.6</code></pre>
<p>And putting this inside the <code>print</code> function allows us to find the test
statistic under 5 different permutations easily:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb82-1" title="1">B &lt;-<span class="st"> </span><span class="dv">5</span></a>
<a class="sourceLine" id="cb82-2" title="2"><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</a>
<a class="sourceLine" id="cb82-3" title="3">  lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span><span class="kw">shuffle</span>(Condition), <span class="dt">data=</span>dsample)</a>
<a class="sourceLine" id="cb82-4" title="4">  Ts &lt;-<span class="st"> </span><span class="kw">coef</span>(lmP)[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb82-5" title="5">  <span class="kw">print</span>(Ts)</a>
<a class="sourceLine" id="cb82-6" title="6">}</a></code></pre></div>
<pre><code>## shuffle(Condition)commute 
##                      -1.8 
## shuffle(Condition)commute 
##                 -9.533333 
## shuffle(Condition)commute 
##                       3.8 
## shuffle(Condition)commute 
##                       9.4 
## shuffle(Condition)commute 
##                  26.73333</code></pre>
<p>Finally, we would like to store the values of the test statistic instead of
just printing them out on each pass through the loop. To do this, we need to
create a variable to store the results, let’s call it <code>Tstar</code>. We know that
we need to store <code>B</code> results so will create a vector<a href="#fn29" class="footnote-ref" id="fnref29"><sup>29</sup></a> of length <em>B</em>, which
contains <em>B</em> elements, full of missing values (NA) using the <code>matrix</code>  function with the <code>nrow</code> option specifying the number of elements:</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" title="1">Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</a>
<a class="sourceLine" id="cb84-2" title="2">Tstar</a></code></pre></div>
<pre><code>##      [,1]
## [1,]   NA
## [2,]   NA
## [3,]   NA
## [4,]   NA
## [5,]   NA</code></pre>
<p>Now we can run our loop <em>B</em> times and store the results in <code>Tstar</code>.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" title="1"><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</a>
<a class="sourceLine" id="cb86-2" title="2">  lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span><span class="kw">shuffle</span>(Condition), <span class="dt">data=</span>dsample)</a>
<a class="sourceLine" id="cb86-3" title="3">  Tstar[b] &lt;-<span class="st"> </span><span class="kw">coef</span>(lmP)[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb86-4" title="4">}</a>
<a class="sourceLine" id="cb86-5" title="5">Tstar</a></code></pre></div>
<pre><code>##            [,1]
## [1,]   9.533333
## [2,]   9.933333
## [3,]  23.000000
## [4,] -11.000000
## [5,] -17.400000</code></pre>
<p>Five permutations are still not enough to assess whether our <span class="math inline">\(T_{obs}\)</span>
of -25.933 is unusual and we need to do many permutations to get an accurate
assessment of the possibilities under the null hypothesis.

It is common practice
to consider something like 1,000 permutations. The <code>Tstar</code> vector when we set
<em>B</em> to be large, say <code>B=1000</code>, contains the permutation distribution  for the
selected test statistic under<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a> the null
hypothesis – what is called the <strong><em>null distribution</em></strong> of the statistic. The
null distribution is the distribution of possible values of a statistic
under the null hypothesis. We want to visualize this distribution and use it to
assess how unusual our <span class="math inline">\(T_{obs}\)</span> result of -25.933 cm was relative to all the
possibilities under permutations (under the null hypothesis). So we repeat the
loop, now with <span class="math inline">\(B=1000\)</span> and generate a histogram, density curve, and summary
statistics of the results:</p>

<div class="figure"><span id="fig:Figure2-9"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-9-1.png" alt="Histogram (left, with counts in bars) and density curve (right) of values of test statistic for B = 1,000 permutations." width="960" />
<p class="caption">
Figure 2.9: Histogram (left, with counts in bars) and density curve (right) of values of test statistic for <em>B</em> = 1,000 permutations.
</p>
</div>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" title="1">B &lt;-<span class="st"> </span><span class="dv">1000</span></a>
<a class="sourceLine" id="cb88-2" title="2">Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</a>
<a class="sourceLine" id="cb88-3" title="3"><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</a>
<a class="sourceLine" id="cb88-4" title="4">  lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span><span class="kw">shuffle</span>(Condition), <span class="dt">data=</span>dsample)</a>
<a class="sourceLine" id="cb88-5" title="5">  Tstar[b] &lt;-<span class="st"> </span><span class="kw">coef</span>(lmP)[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb88-6" title="6">}</a>
<a class="sourceLine" id="cb88-7" title="7"><span class="kw">hist</span>(Tstar, <span class="dt">label=</span>T,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">300</span>))</a>
<a class="sourceLine" id="cb88-8" title="8"><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb89-1" title="1"><span class="kw">favstats</span>(Tstar)</a></code></pre></div>
<pre><code>##        min        Q1     median  Q3      max       mean       sd    n
##  -41.26667 -10.06667 -0.3333333 8.6 37.26667 -0.5054667 13.17156 1000
##  missing
##        0</code></pre>
<p>Figure <a href="2-4-section2-4.html#fig:Figure2-9">2.9</a> contains visualizations of <span class="math inline">\(T^*\)</span> and the <code>favstats</code>
summary provides the related numerical summaries. Our observed <span class="math inline">\(T_{obs}\)</span>
of -25.933 seems somewhat unusual relative to these results with only
9 <span class="math inline">\(T^*\)</span> values smaller than -30 based on the
histogram. We need to make more specific comparisons of the permuted results
versus our observed result to be able to clearly decide whether our observed
result is really unusual.</p>
<p>To make the comparisons more concrete, first we can enhance the previous graphs
by adding the value of the test statistic from the real data set, as shown in
Figure <a href="2-4-section2-4.html#fig:Figure2-10">2.10</a>, using the <code>abline</code>  function to draw a vertical
line at our <span class="math inline">\(T_{obs}\)</span> value specified in the <code>v</code> (for vertical) option.</p>

<div class="figure"><span id="fig:Figure2-10"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-10-1.png" alt="Histogram (left) and density curve (right) of values of test statistic for 1,000 permutations with bold vertical line for value of observed test statistic." width="960" />
<p class="caption">
Figure 2.10: Histogram (left) and density curve (right) of values of test statistic for 1,000 permutations with bold vertical line for value of observed test statistic.
</p>
</div>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" title="1">Tobs &lt;-<span class="st"> </span><span class="fl">-25.933</span></a>
<a class="sourceLine" id="cb91-2" title="2"><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)</a>
<a class="sourceLine" id="cb91-3" title="3"><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a>
<a class="sourceLine" id="cb91-4" title="4"><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</a>
<a class="sourceLine" id="cb91-5" title="5"><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<p>Second, we can calculate the exact number of permuted results that were as
small or smaller
than what we observed. To calculate the proportion of the 1,000 values that were
as small or smaller than what we observed, we will use the <code>pdata</code> function.

To use this
function, we need to provide the distribution of values to compare to the cut-off
(<code>Tstar</code>), the cut-off point (<code>Tobs</code>), and whether we want calculate the
proportion that are below (left of) or above (right of) the cut-off
(<code>lower.tail=T</code> option provides the proportion of values to the left of (below) the cutoff
of interest).</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" title="1"><span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>T)[[<span class="dv">1</span>]]</a></code></pre></div>
<pre><code>## [1] 0.027</code></pre>
<p>The proportion of 0.027 tells us that 27 of the 1,000 permuted results
(2.7%) were as small or smaller than what we observed. This type of
work is how we can
generate <strong><em>p-values</em></strong> using permutation distributions.


P-values,

as you should
remember, are the probability of getting a result as extreme as or more extreme
than what we observed, <span class="math inline">\(\underline{\text{given that the null is true}}\)</span>. Finding
only 27
permutations of 1,000 that were as small or smaller than our observed result suggests that it
is hard to find a result like what we observed if there really were no difference,
although it is not impossible.</p>
<p>When testing hypotheses for two groups, there are two types of alternative
hypotheses, one-sided or two-sided. <strong><em>One-sided tests</em></strong> involve only considering
differences in one-direction (like <span class="math inline">\(\mu_1 &gt; \mu_2\)</span>) and are performed when
researchers can decide <strong><em>a priori</em></strong><a href="#fn31" class="footnote-ref" id="fnref31"><sup>31</sup></a> which group should have a larger mean
if there is going to be any sort of difference. In this situation, we did not
know enough about the potential impacts of the outfits to know which group should
be larger than the other so should do a two-sided test. It is important to
remember that you can’t look at the responses to decide on the hypotheses. It is
often safer and more <strong><em>conservative</em></strong><a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a>   to start with a
<strong><em>two-sided alternative</em></strong> (<span class="math inline">\(\mathbf{H_A: \mu_1 \ne \mu_2}\)</span>). To do a 2-sided
test, find the area smaller than what we observed as above (or larger if the test statistic had been positive). We also need to add
the area in the other tail (here the right tail) similar to what we observed in the
right tail. Some statisticians suggest doubling the area in one tail but we will collect
information on the number that were as or more extreme than the same
value in the other
tail<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a>. In other words, we count the proportion below -25.933 and over 25.933. So
we need to find how many of the permuted results were larger than or equal
to 25.933 cm
to add to our previous proportion. Using <code>pdata</code> with <code>-Tobs</code> as the cut-off
and <code>lower.tail =F</code> provides this result:
</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb94-1" title="1"><span class="kw">pdata</span>(Tstar, <span class="op">-</span>Tobs, <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</a></code></pre></div>
<pre><code>## [1] 0.017</code></pre>
<p>So the p-value to test our null hypothesis of no difference in the true means
between the groups is 0.027 + 0.017, providing a p-value of 0.044.
Figure <a href="2-4-section2-4.html#fig:Figure2-11">2.11</a> shows both cut-offs on the histogram and density curve.</p>

<div class="figure"><span id="fig:Figure2-11"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-11-1.png" alt="Histogram and density curve of values of test statistic for 1,000 permutations with bold lines for value of observed test statistic (-25.933) and its opposite value (25.933) required for performing the two-sided test." width="960" />
<p class="caption">
Figure 2.11: Histogram and density curve of values of test statistic for 1,000 permutations with bold lines for value of observed test statistic (-25.933) and its opposite value (25.933) required for performing the two-sided test.
</p>
</div>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" title="1"><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)</a>
<a class="sourceLine" id="cb96-2" title="2"><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a>
<a class="sourceLine" id="cb96-3" title="3"><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</a>
<a class="sourceLine" id="cb96-4" title="4"><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<p>In general, the <strong><em>one-sided test p-value</em></strong>

is the proportion of the
permuted results that are as extreme or more extreme than observed in the
direction of the <em>alternative</em>
hypothesis (lower or upper tail, remembering that this also depends on the
direction of the difference taken). For the two-sided test, the p-value

is the
proportion of the permuted results that are <em>less than or equal to the negative
version of the observed statistic and greater than or equal to the positive
version of the observed statistic</em>. Using absolute
values (| |), we can simplify this: the <strong><em>two-sided p-value</em></strong> is the
<em>proportion of the |permuted statistics| that are as large or larger than
|observed statistic|</em>.
This will always work and finds areas in both tails regardless of whether the
observed statistic is positive or negative. In R, the <code>abs</code> function  provides the
<strong><em>absolute value</em></strong> and we can again use <code>pdata</code> to find our p-value in one line
of code:
</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb97-1" title="1"><span class="kw">pdata</span>(<span class="kw">abs</span>(Tstar), <span class="kw">abs</span>(Tobs), <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</a></code></pre></div>
<pre><code>## [1] 0.044</code></pre>
<p>We will encourage you to think through what might constitute strong evidence
against your null hypotheses and then discuss how strong you feel the evidence
is against the null hypothesis in the p-value that you obtained. Basically,
p-values present a measure of evidence against the null hypothesis,

with smaller
values presenting more evidence against the null. They range from 0 to 1 and you
should interpret them on a graded scale from strong evidence (close to 0) to little evidence to no
evidence (1). We will discuss the use of a fixed <strong><em>significance level</em></strong>
below as it is still commonly used in many fields and is necessary to discuss to think
about the theory of hypothesis testing, but, for the moment,
we can conclude that there is moderate evidence against the null hypothesis
presented by having a p-value of 0.044 because our observed result is somewhat
rare relative to what we would expect if the null hypothesis was true. And so
we might conclude (in the direction
of the alternative) that there is a difference in the population means in the
two groups, but that depends on what you think about how unusual that result was.</p>
<p>Before we move on, let’s note some interesting features of the permutation
distribution of the difference in the sample means shown in
Figure <a href="2-4-section2-4.html#fig:Figure2-11">2.11</a>. </p>
<ol style="list-style-type: decimal">
<li><p>It is basically centered at 0. Since we are performing permutations assuming
the null model is true, we are assuming that <span class="math inline">\(\mu_1 = \mu_2\)</span> which implies that
<span class="math inline">\(\mu_1 - \mu_2 = 0\)</span>. This also suggests that 0 should be the center of the
permutation distribution and it was.</p></li>
<li><p>It is approximately normally distributed. This is due to the <strong><em>Central Limit Theorem</em></strong><a href="#fn34" class="footnote-ref" id="fnref34"><sup>34</sup></a>, where the
<strong><em>sampling distribution</em></strong> (distribution of all possible results for samples
of this size) of the difference in sample means (<span class="math inline">\(\bar{x}_1 - \bar{x}_2\)</span>) becomes
more normally distributed as the sample sizes increase. With 15
observations in each group, we have no guarantee to have a relatively normal looking
distribution of the difference in the sample means but with the distributions of the original observations looking somewhat normally distributed, the sampling distribution of the sample means likely will look fairly normal. This result will allow us to
use a parametric

method to approximate this sampling distribution under the null
model if some assumptions are met, as we’ll discuss below.</p></li>
<li><p>Our observed difference in the sample means (-25.933) is a fairly unusual
result relative to the rest of these results but there are some permuted data
sets that produce more extreme differences in the sample means. When the
observed differences are really large, we may not see any permuted results
that are as extreme as what we observed. When <code>pdata</code> gives you 0, the p-value

should be reported to be smaller than 0.0001 (<strong>not 0!</strong>) if <em>B</em> is 1,000 since it happened
in less than 1 in 1,000 tries but does occur once – in the actual data set.</p></li>
<li><p>Since our null model is not specific about the direction of the difference,
considering a result like ours but in the other direction (25.933 cm) needs to
be included. The observed result seems to put about the same area in both tails
of the distribution but it is not exactly the same. The small difference in the
tails is a useful aspect of this approach compared to the parametric method
discussed below as it accounts for potential asymmetry in the sampling distribution.</p></li>
</ol>
<p>Earlier, we decided that the p-value provided moderate evidence against
the null hypothesis. You should use your own judgment about whether the p-value obtain is sufficiently small to conclude that you think the null hypothesis is wrong. Remembering
that the p-value is the probability

you would observe a result like you did (or more extreme), assuming the null
hypothesis is true; this tells you that the smaller the p-value is, the more
evidence you have against the null. Figure <a href="2-4-section2-4.html#fig:FigurePValStr">2.12</a> provides a
diagram of some suggestions for the graded p-value interpretation that you can
use. The next section provides a more formal
review of the hypothesis testing

infrastructure, terminology, and some of
things that can happen when testing hypotheses. P-values have been (validly)

criticized for the inability of studies to be reproduced, for the bias in
publications to only include studies that have small p-values, and for the lack of
thought that often accompanies using a fixed significance level to make decisions (and only focusing on that decision). To alleviate
some of these criticisms, we recommend reporting the strength of evidence of the
result based on the p-value and also reporting and discussing the size of the
estimated results (with a measure of precision of the estimated difference). We will explore the implications of how p-values are used in scientific research in Section [SECTIONREFERENCE].</p>

<div class="figure"><span id="fig:FigurePValStr"></span>
<img src="chapter2_files/pvalueStrengths.png" alt="Graphic suggesting potential interpretations of strength of evidence based on gradient of p-values." width="800" />
<p class="caption">
Figure 2.12: Graphic suggesting potential interpretations of strength of evidence based on gradient of p-values.
</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="28">
<li id="fn28"><p>P-values  are the
probability of obtaining a result as extreme as or more extreme than we observed
given that the null hypothesis is true.<a href="2-4-section2-4.html#fnref28" class="footnote-back">↩</a></p></li>
<li id="fn29"><p>In statistics, vectors are one dimensional lists of numeric elements – basically a column from a matrix or our tibble.<a href="2-4-section2-4.html#fnref29" class="footnote-back">↩</a></p></li>
<li id="fn30"><p>We often say
“under” in statistics and we mean “given that the following is true”.<a href="2-4-section2-4.html#fnref30" class="footnote-back">↩</a></p></li>
<li id="fn31"><p>This is a fancy way of saying “in advance”,
here in advance of seeing the observations.<a href="2-4-section2-4.html#fnref31" class="footnote-back">↩</a></p></li>
<li id="fn32"><p>Statistically, a conservative method is
one that provides less chance of rejecting the null hypothesis in comparison to
some other method or less than some pre-defined standard. A liberal method provides higher rates of false rejections.<a href="2-4-section2-4.html#fnref32" class="footnote-back">↩</a></p></li>
<li id="fn33"><p>Both approaches are reasonable. By using both tails of the distribution we can incorporate potential differences in shape in both tails of the permutation distribution.<a href="2-4-section2-4.html#fnref33" class="footnote-back">↩</a></p></li>
<li id="fn34"><p>We’ll leave the discussion of the CLT to your previous statistics coursework
or an internet search. For this material, just remember that it has something to do with distributions of statistics
looking more normal as the sample size increases.<a href="2-4-section2-4.html#fnref34" class="footnote-back">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="2-3-section2-3.html"><button class="btn btn-default">Previous</button></a>
<a href="2-5-section2-5.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
