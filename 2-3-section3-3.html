<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="2.3 One-Way ANOVA Sums of Squares, Mean Squares, and F-test | Intermediate Statistics with R" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="gpeterson406/Greenwood_Book" />

<meta name="author" content="Mark C Greenwood" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="2.3 One-Way ANOVA Sums of Squares, Mean Squares, and F-test | Intermediate Statistics with R">

<title>2.3 One-Way ANOVA Sums of Squares, Mean Squares, and F-test | Intermediate Statistics with R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li class="has-sub"><a href="acknowledgments.html#acknowledgments">Acknowledgments</a><ul>
<li><a href="0-1-section1-5.html#section1-5"><span class="toc-section-number">0.1</span> Summary of important R code</a></li>
</ul></li>
<li class="has-sub"><a href="1-chapter2.html#chapter2"><span class="toc-section-number">1</span> (R)e-Introduction to statistics</a><ul>
<li><a href="1-1-section2-1.html#section2-1"><span class="toc-section-number">1.1</span> Histograms, boxplots, and density curves</a></li>
<li><a href="1-2-section2-2.html#section2-2"><span class="toc-section-number">1.2</span> Pirate-plots</a></li>
<li><a href="1-3-section2-3.html#section2-3"><span class="toc-section-number">1.3</span> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li><a href="1-4-section2-4.html#section2-4"><span class="toc-section-number">1.4</span> Permutation testing for the two sample mean situation</a></li>
<li><a href="1-5-section2-5.html#section2-5"><span class="toc-section-number">1.5</span> Hypothesis testing (general)</a></li>
<li><a href="1-6-section2-6.html#section2-6"><span class="toc-section-number">1.6</span> Connecting randomization (nonparametric) and parametric tests</a></li>
<li><a href="1-7-section2-7.html#section2-7"><span class="toc-section-number">1.7</span> Second example of permutation tests</a></li>
<li><a href="1-8-section2-8.html#section2-8"><span class="toc-section-number">1.8</span> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li><a href="1-9-section2-9.html#section2-9"><span class="toc-section-number">1.9</span> Confidence intervals and bootstrapping</a></li>
<li><a href="1-10-section2-10.html#section2-10"><span class="toc-section-number">1.10</span> Bootstrap confidence intervals for difference in GPAs</a></li>
<li><a href="1-11-section2-11.html#section2-11"><span class="toc-section-number">1.11</span> Chapter summary</a></li>
<li><a href="1-12-section2-12.html#section2-12"><span class="toc-section-number">1.12</span> Summary of important R code</a></li>
<li><a href="1-13-section2-13.html#section2-13"><span class="toc-section-number">1.13</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="2-chapter3.html#chapter3"><span class="toc-section-number">2</span> One-Way ANOVA</a><ul>
<li><a href="2-1-section3-1.html#section3-1"><span class="toc-section-number">2.1</span> Situation</a></li>
<li><a href="2-2-section3-2.html#section3-2"><span class="toc-section-number">2.2</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li><a href="2-3-section3-3.html#section3-3"><span class="toc-section-number">2.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li><a href="2-4-section3-4.html#section3-4"><span class="toc-section-number">2.4</span> ANOVA model diagnostics including QQ-plots</a></li>
<li><a href="2-5-section3-5.html#section3-5"><span class="toc-section-number">2.5</span> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li><a href="2-6-section3-6.html#section3-6"><span class="toc-section-number">2.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li><a href="2-7-section3-7.html#section3-7"><span class="toc-section-number">2.7</span> Pair-wise comparisons for the Overtake data</a></li>
<li><a href="2-8-section3-8.html#section3-8"><span class="toc-section-number">2.8</span> Chapter summary</a></li>
<li><a href="2-9-section3-9.html#section3-9"><span class="toc-section-number">2.9</span> Summary of important R code</a></li>
<li><a href="2-10-section3-10.html#section3-10"><span class="toc-section-number">2.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="3-chapter4.html#chapter4"><span class="toc-section-number">3</span> Two-Way ANOVA</a><ul>
<li><a href="3-1-section4-1.html#section4-1"><span class="toc-section-number">3.1</span> Situation</a></li>
<li><a href="3-2-section4-2.html#section4-2"><span class="toc-section-number">3.2</span> Designing a two-way experiment and visualizing results</a></li>
<li><a href="3-3-section4-3.html#section4-3"><span class="toc-section-number">3.3</span> Two-Way ANOVA models and hypothesis tests</a></li>
<li><a href="3-4-section4-4.html#section4-4"><span class="toc-section-number">3.4</span> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li><a href="3-5-section4-5.html#section4-5"><span class="toc-section-number">3.5</span> Observational study example: The Psychology of Debt</a></li>
<li><a href="3-6-section4-6.html#section4-6"><span class="toc-section-number">3.6</span> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li><a href="3-7-section4-7.html#section4-7"><span class="toc-section-number">3.7</span> Chapter summary</a></li>
<li><a href="3-8-section4-8.html#section4-8"><span class="toc-section-number">3.8</span> Summary of important R code</a></li>
<li><a href="3-9-section4-9.html#section4-9"><span class="toc-section-number">3.9</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="4-chapter5.html#chapter5"><span class="toc-section-number">4</span> Chi-square tests</a><ul>
<li><a href="4-1-section5-1.html#section5-1"><span class="toc-section-number">4.1</span> Situation, contingency tables, and tableplots</a></li>
<li><a href="4-2-section5-2.html#section5-2"><span class="toc-section-number">4.2</span> Homogeneity test hypotheses</a></li>
<li><a href="4-3-section5-3.html#section5-3"><span class="toc-section-number">4.3</span> Independence test hypotheses</a></li>
<li><a href="4-4-section5-4.html#section5-4"><span class="toc-section-number">4.4</span> Models for R by C tables</a></li>
<li><a href="4-5-section5-5.html#section5-5"><span class="toc-section-number">4.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="4-6-section5-6.html#section5-6"><span class="toc-section-number">4.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="4-7-section5-7.html#section5-7"><span class="toc-section-number">4.7</span> Examining residuals for the source of differences</a></li>
<li><a href="4-8-section5-8.html#section5-8"><span class="toc-section-number">4.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li><a href="4-9-section5-9.html#section5-9"><span class="toc-section-number">4.9</span> Political party and voting results: Complete analysis</a></li>
<li><a href="4-10-section5-10.html#section5-10"><span class="toc-section-number">4.10</span> Is cheating and lying related in students?</a></li>
<li><a href="4-11-section5-11.html#section5-11"><span class="toc-section-number">4.11</span> Analyzing a stratified random sample of California schools</a></li>
<li><a href="4-12-section5-12.html#section5-12"><span class="toc-section-number">4.12</span> Chapter summary</a></li>
<li><a href="4-13-section5-13.html#section5-13"><span class="toc-section-number">4.13</span> Summary of important R commands</a></li>
<li><a href="4-14-section5-14.html#section5-14"><span class="toc-section-number">4.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="5-chapter6.html#chapter6"><span class="toc-section-number">5</span> Correlation and Simple Linear Regression</a><ul>
<li><a href="5-1-section6-1.html#section6-1"><span class="toc-section-number">5.1</span> Relationships between two quantitative variables</a></li>
<li><a href="5-2-section6-2.html#section6-2"><span class="toc-section-number">5.2</span> Estimating the correlation coefficient</a></li>
<li><a href="5-3-section6-3.html#section6-3"><span class="toc-section-number">5.3</span> Relationships between variables by groups</a></li>
<li><a href="5-4-section6-4.html#section6-4"><span class="toc-section-number">5.4</span> Inference for the correlation coefficient</a></li>
<li><a href="5-5-section6-5.html#section6-5"><span class="toc-section-number">5.5</span> Are tree diameters related to tree heights?</a></li>
<li><a href="5-6-section6-6.html#section6-6"><span class="toc-section-number">5.6</span> Describing relationships with a regression model</a></li>
<li><a href="5-7-section6-7.html#section6-7"><span class="toc-section-number">5.7</span> Least Squares Estimation</a></li>
<li><a href="5-8-section6-8.html#section6-8"><span class="toc-section-number">5.8</span> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li><a href="5-9-section6-9.html#section6-9"><span class="toc-section-number">5.9</span> Outliers: leverage and influence</a></li>
<li><a href="5-10-section6-10.html#section6-10"><span class="toc-section-number">5.10</span> Residual diagnostics – setting the stage for inference</a></li>
<li><a href="5-11-section6-11.html#section6-11"><span class="toc-section-number">5.11</span> Old Faithful discharge and waiting times</a></li>
<li><a href="5-12-section6-12.html#section6-12"><span class="toc-section-number">5.12</span> Chapter summary</a></li>
<li><a href="5-13-section6-13.html#section6-13"><span class="toc-section-number">5.13</span> Summary of important R code</a></li>
<li><a href="5-14-section6-14.html#section6-14"><span class="toc-section-number">5.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="6-chapter7.html#chapter7"><span class="toc-section-number">6</span> Simple linear regression inference</a><ul>
<li><a href="6-1-section7-1.html#section7-1"><span class="toc-section-number">6.1</span> Model</a></li>
<li><a href="6-2-section7-2.html#section7-2"><span class="toc-section-number">6.2</span> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li><a href="6-3-section7-3.html#section7-3"><span class="toc-section-number">6.3</span> Bozeman temperature trend</a></li>
<li><a href="6-4-section7-4.html#section7-4"><span class="toc-section-number">6.4</span> Randomization-based inferences for the slope coefficient</a></li>
<li><a href="6-5-section7-5.html#section7-5"><span class="toc-section-number">6.5</span> Transformations part I: Linearizing relationships</a></li>
<li><a href="6-6-section7-6.html#section7-6"><span class="toc-section-number">6.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li><a href="6-7-section7-7.html#section7-7"><span class="toc-section-number">6.7</span> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li><a href="6-8-section7-8.html#section7-8"><span class="toc-section-number">6.8</span> Chapter summary</a></li>
<li><a href="6-9-section7-9.html#section7-9"><span class="toc-section-number">6.9</span> Summary of important R code</a></li>
<li><a href="6-10-section7-10.html#section7-10"><span class="toc-section-number">6.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="7-chapter8.html#chapter8"><span class="toc-section-number">7</span> Multiple linear regression</a><ul>
<li><a href="7-1-section8-1.html#section8-1"><span class="toc-section-number">7.1</span> Going from SLR to MLR</a></li>
<li><a href="7-2-section8-2.html#section8-2"><span class="toc-section-number">7.2</span> Validity conditions in MLR</a></li>
<li><a href="7-3-section8-3.html#section8-3"><span class="toc-section-number">7.3</span> Interpretation of MLR terms</a></li>
<li><a href="7-4-section8-4.html#section8-4"><span class="toc-section-number">7.4</span> Comparing multiple regression models</a></li>
<li><a href="7-5-section8-5.html#section8-5"><span class="toc-section-number">7.5</span> General recommendations for MLR interpretations and VIFs</a></li>
<li><a href="7-6-section8-6.html#section8-6"><span class="toc-section-number">7.6</span> MLR inference: Parameter inferences using the t-distribution</a></li>
<li><a href="7-7-section8-7.html#section8-7"><span class="toc-section-number">7.7</span> Overall F-test in multiple linear regression</a></li>
<li><a href="7-8-section8-8.html#section8-8"><span class="toc-section-number">7.8</span> Case study: First year college GPA and SATs</a></li>
<li><a href="7-9-section8-9.html#section8-9"><span class="toc-section-number">7.9</span> Different intercepts for different groups: MLR with indicator variables</a></li>
<li><a href="7-10-section8-10.html#section8-10"><span class="toc-section-number">7.10</span> Additive MLR with more than two groups: Headache example</a></li>
<li><a href="7-11-section8-11.html#section8-11"><span class="toc-section-number">7.11</span> Different slopes and different intercepts</a></li>
<li><a href="7-12-section8-12.html#section8-12"><span class="toc-section-number">7.12</span> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li><a href="7-13-section8-13.html#section8-13"><span class="toc-section-number">7.13</span> AICs for model selection</a></li>
<li><a href="7-14-section8-14.html#section8-14"><span class="toc-section-number">7.14</span> Case study: Forced expiratory volume model selection using AICs</a></li>
<li><a href="7-15-section8-15.html#section8-15"><span class="toc-section-number">7.15</span> Chapter summary</a></li>
<li><a href="7-16-section8-16.html#section8-16"><span class="toc-section-number">7.16</span> Summary of important R code</a></li>
<li><a href="7-17-section8-17.html#section8-17"><span class="toc-section-number">7.17</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="8-chapter9.html#chapter9"><span class="toc-section-number">8</span> Case studies</a><ul>
<li><a href="8-1-section9-1.html#section9-1"><span class="toc-section-number">8.1</span> Overview of material covered</a></li>
<li><a href="8-2-section9-2.html#section9-2"><span class="toc-section-number">8.2</span> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li><a href="8-3-section9-3.html#section9-3"><span class="toc-section-number">8.3</span> Ants learn to rely on more informative attributes during decision-making</a></li>
<li><a href="8-4-section9-4.html#section9-4"><span class="toc-section-number">8.4</span> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li><a href="8-5-section9-5.html#section9-5"><span class="toc-section-number">8.5</span> What do didgeridoos really do about sleepiness?</a></li>
<li><a href="8-6-section9-6.html#section9-6"><span class="toc-section-number">8.6</span> General summary</a></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="section3-3" class="section level2">
<h2><span class="header-section-number">2.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</h2>
<p>The previous discussion showed two ways of parameterizing models for the
One-Way ANOVA model and getting estimates from output but still hasn’t
addressed how to assess evidence related to whether the observed differences
in the means among the groups is “real”. In this section, we develop what is
called the <strong><em>ANOVA F-test</em></strong>

that provides a method of aggregating the
differences among the means of 2 or more groups and testing (assessing evidence against) our null hypothesis
of no difference in the means vs the alternative. In order to develop the test,
some additional notation is needed. The sample size in each group is denoted
<span class="math inline">\(n_j\)</span> and the total sample size is
<span class="math inline">\(\boldsymbol{N=\Sigma n_j = n_1 + n_2 + \ldots + n_J}\)</span> where <span class="math inline">\(\Sigma\)</span>
(capital sigma) means “add up over whatever follows”. An estimated
<strong><em>residual</em></strong> (<span class="math inline">\(e_{ij}\)</span>) is the difference between an observation, <span class="math inline">\(y_{ij}\)</span>,
and the model estimate, <span class="math inline">\(\hat{y}_{ij} = \hat{\mu}_j\)</span>, for that observation,
<span class="math inline">\(y_{ij}-\hat{y}_{ij} = e_{ij}\)</span>. It is basically what is left over that the mean
part of the model (<span class="math inline">\(\hat{\mu}_{j}\)</span>) does not explain. It is also a window
into how “good” the model might be because it reflects what the model was unable to explain. </p>
<p>Consider the four different fake results for a situation with four groups (<span class="math inline">\(J=4\)</span>)
displayed in Figure <a href="2-3-section3-3.html#fig:Figure3-3">1.30</a>. Which of the different results shows
the most and least evidence of differences in the means? In trying to answer
this, think about both how different the means are (obviously important) and
how variable the results are around the mean. These situations were created to
have the same means in Scenarios 1 and 2 as well as matching means in Scenarios
3 and 4. In Scenarios 1 and 2, the differences in the means is smaller than in
the other two results. But Scenario 2 should provide more evidence of what
little difference is present than Scenario 1 because it has less variability
around the means. The best situation for finding group differences here is
Scenario 4 since it has the largest difference in the means and the least
variability around those means. Our test statistic somehow needs to allow a
comparison of the variability in the means to the overall variability to help
us get results that reflect that Scenario 4 has the strongest evidence of a
difference (most variability in the means and least variability around those means) and Scenario 1 would have the least evidence(least variability in the means and most variability around those means).</p>
<p>(ref:fig3-3) Demonstration of different amounts of difference in means relative
to variability. Scenarios have the same means in rows and same variance around means
in columns of plot. Confidence intervals not reported in the pirate-plots.</p>
<div class="figure"><span id="fig:Figure3-3"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-3-1.png" alt="(ref:fig3-3)" width="576" />
<p class="caption">
Figure 1.30: (ref:fig3-3)
</p>
</div>
<p>The statistic that allows the comparison of relative amounts of variation is called
the <strong><em>ANOVA F-statistic</em></strong>. It is developed using <strong><em>sums of squares</em></strong>

which are measures of total variation like those that are used in the numerator of the
standard deviation (<span class="math inline">\(\Sigma_1^N(y_i-\bar{y})^2\)</span>) that took all the observations,
subtracted the mean, squared the differences, and then added up the results
over all the observations to generate a measure of total variability. With
multiple groups, we will focus on decomposing that total variability
(<strong><em>Total Sums of Squares</em></strong>) into variability among the means (we’ll call this
<strong><em>Explanatory Variable</em></strong> <span class="math inline">\(\mathbf{A}\textbf{&#39;s}\)</span> <strong><em>Sums of Squares</em></strong>) and
variability in the residuals 
or errors (<strong><em>Error Sums of Squares</em></strong>). We define each of these quantities in
the One-Way ANOVA situation as follows:</p>
<ul>
<li><p><span class="math inline">\(\textbf{SS}_{\textbf{Total}} =\)</span> Total Sums of Squares
<span class="math inline">\(= \Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(y_{ij}-\bar{\bar{y}})^2\)</span></p>
<ul>
<li><p>This is the total variation in the responses around the <strong><em>grand mean</em></strong> (<span class="math inline">\(\bar{\bar{y}}\)</span>, the estimated mean for all the
observations and available from the mean-only model).</p></li>
<li><p>By summing over all <span class="math inline">\(n_j\)</span> observations in each group, <span class="math inline">\(\Sigma^{n_j}_{i=1}(\ )\)</span>,
and then adding those results up across the groups, <span class="math inline">\(\Sigma^J_{j=1}(\ )\)</span>,
we accumulate the variation across all <span class="math inline">\(N\)</span> observations.</p></li>
<li><p>Note: this is the residual variation if the null model is used, so there
is no further decomposition possible for that model.</p></li>
<li><p>This is also equivalent to the numerator of the sample variance,
<span class="math inline">\(\Sigma^{N}_{1}(y_{i}-\bar{y})^2\)</span> which is what you get when you ignore
the information on the potential differences in the groups.</p></li>
</ul></li>
<li><p><span class="math inline">\(\textbf{SS}_{\textbf{A}} =\)</span> Explanatory Variable <em>A</em>’s Sums of Squares
<span class="math inline">\(=\Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(\bar{y}_{j}-\bar{\bar{y}})^2 =\Sigma^J_{j=1}n_j(\bar{y}_{j}-\bar{\bar{y}})^2\)</span></p>
<ul>
<li><p>This is the variation in the group means around the grand mean based on
the explanatory variable <span class="math inline">\(A\)</span>.</p></li>
<li><p>This is also called sums of squares for the treatment, regression, or model.</p></li>
</ul></li>
<li><p><span class="math inline">\(\textbf{SS}_\textbf{E} =\)</span> Error (Residual) Sums of Squares
<span class="math inline">\(=\Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(y_{ij}-\bar{y}_j)^2 = \Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(e_{ij})^2\)</span></p>
<ul>
<li><p>This is the variation in the responses around the group means.</p></li>
<li><p>Also called the sums of squares for the residuals, especially when using the second
version of the formula, which shows that it is just the squared residuals added
up across all the observations.</p></li>
</ul></li>
</ul>
<p>The possibly surprising result given the mass of notation just presented is that
the total sums of squares is <strong>ALWAYS</strong> equal to the sum of explanatory variable
<span class="math inline">\(A\text{&#39;s}\)</span> sum of squares and the error sums of squares,</p>
<p><span class="math display">\[\textbf{SS}_{\textbf{Total}} \mathbf{=} \textbf{SS}_\textbf{A} \mathbf{+} \textbf{SS}_\textbf{E}.\]</span></p>
<p>This result is called the <strong><em>sums of squares decomposition formula</em></strong>.

The equality implies that if the <span class="math inline">\(\textbf{SS}_\textbf{A}\)</span> goes up, then the
<span class="math inline">\(\textbf{SS}_\textbf{E}\)</span> must go down if <span class="math inline">\(\textbf{SS}_{\textbf{Total}}\)</span> remains
the same. We use these results to build our test statistic and organize this information in
what is called an <strong><em>ANOVA table</em></strong>.

The ANOVA table is generated using the
<code>anova</code> function applied to the reference-coded model, <code>lm2</code>:

</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb221-1" title="1">lm2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>dd)</a>
<a class="sourceLine" id="cb221-2" title="2"><span class="kw">anova</span>(lm2)</a></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: Distance
##             Df  Sum Sq Mean Sq F value    Pr(&gt;F)
## Condition    6   34948  5824.7  6.5081 7.392e-07
## Residuals 5683 5086298   895.0</code></pre>
<p>Note that the ANOVA table has a row labelled <code>Condition</code>, which contains information
for the grouping variable (we’ll generally refer to this as explanatory variable
<span class="math inline">\(A\)</span> but here it is the outfit group that was randomly assigned), and a row
labeled <code>Residuals</code>, which is synonymous with “Error”. The Sums of Squares
(SS) are available in the <code>Sum Sq</code> column. It doesn’t show a row for “Total” but
the <span class="math inline">\(\textbf{SS}_{\textbf{Total}} \mathbf{=} \textbf{SS}_\textbf{A} \mathbf{+} \textbf{SS}_\textbf{E} = 5,121,246\)</span>.</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb223-1" title="1"><span class="dv">34948</span> <span class="op">+</span><span class="st"> </span><span class="dv">5086298</span></a></code></pre></div>
<pre><code>## [1] 5121246</code></pre>
<p>(ref:fig3-4) Plot of means and 95% confidence intervals for the three groups
for the real overtake data (a) and three different permutations of the outfit group labels
to the same responses in (b), (c), and (d). Note that <code>SSTotal</code> is always the same
but the different amounts of variation associated with the means (<code>SSA</code>) or the
errors (<code>SSE</code>) changes in permutation.</p>
<div class="figure"><span id="fig:Figure3-4"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-4-1.png" alt="(ref:fig3-4)" width="768" />
<p class="caption">
Figure 1.31: (ref:fig3-4)
</p>
</div>
<p>It may be easiest to understand the <em>sums of squares decomposition</em> by connecting
it to our permutation ideas.


In a permutation situation, the total variation
(<span class="math inline">\(SS_\text{Total}\)</span>) cannot change – it is the same responses varying
around the same grand mean. However, the amount of variation attributed to variation
among the means and in the residuals can change if we change which observations go
with which group. In Figure <a href="2-3-section3-3.html#fig:Figure3-4">1.31</a> (panel a), the means, sums of
squares, and 95% confidence intervals for each mean are displayed for the seven
groups from the original overtake data. Three permuted versions
of the data set are summarized in panels (b), (c), and (d). The <span class="math inline">\(\text{SS}_A\)</span> is 3.494810^{4}
in the real data set and between 857 and 4539 in the permuted data sets.
If you had
to pick among the plots for the one with the most evidence of a difference in the
means, you hopefully would pick panel (a). This visual “unusualness” suggests
that this observed result is unusual relative to the possibilities under
permutations, which are, again, the possibilities tied to having the null
hypothesis being true. But note that the differences here are not that great
between these three permuted data sets and the real one. It is likely that at
least some might have selected panel (d) as also looking like it shows
some evidence of differences, although the variation in the means in the real data set is clearly more pronounced than in this or the other permutations.</p>
<p>One way to think about <span class="math inline">\(\textbf{SS}_\textbf{A}\)</span> is that it is a function that
converts the variation in the group means into a single value. This makes it a
reasonable test statistic in a permutation testing context.

By comparing the
observed <span class="math inline">\(\text{SS}_A =\)</span> 3.494810^{4} to the permutation results of
857, 3828, and 4539 we see
that the observed result is much more extreme than the three alternate versions.
In contrast to our previous test statistics where positive and negative
differences were possible, <span class="math inline">\(\text{SS}_A\)</span> is always positive with a value of 0
corresponding to no variation in the means. The larger the <span class="math inline">\(\text{SS}_A\)</span>, the more
variation there is in the means. The permutation p-value for the alternative
hypothesis of <strong>some</strong> (not of greater or less than!) difference in the true
means of the groups will involve counting the number of permuted <span class="math inline">\(SS_A^*\)</span> results
that are as large or larger than what we observed.
</p>
<p>To do a permutation test,

we need to be able to calculate and extract the
<span class="math inline">\(\text{SS}_A\)</span> value. In the ANOVA table, it is the second number in the first row;
we can use the bracket, <code>[,]</code>, referencing to extract that
number from the ANOVA table that <code>anova</code> produces with
<code>anova(lm(Distance~Condition, data=dd))[1, 2]</code>. We’ll store the observed value
of <span class="math inline">\(\text{SS}_A\)</span> in <code>Tobs</code>, reusing some ideas from Chapter <a href="1-chapter2.html#chapter2">1</a>.
</p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb225-1" title="1">Tobs &lt;-<span class="st"> </span><span class="kw">anova</span>(<span class="kw">lm</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>dd))[<span class="dv">1</span>,<span class="dv">2</span>]; Tobs</a></code></pre></div>
<pre><code>## [1] 34948.43</code></pre>
<p>The following code performs the permutations <code>B=1,000</code> times using the
<code>shuffle</code> function, builds up a vector of results in <code>Tobs</code>, and then makes
a plot of the resulting permutation distribution:</p>
<p>(ref:fig3-5) Histogram and density curve of permutation distribution of
<span class="math inline">\(\text{SS}_A\)</span> with the observed value of <span class="math inline">\(\text{SS}_A\)</span> displayed as a bold,
vertical line. The proportion of results that are as large or larger than the observed
value of <span class="math inline">\(\text{SS}_A\)</span> provides an estimate of the p-value.</p>
<div class="figure"><span id="fig:Figure3-5"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-5-1.png" alt="(ref:fig3-5)" width="960" />
<p class="caption">
Figure 1.32: (ref:fig3-5)
</p>
</div>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb227-1" title="1">B &lt;-<span class="st"> </span><span class="dv">1000</span></a>
<a class="sourceLine" id="cb227-2" title="2">Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</a>
<a class="sourceLine" id="cb227-3" title="3"><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</a>
<a class="sourceLine" id="cb227-4" title="4">  Tstar[b] &lt;-<span class="st"> </span><span class="kw">anova</span>(<span class="kw">lm</span>(Distance<span class="op">~</span><span class="kw">shuffle</span>(Condition), <span class="dt">data=</span>dd))[<span class="dv">1</span>,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb227-5" title="5">  }</a>
<a class="sourceLine" id="cb227-6" title="6"><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">300</span>))</a>
<a class="sourceLine" id="cb227-7" title="7"><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb227-8" title="8"><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</a>
<a class="sourceLine" id="cb227-9" title="9"><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a></code></pre></div>
<p>The right-skewed distribution (Figure <a href="2-3-section3-3.html#fig:Figure3-5">1.32</a>) contains the
distribution of <span class="math inline">\(\text{SS}^*_A\text{&#39;s}\)</span> under permutations (where
all the groups are assumed to be equivalent under the null hypothesis). The observed result is larger than all of the <span class="math inline">\(\text{SS}^*_A\text{&#39;s}\)</span>. The proportion
of permuted results that exceed the observed value is found using <code>pdata</code>
as before, except only for the area to the right of the observed result. We know
that <code>Tobs</code> will always be positive so no absolute values are required here.
</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb228-1" title="1"><span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</a></code></pre></div>
<pre><code>## [1] 0</code></pre>
<p>Because there were no permutations that exceeded the observed value, the p-value should be reported as p-value &lt; 0.001 (less than 1 in 1,000) and not 0. This suggests very strong evidence
against the null hypothesis of no difference in the true means. We would interpret
this p-value as saying that there is less than a 0.1% chance of getting a <span class="math inline">\(\text{SS}_A\)</span>
as large or larger than we observed, given that the null hypothesis is true.
</p>
<p>It ends up that some nice parametric statistical results
are available (if our assumptions are met) for the ratio of estimated variances,
the estimated variances are called <strong><em>Mean Squares</em></strong>. 

To turn sums of squares into mean square (variance) estimates,
we divide the sums of squares by the amount of free information available. For
example, remember the typical variance estimator introductory statistics,
<span class="math inline">\(\Sigma^N_1(y_i-\bar{y})^2/(N-1)\)</span>? Your instructor probably spent some time trying various
approaches to explaining why the denominator is the sample size minus 1. The most useful explanation for our
purposes moving forward is that we “lose” one piece of information to estimate
the mean and there are <span class="math inline">\(N\)</span> deviations around the single mean so we divide by
<span class="math inline">\(N-1\)</span>. The main point is that the sums of squares were divided by something and
we got an estimator for the variance, in that situation for the observations overall.</p>
<p>Now consider <span class="math inline">\(\text{SS}_E = \Sigma^J_{j=1}\Sigma^{n_j}_{i=1}(y_{ij}-\bar{y}_j)^2\)</span>
which still has <span class="math inline">\(N\)</span> deviations but it varies around the <span class="math inline">\(J\)</span> means, so the</p>
<p><span class="math display">\[\textbf{Mean Square Error} = \text{MS}_E = \text{SS}_E/(N-J).\]</span></p>
<p>Basically, we lose <span class="math inline">\(J\)</span> pieces of information in this calculation because we have
to estimate <span class="math inline">\(J\)</span> means. The similar calculation of the <strong><em>Mean Square for variable</em></strong> <span class="math inline">\(\mathbf{A}\)</span>
(<span class="math inline">\(\text{MS}_A\)</span>) is harder to see in the formula
(<span class="math inline">\(\text{SS}_A = \Sigma^J_{j=1}n_j(\bar{y}_i-\bar{\bar{y}})^2\)</span>), but the same
reasoning can be used to understand the denominator for forming <span class="math inline">\(\text{MS}_A\)</span>:
there are <span class="math inline">\(J\)</span> means that vary around the grand mean so</p>
<p><span class="math display">\[\text{MS}_A = \text{SS}_A/(J-1).\]</span></p>
<p>In summary, the two mean squares are simply:</p>
<ul>
<li><p><span class="math inline">\(\text{MS}_A = \text{SS}_A/(J-1)\)</span>, which estimates the variance of the group
means around the grand mean.</p></li>
<li><p><span class="math inline">\(\text{MS}_{\text{Error}} = \text{SS}_{\text{Error}}/(N-J)\)</span>, which estimates
the variation of the errors around the group means.</p></li>
</ul>

<p>These results are put together using a ratio to define the <strong><em>ANOVA F-statistic</em></strong>
(also called the <strong><em>F-ratio</em></strong>) as: </p>
<p><span class="math display">\[F=\text{MS}_A/\text{MS}_{\text{Error}}.\]</span></p>
<p>If the variability in the means is “similar” to the variability in the residuals,
the statistic would have a value around 1. If that variability is similar then
there would be no evidence of a difference in the means. If the <span class="math inline">\(\text{MS}_A\)</span> is much
larger than the <span class="math inline">\(\text{MS}_E\)</span>, the <span class="math inline">\(F\)</span>-statistic will provide evidence against
the null hypothesis. The “size” of the <span class="math inline">\(F\)</span>-statistic is formalized by finding the
p-value. The <span class="math inline">\(F\)</span>-statistic, if assumptions discussed below are not violated and we assume
the null hypothesis is true, follows what is called an <span class="math inline">\(F\)</span>-distribution.

The
<strong><em>F-distribution</em></strong> is a right-skewed distribution whose shape is defined by what
are called the <strong><em>numerator degrees of freedom</em></strong> (<span class="math inline">\(J-1\)</span>) and the
<strong><em>denominator degrees of freedom</em></strong> (<span class="math inline">\(N-J\)</span>). These names correspond to the values
that we used to calculate the mean squares and where in the <span class="math inline">\(F\)</span>-ratio each mean
square was used; <span class="math inline">\(F\)</span>-distributions are denoted by their degrees of freedom using
the convention of <span class="math inline">\(F\)</span> (<em>numerator df</em>, <em>denominator df</em>). Some examples of
different <span class="math inline">\(F\)</span>-distributions are displayed for you in Figure <a href="2-3-section3-3.html#fig:Figure3-6">1.33</a>. </p>
<p>The characteristics of the F-distribution can be summarized as:</p>
<ul>
<li><p>Right skewed,</p></li>
<li><p>Nonzero probabilities for values greater than 0,</p></li>
<li><p>Its shape changes depending on the <strong>numerator DF</strong> and <strong>denominator DF</strong>, and</p></li>
<li><p><strong>Always use the right-tailed area for p-values.</strong></p></li>
</ul>
<p>(ref:fig3-6) Density curves of four different <span class="math inline">\(F\)</span>-distributions. Upper left is an
<span class="math inline">\(F(6, 5683)\)</span>, upper right is <span class="math inline">\(F(2, 10)\)</span>, lower left is <span class="math inline">\(F(6, 10)\)</span>, and lower right
is <span class="math inline">\(F(3, 20)\)</span>. P-values are found using the areas to the right of the observed
<span class="math inline">\(F\)</span>-statistic value in all F-distributions. </p>
<div class="figure"><span id="fig:Figure3-6"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-6-1.png" alt="(ref:fig3-6)" width="480" />
<p class="caption">
Figure 1.33: (ref:fig3-6)
</p>
</div>
<p>Now we are ready to discuss an ANOVA table since we know about each of its
components. Note the general format of the ANOVA table is in Table <a href="2-3-section3-3.html#tab:Table3-2">1.4</a><a href="#fn54" class="footnote-ref" id="fnref54"><sup>54</sup></a>:
</p>

<table>
<caption><span id="tab:Table3-2">Table 1.4: </span> General One-Way ANOVA table.</caption>
<colgroup>
<col width="13%" />
<col width="7%" />
<col width="18%" />
<col width="21%" />
<col width="19%" />
<col width="19%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Source  </th>
<th align="left">DF </th>
<th align="left">Sums of<br />
Squares</th>
<th align="left">Mean Squares</th>
<th align="left">F-ratio</th>
<th align="left">P-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Variable A</td>
<td align="left"><span class="math inline">\(J-1\)</span></td>
<td align="left"><span class="math inline">\(\text{SS}_A\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_A=\text{SS}_A/(J-1)\)</span></td>
<td align="left"><span class="math inline">\(F=\text{MS}_A/\text{MS}_E\)</span></td>
<td align="left">Right tail of <span class="math inline">\(F(J-1,N-J)\)</span></td>
</tr>
<tr class="even">
<td align="left">Residuals</td>
<td align="left"><span class="math inline">\(N-J\)</span></td>
<td align="left"><span class="math inline">\(\text{SS}_E\)</span></td>
<td align="left"><span class="math inline">\(\text{MS}_E = \text{SS}_E/(N-J)\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="left"><span class="math inline">\(N-1\)</span></td>
<td align="left"><span class="math inline">\(\text{SS}_{\text{Total}}\)</span></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>
<p>The table is oriented to help you reconstruct the <span class="math inline">\(F\)</span>-ratio from each of its
components. The output from R is similar although it does not provide the last row
and sometimes switches the order of columns in different functions we will use. The R version of the table for the type
of outfit effect (<code>Condition</code>) with <span class="math inline">\(J=7\)</span> levels and <span class="math inline">\(N=5,690\)</span> observations, repeated
from above, is:</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb230-1" title="1"><span class="kw">anova</span>(lm2)</a></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: Distance
##             Df  Sum Sq Mean Sq F value    Pr(&gt;F)
## Condition    6   34948  5824.7  6.5081 7.392e-07
## Residuals 5683 5086298   895.0</code></pre>
<p>The p-value from the <span class="math inline">\(F\)</span>-distribution is 710^{-7} so we can report it as a p-value &lt; 0.0001<a href="#fn55" class="footnote-ref" id="fnref55"><sup>55</sup></a>.   We can
verify this result using the observed <span class="math inline">\(F\)</span>-statistic of 6.51
(which came from taking the ratio of the two mean squares,
F=5824.74/895)
which follows an <span class="math inline">\(F(6, 5683)\)</span> distribution if the null hypothesis is true and some
other assumptions are met. Using the <code>pf</code> function provides us with areas in the
specified <span class="math inline">\(F\)</span>-distribution with the <code>df1</code> provided to the function as the
numerator <em>df</em> and <code>df2</code> as the denominator <em>df</em> and <code>lower.tail=F</code> reflecting
our desire for a right tailed area. 
</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb232-1" title="1"><span class="kw">pf</span>(<span class="fl">6.51</span>, <span class="dt">df1=</span><span class="dv">6</span>, <span class="dt">df2=</span><span class="dv">5683</span>, <span class="dt">lower.tail=</span>F)</a></code></pre></div>
<pre><code>## [1] 7.353832e-07</code></pre>
<p>The result from the <span class="math inline">\(F\)</span>-distribution using this parametric procedure is similar to
the p-value obtained using permutations with the test statistic of the
<span class="math inline">\(\text{SS}_A\)</span>, which was &lt; 0.0001. The <span class="math inline">\(F\)</span>-statistic obviously is another
potential test statistic to use as a test statistic in a permutation approach,
now that we know about it. We should check that we get similar results from it
with permutations as we did from using <span class="math inline">\(\text{SS}_A\)</span> as a permutation-test test
statistic. The following code generates the permutation distribution

for the
<span class="math inline">\(F\)</span>-statistic (Figure <a href="2-3-section3-3.html#fig:Figure3-7">1.34</a>) and assesses how unusual the observed
<span class="math inline">\(F\)</span>-statistic of 6.51 was in this permutation distribution.
The only change in the code involves moving from extracting <span class="math inline">\(\text{SS}_A\)</span> to
extracting the <span class="math inline">\(F\)</span>-ratio which is in the 4<sup>th</sup> column of the <code>anova</code>
output:</p>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb234-1" title="1">Tobs &lt;-<span class="st"> </span><span class="kw">anova</span>(<span class="kw">lm</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>dd))[<span class="dv">1</span>,<span class="dv">4</span>]; Tobs</a></code></pre></div>
<pre><code>## [1] 6.508071</code></pre>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb236-1" title="1">B &lt;-<span class="st"> </span><span class="dv">1000</span></a>
<a class="sourceLine" id="cb236-2" title="2">Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</a>
<a class="sourceLine" id="cb236-3" title="3"><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</a>
<a class="sourceLine" id="cb236-4" title="4">  Tstar[b] &lt;-<span class="st"> </span><span class="kw">anova</span>(<span class="kw">lm</span>(Distance<span class="op">~</span><span class="kw">shuffle</span>(Condition), <span class="dt">data=</span>dd))[<span class="dv">1</span>,<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb236-5" title="5">}</a>
<a class="sourceLine" id="cb236-6" title="6"></a>
<a class="sourceLine" id="cb236-7" title="7"><span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>F)[[<span class="dv">1</span>]]</a></code></pre></div>
<pre><code>## [1] 0</code></pre>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb238-1" title="1"><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)</a>
<a class="sourceLine" id="cb238-2" title="2"><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb238-3" title="3"><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</a>
<a class="sourceLine" id="cb238-4" title="4"><span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a></code></pre></div>
<p>(ref:fig3-7) Histogram and density curve of the permutation distribution of
the F-statistic with bold, vertical line for the observed value of the test
statistic of 6.51.</p>
<div class="figure"><span id="fig:Figure3-7"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-7-1.png" alt="(ref:fig3-7)" width="960" />
<p class="caption">
Figure 1.34: (ref:fig3-7)
</p>
</div>
<p>The permutation-based p-value is again at less than 1 in 1,000, which matches the other
results closely. The first conclusion is that using a test statistic of either
the <span class="math inline">\(F\)</span>-statistic or the <span class="math inline">\(\text{SS}_A\)</span> provide similar permutation results.
However, we tend to favor using the <span class="math inline">\(F\)</span>-statistic because it is more commonly used
in reporting ANOVA results, not because it is any better in a permutation context .</p>
<p>It is also interesting to compare the permutation distribution for the
<span class="math inline">\(F\)</span>-statistic and the parametric <span class="math inline">\(F(6, 6583)\)</span> distribution
(Figure <a href="2-3-section3-3.html#fig:Figure3-8">1.35</a>). They do not match perfectly but are quite similar.
Some the differences around 0 are due to the behavior of the method used to create
the density curve and are not really a problem for the methods. The similarity in
the two curves explains why both methods would give similar p-value results for almost any test statistic value. In some
situations, the correspondence will not be quite so close.</p>
<p>(ref:fig3-8) Comparison of <span class="math inline">\(F(6, 6583)\)</span> (dashed line) and permutation distribution
(solid line).</p>
<div class="figure"><span id="fig:Figure3-8"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-8-1.png" alt="(ref:fig3-8)" width="480" />
<p class="caption">
Figure 1.35: (ref:fig3-8)
</p>
</div>
<p>So how can we rectify this result (p-value &lt; 0.0001) and the
Chapter <a href="1-chapter2.html#chapter2">1</a> result that reported moderate evidence against the null hypothesis of no difference between <em>commute</em>
and <em>casual</em> with a <span class="math inline">\(\text{p-value}\approx 0.04\)</span>? I selected the two groups
to compare in Chapter <a href="1-chapter2.html#chapter2">1</a> because they were somewhat far apart but not too far apart. I could have selected <em>police</em> and <em>polite</em> as they are furthest apart and just focused on that difference. “Cherry-picking” a comparison when many are present, especially one that is most different, without accounting for this choice creates a
false sense of the real situation and inflates the Type I error rate because of
the selection<a href="#fn56" class="footnote-ref" id="fnref56"><sup>56</sup></a>.


If the entire suite of pairwise comparisons are considered, this
result may lose some of its luster. In other words, if we consider the suite of
21 pair-wise differences (and the tests) implicit in comparing all of them,
we may need really strong evidence against the null in at least some of the pairs to suggest overall differences. In this situation,
the <em>hiviz</em> and <em>casual</em>
groups are not that different from each other so their difference does not
contribute much to the overall <span class="math inline">\(F\)</span>-test. In Section <a href="2-6-section3-6.html#section3-6">2.6</a>, we will
revisit this topic and consider a method that is
statistically valid for performing all possible pair-wise comparisons that is also
consistent with our overall test results.</p>
</div>
<div class="footnotes">
<hr />
<ol start="54">
<li id="fn54"><p>Make sure you can work
from left to right and up and down to fill in the ANOVA table given just the
necessary information to determine the other components or from a study description to complete the <em>DF</em> part of the table – there are always questions like these on exams…<a href="2-3-section3-3.html#fnref54" class="footnote-back">↩</a></p></li>
<li id="fn55"><p>Any further claimed precision is an exaggeration and eventually we might see p-values that approach the precision of the computer at 2.2e-16 and anything below 0.0001 should just be reported as being below 0.0001. Also note the way that R represents small or extremely large numbers using scientific notation such as <code>3e-4</code> which is <span class="math inline">\(3 \cdot 10^{-4} = 0.0003\)</span><a href="2-3-section3-3.html#fnref55" class="footnote-back">↩</a></p></li>
<li id="fn56"><p>This would be another type of publication bias – where researchers search across groups and only report their biggest differences and fail to report the other pairs that they compared. As discussed before, this biases the
results to detecting results more than they should be and then when
other researchers try to repeat the same studies and compare just, say, two groups, they likely will fail to find
similar results unless they also search across many different possible comparisons and only report the most extreme. The better approach is to do the ANOVA <span class="math inline">\(F\)</span>-test first and then Tukey’s comparisons and report all these results, as discussed below.<a href="2-3-section3-3.html#fnref56" class="footnote-back">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="2-2-section3-2.html"><button class="btn btn-default">Previous</button></a>
<a href="2-4-section3-4.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
