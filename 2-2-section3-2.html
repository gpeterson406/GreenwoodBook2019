<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="2.2 Linear model for One-Way ANOVA (cell-means and reference-coding) | Intermediate Statistics with R" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="gpeterson406/Greenwood_Book" />

<meta name="author" content="Mark C Greenwood" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="2.2 Linear model for One-Way ANOVA (cell-means and reference-coding) | Intermediate Statistics with R">

<title>2.2 Linear model for One-Way ANOVA (cell-means and reference-coding) | Intermediate Statistics with R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li class="has-sub"><a href="acknowledgments.html#acknowledgments">Acknowledgments</a><ul>
<li><a href="0-1-section1-5.html#section1-5"><span class="toc-section-number">0.1</span> Summary of important R code</a></li>
</ul></li>
<li class="has-sub"><a href="1-chapter2.html#chapter2"><span class="toc-section-number">1</span> (R)e-Introduction to statistics</a><ul>
<li><a href="1-1-section2-1.html#section2-1"><span class="toc-section-number">1.1</span> Histograms, boxplots, and density curves</a></li>
<li><a href="1-2-section2-2.html#section2-2"><span class="toc-section-number">1.2</span> Pirate-plots</a></li>
<li><a href="1-3-section2-3.html#section2-3"><span class="toc-section-number">1.3</span> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li><a href="1-4-section2-4.html#section2-4"><span class="toc-section-number">1.4</span> Permutation testing for the two sample mean situation</a></li>
<li><a href="1-5-section2-5.html#section2-5"><span class="toc-section-number">1.5</span> Hypothesis testing (general)</a></li>
<li><a href="1-6-section2-6.html#section2-6"><span class="toc-section-number">1.6</span> Connecting randomization (nonparametric) and parametric tests</a></li>
<li><a href="1-7-section2-7.html#section2-7"><span class="toc-section-number">1.7</span> Second example of permutation tests</a></li>
<li><a href="1-8-section2-8.html#section2-8"><span class="toc-section-number">1.8</span> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li><a href="1-9-section2-9.html#section2-9"><span class="toc-section-number">1.9</span> Confidence intervals and bootstrapping</a></li>
<li><a href="1-10-section2-10.html#section2-10"><span class="toc-section-number">1.10</span> Bootstrap confidence intervals for difference in GPAs</a></li>
<li><a href="1-11-section2-11.html#section2-11"><span class="toc-section-number">1.11</span> Chapter summary</a></li>
<li><a href="1-12-section2-12.html#section2-12"><span class="toc-section-number">1.12</span> Summary of important R code</a></li>
<li><a href="1-13-section2-13.html#section2-13"><span class="toc-section-number">1.13</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="2-chapter3.html#chapter3"><span class="toc-section-number">2</span> One-Way ANOVA</a><ul>
<li><a href="2-1-section3-1.html#section3-1"><span class="toc-section-number">2.1</span> Situation</a></li>
<li><a href="2-2-section3-2.html#section3-2"><span class="toc-section-number">2.2</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li><a href="2-3-section3-3.html#section3-3"><span class="toc-section-number">2.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li><a href="2-4-section3-4.html#section3-4"><span class="toc-section-number">2.4</span> ANOVA model diagnostics including QQ-plots</a></li>
<li><a href="2-5-section3-5.html#section3-5"><span class="toc-section-number">2.5</span> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li><a href="2-6-section3-6.html#section3-6"><span class="toc-section-number">2.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li><a href="2-7-section3-7.html#section3-7"><span class="toc-section-number">2.7</span> Pair-wise comparisons for the Overtake data</a></li>
<li><a href="2-8-section3-8.html#section3-8"><span class="toc-section-number">2.8</span> Chapter summary</a></li>
<li><a href="2-9-section3-9.html#section3-9"><span class="toc-section-number">2.9</span> Summary of important R code</a></li>
<li><a href="2-10-section3-10.html#section3-10"><span class="toc-section-number">2.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="3-chapter4.html#chapter4"><span class="toc-section-number">3</span> Two-Way ANOVA</a><ul>
<li><a href="3-1-section4-1.html#section4-1"><span class="toc-section-number">3.1</span> Situation</a></li>
<li><a href="3-2-section4-2.html#section4-2"><span class="toc-section-number">3.2</span> Designing a two-way experiment and visualizing results</a></li>
<li><a href="3-3-section4-3.html#section4-3"><span class="toc-section-number">3.3</span> Two-Way ANOVA models and hypothesis tests</a></li>
<li><a href="3-4-section4-4.html#section4-4"><span class="toc-section-number">3.4</span> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li><a href="3-5-section4-5.html#section4-5"><span class="toc-section-number">3.5</span> Observational study example: The Psychology of Debt</a></li>
<li><a href="3-6-section4-6.html#section4-6"><span class="toc-section-number">3.6</span> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li><a href="3-7-section4-7.html#section4-7"><span class="toc-section-number">3.7</span> Chapter summary</a></li>
<li><a href="3-8-section4-8.html#section4-8"><span class="toc-section-number">3.8</span> Summary of important R code</a></li>
<li><a href="3-9-section4-9.html#section4-9"><span class="toc-section-number">3.9</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="4-chapter5.html#chapter5"><span class="toc-section-number">4</span> Chi-square tests</a><ul>
<li><a href="4-1-section5-1.html#section5-1"><span class="toc-section-number">4.1</span> Situation, contingency tables, and tableplots</a></li>
<li><a href="4-2-section5-2.html#section5-2"><span class="toc-section-number">4.2</span> Homogeneity test hypotheses</a></li>
<li><a href="4-3-section5-3.html#section5-3"><span class="toc-section-number">4.3</span> Independence test hypotheses</a></li>
<li><a href="4-4-section5-4.html#section5-4"><span class="toc-section-number">4.4</span> Models for R by C tables</a></li>
<li><a href="4-5-section5-5.html#section5-5"><span class="toc-section-number">4.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="4-6-section5-6.html#section5-6"><span class="toc-section-number">4.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="4-7-section5-7.html#section5-7"><span class="toc-section-number">4.7</span> Examining residuals for the source of differences</a></li>
<li><a href="4-8-section5-8.html#section5-8"><span class="toc-section-number">4.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li><a href="4-9-section5-9.html#section5-9"><span class="toc-section-number">4.9</span> Political party and voting results: Complete analysis</a></li>
<li><a href="4-10-section5-10.html#section5-10"><span class="toc-section-number">4.10</span> Is cheating and lying related in students?</a></li>
<li><a href="4-11-section5-11.html#section5-11"><span class="toc-section-number">4.11</span> Analyzing a stratified random sample of California schools</a></li>
<li><a href="4-12-section5-12.html#section5-12"><span class="toc-section-number">4.12</span> Chapter summary</a></li>
<li><a href="4-13-section5-13.html#section5-13"><span class="toc-section-number">4.13</span> Summary of important R commands</a></li>
<li><a href="4-14-section5-14.html#section5-14"><span class="toc-section-number">4.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="5-chapter6.html#chapter6"><span class="toc-section-number">5</span> Correlation and Simple Linear Regression</a><ul>
<li><a href="5-1-section6-1.html#section6-1"><span class="toc-section-number">5.1</span> Relationships between two quantitative variables</a></li>
<li><a href="5-2-section6-2.html#section6-2"><span class="toc-section-number">5.2</span> Estimating the correlation coefficient</a></li>
<li><a href="5-3-section6-3.html#section6-3"><span class="toc-section-number">5.3</span> Relationships between variables by groups</a></li>
<li><a href="5-4-section6-4.html#section6-4"><span class="toc-section-number">5.4</span> Inference for the correlation coefficient</a></li>
<li><a href="5-5-section6-5.html#section6-5"><span class="toc-section-number">5.5</span> Are tree diameters related to tree heights?</a></li>
<li><a href="5-6-section6-6.html#section6-6"><span class="toc-section-number">5.6</span> Describing relationships with a regression model</a></li>
<li><a href="5-7-section6-7.html#section6-7"><span class="toc-section-number">5.7</span> Least Squares Estimation</a></li>
<li><a href="5-8-section6-8.html#section6-8"><span class="toc-section-number">5.8</span> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li><a href="5-9-section6-9.html#section6-9"><span class="toc-section-number">5.9</span> Outliers: leverage and influence</a></li>
<li><a href="5-10-section6-10.html#section6-10"><span class="toc-section-number">5.10</span> Residual diagnostics – setting the stage for inference</a></li>
<li><a href="5-11-section6-11.html#section6-11"><span class="toc-section-number">5.11</span> Old Faithful discharge and waiting times</a></li>
<li><a href="5-12-section6-12.html#section6-12"><span class="toc-section-number">5.12</span> Chapter summary</a></li>
<li><a href="5-13-section6-13.html#section6-13"><span class="toc-section-number">5.13</span> Summary of important R code</a></li>
<li><a href="5-14-section6-14.html#section6-14"><span class="toc-section-number">5.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="6-chapter7.html#chapter7"><span class="toc-section-number">6</span> Simple linear regression inference</a><ul>
<li><a href="6-1-section7-1.html#section7-1"><span class="toc-section-number">6.1</span> Model</a></li>
<li><a href="6-2-section7-2.html#section7-2"><span class="toc-section-number">6.2</span> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li><a href="6-3-section7-3.html#section7-3"><span class="toc-section-number">6.3</span> Bozeman temperature trend</a></li>
<li><a href="6-4-section7-4.html#section7-4"><span class="toc-section-number">6.4</span> Randomization-based inferences for the slope coefficient</a></li>
<li><a href="6-5-section7-5.html#section7-5"><span class="toc-section-number">6.5</span> Transformations part I: Linearizing relationships</a></li>
<li><a href="6-6-section7-6.html#section7-6"><span class="toc-section-number">6.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li><a href="6-7-section7-7.html#section7-7"><span class="toc-section-number">6.7</span> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li><a href="6-8-section7-8.html#section7-8"><span class="toc-section-number">6.8</span> Chapter summary</a></li>
<li><a href="6-9-section7-9.html#section7-9"><span class="toc-section-number">6.9</span> Summary of important R code</a></li>
<li><a href="6-10-section7-10.html#section7-10"><span class="toc-section-number">6.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="7-chapter8.html#chapter8"><span class="toc-section-number">7</span> Multiple linear regression</a><ul>
<li><a href="7-1-section8-1.html#section8-1"><span class="toc-section-number">7.1</span> Going from SLR to MLR</a></li>
<li><a href="7-2-section8-2.html#section8-2"><span class="toc-section-number">7.2</span> Validity conditions in MLR</a></li>
<li><a href="7-3-section8-3.html#section8-3"><span class="toc-section-number">7.3</span> Interpretation of MLR terms</a></li>
<li><a href="7-4-section8-4.html#section8-4"><span class="toc-section-number">7.4</span> Comparing multiple regression models</a></li>
<li><a href="7-5-section8-5.html#section8-5"><span class="toc-section-number">7.5</span> General recommendations for MLR interpretations and VIFs</a></li>
<li><a href="7-6-section8-6.html#section8-6"><span class="toc-section-number">7.6</span> MLR inference: Parameter inferences using the t-distribution</a></li>
<li><a href="7-7-section8-7.html#section8-7"><span class="toc-section-number">7.7</span> Overall F-test in multiple linear regression</a></li>
<li><a href="7-8-section8-8.html#section8-8"><span class="toc-section-number">7.8</span> Case study: First year college GPA and SATs</a></li>
<li><a href="7-9-section8-9.html#section8-9"><span class="toc-section-number">7.9</span> Different intercepts for different groups: MLR with indicator variables</a></li>
<li><a href="7-10-section8-10.html#section8-10"><span class="toc-section-number">7.10</span> Additive MLR with more than two groups: Headache example</a></li>
<li><a href="7-11-section8-11.html#section8-11"><span class="toc-section-number">7.11</span> Different slopes and different intercepts</a></li>
<li><a href="7-12-section8-12.html#section8-12"><span class="toc-section-number">7.12</span> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li><a href="7-13-section8-13.html#section8-13"><span class="toc-section-number">7.13</span> AICs for model selection</a></li>
<li><a href="7-14-section8-14.html#section8-14"><span class="toc-section-number">7.14</span> Case study: Forced expiratory volume model selection using AICs</a></li>
<li><a href="7-15-section8-15.html#section8-15"><span class="toc-section-number">7.15</span> Chapter summary</a></li>
<li><a href="7-16-section8-16.html#section8-16"><span class="toc-section-number">7.16</span> Summary of important R code</a></li>
<li><a href="7-17-section8-17.html#section8-17"><span class="toc-section-number">7.17</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="8-chapter9.html#chapter9"><span class="toc-section-number">8</span> Case studies</a><ul>
<li><a href="8-1-section9-1.html#section9-1"><span class="toc-section-number">8.1</span> Overview of material covered</a></li>
<li><a href="8-2-section9-2.html#section9-2"><span class="toc-section-number">8.2</span> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li><a href="8-3-section9-3.html#section9-3"><span class="toc-section-number">8.3</span> Ants learn to rely on more informative attributes during decision-making</a></li>
<li><a href="8-4-section9-4.html#section9-4"><span class="toc-section-number">8.4</span> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li><a href="8-5-section9-5.html#section9-5"><span class="toc-section-number">8.5</span> What do didgeridoos really do about sleepiness?</a></li>
<li><a href="8-6-section9-6.html#section9-6"><span class="toc-section-number">8.6</span> General summary</a></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="section3-2" class="section level2">
<h2><span class="header-section-number">2.2</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</h2>
<p>We introduced the statistical model <span class="math inline">\(y_{ij} = \mu_j+\varepsilon_{ij}\)</span> in
Chapter <a href="1-chapter2.html#chapter2">1</a> for the situation with <span class="math inline">\(j = 1 \text{ or } 2\)</span> to denote
a situation where there were two groups and, for the model that is consistent
with the alternative hypothesis, the means differed. Now there are seven groups
and the previous model can be extended to this new situation by allowing <span class="math inline">\(j\)</span>
to be 1, 2, 3, …, 7.  As before, the linear model assumes that the responses follow a normal
distribution with the model defining the mean of the normal distributions and all observations have the
same variance. *<strong>Linear models</strong> assume that the parameters for the mean in the model enter linearly. This
last condition is hard to explain at this level of material – it is sufficient
to know that there are models where the parameters enter the model nonlinearly and
that they are beyond the scope of this function and this material and you won’t run into them in most statistical models. By employing this general “linear” modeling methodology, we will be able to use the same general modeling framework for the methods
in Chapters <a href="2-chapter3.html#chapter3"><strong>??</strong></a>, <a href="3-chapter4.html#chapter4"><strong>??</strong></a>, <a href="5-chapter6.html#chapter6"><strong>??</strong></a>,
<a href="6-chapter7.html#chapter7"><strong>??</strong></a>, and <a href="7-chapter8.html#chapter8"><strong>??</strong></a>.</p>
<p>As in Chapter <a href="1-chapter2.html#chapter2">1</a>, the null hypothesis defines a
situation (and model) where all the groups have the same mean. Specifically,
the <strong><em>null hypothesis</em></strong> in the general situation with <span class="math inline">\(J\)</span> groups
(<span class="math inline">\(J\ge 2\)</span>) is to have all the <span class="math inline">\(\underline{\text{true}}\)</span> group means equal,</p>
<p><span class="math display">\[H_0:\mu_1 = \ldots = \mu_J.\]</span></p>
<p>This defines a model where all the groups have the same mean so it can be
defined in terms of a single mean, <span class="math inline">\(\mu\)</span>, for the <span class="math inline">\(i^{th}\)</span> observation from
the <span class="math inline">\(j^{th}\)</span> group as <span class="math inline">\(y_{ij} = \mu+\varepsilon_{ij}\)</span>. This is not the model
that most researchers want to be the final description of their study as it
implies no difference in the groups. There is more caution required to specify
the alternative hypothesis with more than two groups. The
<strong><em>alternative hypothesis</em></strong> needs to be the logical negation of this null
hypothesis of all groups having equal means; to make the null hypothesis
false, we only need one group to differ but more than one group could differ
from the others. Essentially, there are many ways to “violate” the null
hypothesis so we choose some delicate wording for the alternative hypothesis
when there are more than 2 groups. Specifically, we state the alternative as</p>
<p><span class="math display">\[H_A: \text{ Not all } \mu_j \text{ are equal}\]</span></p>
<p>or, in words, <strong>at least one of the true means differs among the J groups</strong>.
You might be attracted to trying to say that all means are different in the
alternative but we do not put this strict a requirement in place to reject the
null hypothesis. The alternative model

allows all the true group means to
differ but does require that they are actually all different with the model written as</p>
<p><span class="math display">\[y_{ij} = {\color{red}{\mu_j}}+\varepsilon_{ij}.\]</span></p>
<p>This linear model

states that the response for the <span class="math inline">\(i^{th}\)</span> observation in
the <span class="math inline">\(j^{th}\)</span> group, <span class="math inline">\(\mathbf{y_{ij}}\)</span>, is modeled with a group <span class="math inline">\(j\)</span>
(<span class="math inline">\(j=1, \ldots, J\)</span>) population mean, <span class="math inline">\(\mu_j\)</span>, and a random error for each subject
in each group, <span class="math inline">\(\varepsilon_{ij}\)</span>, that we assume follows a normal distribution and
that all the random errors have the same variance, <span class="math inline">\(\sigma^2\)</span>. We can write the assumption about the random errors, often called the <strong><em>normality assumption</em></strong>,
as <span class="math inline">\(\varepsilon_{ij} \sim N(0,\sigma^2)\)</span>. There is a second way to write out this
model that allows extension to more complex models discussed below, so we
need a name for this version of the model. The model written in terms of the
<span class="math inline">\({\color{red}{\mu_j}}\text{&#39;s}\)</span> is called the
<b><font color='red'>cell means model</font></b> and is the
easier version of this model to understand.
</p>
<p>One of the reasons we learned about pirate-plots is that
it helps us visually consider all the aspects of this model.

In Figure <a href="2-1-section3-1.html#fig:Figure3-1">1.28</a>,
we can see the bold horizontal lines that provide the estimated (sample) group means.
The bigger the differences in the sample means (especially relative to the variability around the means), the more evidence we will find
against the null hypothesis. You can also see the null model on the plot
that assumes all the groups have the same mean as displayed in the
dashed horizontal line
at 117.1 cm (the R code below shows the overall mean of <em>Distance</em> is 117.1). While
the hypotheses focus on the means, the model also contains assumptions about the
distribution of the responses – specifically that the distributions are normal
and that all the groups have the same variability, which do not appear to be clearly violated in this situation.
</p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb212-1" title="1"><span class="kw">mean</span>(dd<span class="op">$</span>Distance)</a></code></pre></div>
<pre><code>## [1] 117.126</code></pre>
<p>There is a second way to write out the One-Way ANOVA model

that provides a framework
for extensions to more complex models described in Chapter <a href="3-chapter4.html#chapter4"><strong>??</strong></a> and
beyond. The other <strong><em>parameterization</em></strong> (way of writing out or defining) of the
model is called the <b><font color='purple'>reference-coded model</font></b> since it
writes out the model in terms of a


<strong><em>baseline group</em></strong> and deviations from that baseline or reference level. The
reference-coded model for the <span class="math inline">\(i^{th}\)</span> subject in the <span class="math inline">\(j^{th}\)</span> group is
<span class="math inline">\(y_{ij} ={\color{purple}{\boldsymbol{\alpha + \tau_j}}}+\varepsilon_{ij}\)</span> where
<span class="math inline">\(\color{purple}{\boldsymbol{\alpha}}\)</span> (“alpha”) is the true mean for the
baseline group (usually first alphabetically) and the <span class="math inline">\(\color{purple}{\boldsymbol{\tau_j}}\)</span>
(tau <span class="math inline">\(j\)</span>) are the deviations from the baseline group for group <span class="math inline">\(j\)</span>. The deviation
for the baseline group, <span class="math inline">\(\color{purple}{\boldsymbol{\tau_1}}\)</span>, is always set to 0
so there are really just deviations for groups 2 through <span class="math inline">\(J\)</span>. The equivalence
between the reference-coded and cell-means models can be seen by considering the mean for the first, second,
and <span class="math inline">\(J^{th}\)</span> groups in both models:</p>
<p><span class="math display">\[\begin{array}{lccc}
&amp; \textbf{Cell means:} &amp;&amp; \textbf{Reference-coded:}\\
\textbf{Group } 1: &amp; \color{red}{\mu_1} &amp;&amp; \color{purple}{\boldsymbol{\alpha}} \\
\textbf{Group } 2: &amp; \color{red}{\mu_2} &amp;&amp; \color{purple}{\boldsymbol{\alpha + \tau_2}} \\
\ldots &amp; \ldots &amp;&amp; \ldots \\
\textbf{Group } J: &amp; \color{red}{\mu_J} &amp;&amp; \color{purple}{\boldsymbol{\alpha +\tau_J}}
\end{array}\]</span></p>
<p>The hypotheses for the reference-coded model are similar to those in the
cell-means coding except that they are defined in terms of the deviations,
<span class="math inline">\({\color{purple}{\boldsymbol{\tau_j}}}\)</span>. The null hypothesis is that there is
no deviation from the baseline for any group – that all the <span class="math inline">\({\color{purple}{\boldsymbol{\tau_j\text{&#39;s}}}}=0\)</span>,</p>
<p><span class="math display">\[\boldsymbol{H_0: \tau_2=\ldots=\tau_J=0}.\]</span></p>
<p>The alternative hypothesis is that at least one of the deviations is not 0,</p>
<p><span class="math display">\[\boldsymbol{H_A:} \textbf{ Not all } \boldsymbol{\tau_j} \textbf{ equal } \bf{0}.\]</span></p>
<p>In this chapter, you are welcome to use either version (unless we instruct you
otherwise) but we have to use the reference-coding in subsequent chapters. The
next task is to learn how to use R’s linear model, <code>lm</code>, function to get
estimates of the parameters<a href="#fn50" class="footnote-ref" id="fnref50"><sup>50</sup></a> in each model, but first a quick review of these
new ideas:</p>
<p><b><font color='red'>Cell Means Version</font></b></p>
<ul>
<li><p><span class="math inline">\(H_0: {\color{red}{\mu_1= \ldots = \mu_J}}\)</span>        
    <span class="math inline">\(H_A: {\color{red}{\text{ Not all } \mu_j \text{ equal}}}\)</span></p></li>
<li><p>Null hypothesis in words: No difference in the true means among the groups.</p></li>
<li><p>Null model: <span class="math inline">\(y_{ij} = \mu+\varepsilon_{ij}\)</span> </p></li>
<li><p>Alternative hypothesis in words: At least one of the true means differs among
the groups.</p></li>
<li><p>Alternative model: <span class="math inline">\(y_{ij} = \color{red}{\mu_j}+\varepsilon_{ij}.\)</span>
</p></li>
</ul>
<p><b><font color='purple'>Reference-coded Version</font></b></p>
<ul>
<li><p><span class="math inline">\(H_0: \color{purple}{\boldsymbol{\tau_2 = \ldots = \tau_J = 0}}\)</span>
        
<span class="math inline">\(H_A: \color{purple}{\text{ Not all } \tau_j \text{ equal 0}}\)</span></p></li>
<li><p>Null hypothesis in words: No deviation of the true mean for any groups from the
baseline group.</p></li>
<li><p>Null model: <span class="math inline">\(y_{ij} =\boldsymbol{\alpha} +\varepsilon_{ij}\)</span>
</p></li>
<li><p>Alternative hypothesis in words: At least one of the true deviations is
different from 0 or that at least one group has a different true mean than the
baseline group.</p></li>
<li><p>Alternative model: <span class="math inline">\(y_{ij} =\color{purple}{\boldsymbol{\alpha + \tau_j}}+\varepsilon_{ij}\)</span> </p></li>
</ul>
<p>In order to estimate the models discussed above, the <code>lm</code> function is used<a href="#fn51" class="footnote-ref" id="fnref51"><sup>51</sup></a>. The <code>lm</code> function continues to
use the same format as previous functions and in Chapter <a href="1-chapter2.html#chapter2">1</a> , <code>lm(Y~X, data=datasetname)</code>.
It ends up that <code>lm</code> generates the reference-coded version of the
model by default (The developers of R thought it was that important!).


But we want to start with the
cell-means version of the model,

so we have to override the standard technique
and add a “<code>-1</code>” to the formula interface to tell R that we want to the
cell-means coding. Generally, this looks like <code>lm(Y~X-1, data=datasetname).</code>

Once we fit a model in R, the <code>summary</code> function run on the model provides a
useful “summary” of the model coefficients and a suite of other potentially
interesting information. For the moment, we will focus on the estimated model
coefficients, so only those lines are provided. When fitting the cell-means version
of the One-Way ANOVA model,
you will find a row of output for each group relating estimating the <span class="math inline">\(\mu_j\text{&#39;s}\)</span>.
The output contains columns for an estimate (<code>Estimate</code>), standard error
(<code>Std. Error</code>), <span class="math inline">\(t\)</span>-value (<code>t value</code>), and p-value (<code>Pr(&gt;|t|)</code>). We’ll
explore which of these are of interest in these models below, but focus
on the estimates of the parameters that the function provides in the first
column (“Estimate”) of the coefficient table and compare these results to what was found using <code>favstats</code>.</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb214-1" title="1">lm1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span>Condition<span class="dv">-1</span>, <span class="dt">data=</span>dd)</a>
<a class="sourceLine" id="cb214-2" title="2"><span class="kw">summary</span>(lm1)<span class="op">$</span>coefficients</a></code></pre></div>
<pre><code>##                  Estimate Std. Error  t value Pr(&gt;|t|)
## Conditioncasual  117.6110   1.071873 109.7248        0
## Conditioncommute 114.6079   1.021931 112.1484        0
## Conditionhiviz   118.4383   1.101992 107.4765        0
## Conditionnovice  116.9405   1.053114 111.0426        0
## Conditionpolice  122.1215   1.064384 114.7344        0
## Conditionpolite  114.0518   1.015435 112.3182        0
## Conditionracer   116.7559   1.024925 113.9164        0</code></pre>
<p>In general, we denote estimated parameters   with a hat over the parameter of
interest to show that it is an estimate. For the true mean of group <span class="math inline">\(j\)</span>,
<span class="math inline">\(\mu_j\)</span>, we estimate it with <span class="math inline">\(\hat{\mu}_j\)</span>, which is just the sample mean for group
<span class="math inline">\(j\)</span>, <span class="math inline">\(\bar{x}_j\)</span>. The model suggests an estimate for each observation that we denote
as <span class="math inline">\(\hat{y}_{ij}\)</span> that we will also call a <strong><em>fitted value</em></strong> based on the model
being considered. The
same estimate is used for all observations in the each group in this model. R tries to help you to
sort out which row of output corresponds to which group by appending the group name
with the variable name. Here, the variable name was <code>Condition</code> and the first group
alphabetically was <em>casual</em>, so R provides a row labeled <code>Conditioncasual</code>
with an estimate of 117.61. The sample means from the seven groups can be seen to
directly match the <code>favstats</code> results presented previously.</p>
<p>The reference-coded version of the same model is more complicated but ends up

giving the same results once we understand what it is doing. It uses a different
parameterization to accomplish this, so has different model output. Here is the model
summary:</p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb216-1" title="1">lm2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>dd)</a>
<a class="sourceLine" id="cb216-2" title="2"><span class="kw">summary</span>(lm2)<span class="op">$</span>coefficients</a></code></pre></div>
<pre><code>##                     Estimate Std. Error     t value    Pr(&gt;|t|)
## (Intercept)      117.6110398   1.071873 109.7247845 0.000000000
## Conditioncommute  -3.0031051   1.480964  -2.0278039 0.042626835
## Conditionhiviz     0.8272234   1.537302   0.5381008 0.590528548
## Conditionnovice   -0.6705193   1.502651  -0.4462242 0.655452292
## Conditionpolice    4.5104792   1.510571   2.9859423 0.002839115
## Conditionpolite   -3.5591965   1.476489  -2.4105807 0.015958695
## Conditionracer    -0.8551713   1.483032  -0.5766371 0.564207492</code></pre>
<p>The estimated model coefficients are <span class="math inline">\(\hat{\alpha} = 117.61\)</span> cm,
<span class="math inline">\(\hat{\tau}_2 =-3.00\)</span> cm, <span class="math inline">\(\hat{\tau}_3=0.83\)</span> cm, and so on up to <span class="math inline">\(\hat{\tau}_7=-0.86\)</span> cm, where R selected group 1
for <em>casual</em>, 2 for <em>commute</em>, 3 for <em>hiviz</em>, all the way up to group 7 for <em>racer</em>. The way you can figure
out the baseline group (group 1 is <em>casual</em> here) is to see which category label
is <em>not present</em> in the reference-coded output. <strong>The baseline level is typically the first group
label alphabetically</strong>, but you should always check this<a href="#fn52" class="footnote-ref" id="fnref52"><sup>52</sup></a>. Based on these definitions,
there are interpretations available for each coefficient. For <span class="math inline">\(\hat{\alpha} = 117.61\)</span> cm, this is an estimate of the mean overtake distance for the <em>casual</em> outfit group.
<span class="math inline">\(\hat{\tau}_2 =-3.00\)</span> cm is the deviation of the <em>commute</em> group’s mean from
the <em>causal</em> group’s mean (specifically, it is <span class="math inline">\(3.00\)</span> cm lower and was a quantity we explored in detail in Chapter <a href="1-chapter2.html#chapter2">1</a> when we just focused on comparing <em>casual</em> and <em>commute</em> groups).
<span class="math inline">\(\hat{\tau}_3=0.83\)</span> cm tells us that the <em>hiviz</em> group mean distance is 0.83 cm higher than the <em>casual</em> group mean and <span class="math inline">\(\hat{\tau}_3=-0.86\)</span> says that the <em>racer</em> sample mean was 0.86 cm lower than for the <em>casual</em> group. These
interpretations are interesting as they directly relate to comparisons of groups with the baseline and lead directly to
reconstructing the estimated means for each group by combining the baseline and
a pertinent deviation as shown in Table <a href="2-2-section3-2.html#tab:Table3-1">1.3</a>.</p>
<p>(ref:tab3-1) Constructing group mean estimates from the reference-coded linear
model estimates.</p>
<table>
<caption><span id="tab:Table3-1">Table 1.3: </span> (ref:tab3-1)</caption>
<colgroup>
<col width="13%" />
<col width="41%" />
<col width="44%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Group</th>
<th align="left">Formula</th>
<th align="left">Estimates</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">casual</td>
<td align="left"><span class="math inline">\(\hat{\alpha}\)</span></td>
<td align="left"><strong>117.61</strong> cm</td>
</tr>
<tr class="even">
<td align="left">commute</td>
<td align="left"><span class="math inline">\(\hat{\alpha}+\hat{\tau}_2\)</span></td>
<td align="left">117.61 - 3.00 = <strong>114.61</strong> cm</td>
</tr>
<tr class="odd">
<td align="left">hiviz</td>
<td align="left"><span class="math inline">\(\hat{\alpha}+\hat{\tau}_3\)</span></td>
<td align="left">117.61 + 0.82 = <strong>118.43</strong> cm</td>
</tr>
</tbody>
</table>
<p>We can also visualize the results of our linear models using what are called
<strong><em>term-plots</em></strong> or <strong><em>effect-plots</em></strong>
 
(from the <code>effects</code> package; <span class="citation">(Fox et al. <a href="#ref-R-effects" role="doc-biblioref">2019</a>)</span>)

as displayed in Figure <a href="2-2-section3-2.html#fig:Figure3-2">1.29</a>. We don’t want to use the word
“effect” for these model components unless we have random assignment in the study

design so we generically call these <strong><em>term-plots</em></strong> as they display terms or
components from the model in hopefully useful ways to aid in model interpretation
even in the presence of complicated model parameterizations. The word “effect” has a causal connotation that we want to avoid as much as possible in non-causal (so non-randomly assigned) situations. Term-plots take an estimated model and show you its estimates along with 95% confidence
intervals generated by the linear model. These confidence intervals may differ from the confidence intervals in the pirate-plots since the pirate-plots make them for each group separately and term-plots are combining information across groups via the estimated model and then doing inferences for individual group means. To make term-plots, you need to install and
load the <code>effects</code> package and then use <code>plot(allEffects(...))</code> functions
together on the <code>lm</code> object called <code>lm2</code> that was estimated above. You can
find the correspondence between the displayed means and the estimates that were
constructed in Table <a href="2-2-section3-2.html#tab:Table3-1">1.3</a>.
</p>
<p>(ref:fig3-2) Plot of the estimated group mean distances from the reference-coded
model for the overtake data from the <code>effects</code> package.</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb218-1" title="1"><span class="kw">require</span>(effects)</a>
<a class="sourceLine" id="cb218-2" title="2"><span class="kw">plot</span>(<span class="kw">allEffects</span>(lm2))</a></code></pre></div>
<div class="figure"><span id="fig:Figure3-2"></span>
<img src="03-oneWayAnova_files/figure-html/Figure3-2-1.png" alt="(ref:fig3-2)" width="672" />
<p class="caption">
Figure 1.29: (ref:fig3-2)
</p>
</div>
<p>In order to assess overall evidence against having the same means for the all groups (vs not having the same means), we
compare either of the previous models (cell-means or reference-coded) to a null
model based on the null hypothesis of <span class="math inline">\(H_0: \mu_1 = \ldots = \mu_J\)</span>, which implies a
model of <span class="math inline">\(\color{red}{y_{ij} = \mu+\varepsilon_{ij}}\)</span> in the cell-means version
where <span class="math inline">\({\color{red}{\mu}}\)</span> is a common mean for all the observations. We will call
this the <b><font color='red'>mean-only</font></b> model since it only has a single mean
in it. In the reference-coded version of the model, we have a null hypothesis of
<span class="math inline">\(H_0: \tau_2 = \ldots = \tau_J = 0\)</span>, so the “mean-only” model is

<span class="math inline">\(\color{purple}{y_{ij} =\boldsymbol{\alpha}+\varepsilon_{ij}}\)</span> with
<span class="math inline">\(\color{purple}{\boldsymbol{\alpha}}\)</span> having the same definition as
<span class="math inline">\(\color{red}{\mu}\)</span> for the cell-means model – it forces a common value for the
mean for all the groups. Moving from the <em>reference-coded</em> model to the <em>mean-only</em>
model is also an example of a situation where we move from a “full” model


to a
“reduced” model by setting some coefficients in the “full” model to 0 and, by doing
this, get a simpler or “reduced” model.

Simple models can be good as they are easier
to interpret, but having a model for <span class="math inline">\(J\)</span> groups that suggests no difference in the
groups is not a very exciting result
in most, but not all, situations<a href="#fn53" class="footnote-ref" id="fnref53"><sup>53</sup></a>. In order for R to provide
results for the mean-only model, we remove the grouping variable, <code>Condition</code>, from
the model formula and just include a “1”. The <code>(Intercept)</code> row of the output
provides the estimate for the mean-only model as a reduced model from either the
cell-means or reference-coded models when we assume that the mean is the same
for all groups:</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb219-1" title="1">lm3 &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span><span class="dv">1</span>, <span class="dt">data=</span>dd)</a>
<a class="sourceLine" id="cb219-2" title="2"><span class="kw">summary</span>(lm3)<span class="op">$</span>coefficients</a></code></pre></div>
<pre><code>##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  117.126  0.3977533 294.469        0</code></pre>
<p>This model provides an estimate of the common mean for all observations of
<span class="math inline">\(117.13 = \hat{\mu} = \hat{\alpha}\)</span> cm. This value also is the dashed horizontal
line in the pirate-plot in Figure <a href="2-1-section3-1.html#fig:Figure3-1">1.28</a>. Some people
call this mean-only model estimate the “grand” or “overall” mean and notationally is represented as <span class="math inline">\(\bar{\bar{y}}\)</span>. </p>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-effects">
<p>Fox, John, Sanford Weisberg, Brad Price, Michael Friendly, and Jangman Hong. 2019. <em>Effects: Effect Displays for Linear, Generalized Linear, and Other Models</em>. <a href="https://CRAN.R-project.org/package=effects">https://CRAN.R-project.org/package=effects</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="50">
<li id="fn50"><p>In Chapter <a href="1-chapter2.html#chapter2">1</a>, we used <code>lm</code> to get these estimates and focused on the estimate of the difference between the second group and the baseline - that was and still is the difference in the sample means. Now there are potentially more than two groups and we need to formalize notation to handle this more complex sitation.<a href="2-2-section3-2.html#fnref50" class="footnote-back">↩</a></p></li>
<li id="fn51"><p>
If you look closely in the code for the rest of the book, any model for a
quantitative response will use this function, suggesting a common thread in
the most commonly used statistical models.<a href="2-2-section3-2.html#fnref51" class="footnote-back">↩</a></p></li>
<li id="fn52"><p>We can and will select the order of the levels of categorical variables as it can make plots easier to interpret.<a href="2-2-section3-2.html#fnref52" class="footnote-back">↩</a></p></li>
<li id="fn53"><p>Suppose we were doing environmental monitoring
and were studying asbestos levels in soils. We might be hoping that the mean-only
model were reasonable to use if the groups being compared were in remediated areas
and in areas known to have never been contaminated.<a href="2-2-section3-2.html#fnref53" class="footnote-back">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="2-1-section3-1.html"><button class="btn btn-default">Previous</button></a>
<a href="2-3-section3-3.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
