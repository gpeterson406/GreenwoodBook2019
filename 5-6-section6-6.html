<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="5.6 Describing relationships with a regression model | Intermediate Statistics with R" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="gpeterson406/Greenwood_Book" />

<meta name="author" content="Mark C Greenwood" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="5.6 Describing relationships with a regression model | Intermediate Statistics with R">

<title>5.6 Describing relationships with a regression model | Intermediate Statistics with R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li class="has-sub"><a href="acknowledgments.html#acknowledgments">Acknowledgments</a><ul>
<li><a href="0-1-section1-5.html#section1-5"><span class="toc-section-number">0.1</span> Summary of important R code</a></li>
</ul></li>
<li class="has-sub"><a href="1-chapter2.html#chapter2"><span class="toc-section-number">1</span> (R)e-Introduction to statistics</a><ul>
<li><a href="1-1-section2-1.html#section2-1"><span class="toc-section-number">1.1</span> Histograms, boxplots, and density curves</a></li>
<li><a href="1-2-section2-2.html#section2-2"><span class="toc-section-number">1.2</span> Pirate-plots</a></li>
<li><a href="1-3-section2-3.html#section2-3"><span class="toc-section-number">1.3</span> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li><a href="1-4-section2-4.html#section2-4"><span class="toc-section-number">1.4</span> Permutation testing for the two sample mean situation</a></li>
<li><a href="1-5-section2-5.html#section2-5"><span class="toc-section-number">1.5</span> Hypothesis testing (general)</a></li>
<li><a href="1-6-section2-6.html#section2-6"><span class="toc-section-number">1.6</span> Connecting randomization (nonparametric) and parametric tests</a></li>
<li><a href="1-7-section2-7.html#section2-7"><span class="toc-section-number">1.7</span> Second example of permutation tests</a></li>
<li><a href="1-8-section2-8.html#section2-8"><span class="toc-section-number">1.8</span> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li><a href="1-9-section2-9.html#section2-9"><span class="toc-section-number">1.9</span> Confidence intervals and bootstrapping</a></li>
<li><a href="1-10-section2-10.html#section2-10"><span class="toc-section-number">1.10</span> Bootstrap confidence intervals for difference in GPAs</a></li>
<li><a href="1-11-section2-11.html#section2-11"><span class="toc-section-number">1.11</span> Chapter summary</a></li>
<li><a href="1-12-section2-12.html#section2-12"><span class="toc-section-number">1.12</span> Summary of important R code</a></li>
<li><a href="1-13-section2-13.html#section2-13"><span class="toc-section-number">1.13</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="2-chapter3.html#chapter3"><span class="toc-section-number">2</span> One-Way ANOVA</a><ul>
<li><a href="2-1-section3-1.html#section3-1"><span class="toc-section-number">2.1</span> Situation</a></li>
<li><a href="2-2-section3-2.html#section3-2"><span class="toc-section-number">2.2</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li><a href="2-3-section3-3.html#section3-3"><span class="toc-section-number">2.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li><a href="2-4-section3-4.html#section3-4"><span class="toc-section-number">2.4</span> ANOVA model diagnostics including QQ-plots</a></li>
<li><a href="2-5-section3-5.html#section3-5"><span class="toc-section-number">2.5</span> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li><a href="2-6-section3-6.html#section3-6"><span class="toc-section-number">2.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li><a href="2-7-section3-7.html#section3-7"><span class="toc-section-number">2.7</span> Pair-wise comparisons for the Overtake data</a></li>
<li><a href="2-8-section3-8.html#section3-8"><span class="toc-section-number">2.8</span> Chapter summary</a></li>
<li><a href="2-9-section3-9.html#section3-9"><span class="toc-section-number">2.9</span> Summary of important R code</a></li>
<li><a href="2-10-section3-10.html#section3-10"><span class="toc-section-number">2.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="3-chapter4.html#chapter4"><span class="toc-section-number">3</span> Two-Way ANOVA</a><ul>
<li><a href="3-1-section4-1.html#section4-1"><span class="toc-section-number">3.1</span> Situation</a></li>
<li><a href="3-2-section4-2.html#section4-2"><span class="toc-section-number">3.2</span> Designing a two-way experiment and visualizing results</a></li>
<li><a href="3-3-section4-3.html#section4-3"><span class="toc-section-number">3.3</span> Two-Way ANOVA models and hypothesis tests</a></li>
<li><a href="3-4-section4-4.html#section4-4"><span class="toc-section-number">3.4</span> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li><a href="3-5-section4-5.html#section4-5"><span class="toc-section-number">3.5</span> Observational study example: The Psychology of Debt</a></li>
<li><a href="3-6-section4-6.html#section4-6"><span class="toc-section-number">3.6</span> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li><a href="3-7-section4-7.html#section4-7"><span class="toc-section-number">3.7</span> Chapter summary</a></li>
<li><a href="3-8-section4-8.html#section4-8"><span class="toc-section-number">3.8</span> Summary of important R code</a></li>
<li><a href="3-9-section4-9.html#section4-9"><span class="toc-section-number">3.9</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="4-chapter5.html#chapter5"><span class="toc-section-number">4</span> Chi-square tests</a><ul>
<li><a href="4-1-section5-1.html#section5-1"><span class="toc-section-number">4.1</span> Situation, contingency tables, and tableplots</a></li>
<li><a href="4-2-section5-2.html#section5-2"><span class="toc-section-number">4.2</span> Homogeneity test hypotheses</a></li>
<li><a href="4-3-section5-3.html#section5-3"><span class="toc-section-number">4.3</span> Independence test hypotheses</a></li>
<li><a href="4-4-section5-4.html#section5-4"><span class="toc-section-number">4.4</span> Models for R by C tables</a></li>
<li><a href="4-5-section5-5.html#section5-5"><span class="toc-section-number">4.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="4-6-section5-6.html#section5-6"><span class="toc-section-number">4.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="4-7-section5-7.html#section5-7"><span class="toc-section-number">4.7</span> Examining residuals for the source of differences</a></li>
<li><a href="4-8-section5-8.html#section5-8"><span class="toc-section-number">4.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li><a href="4-9-section5-9.html#section5-9"><span class="toc-section-number">4.9</span> Political party and voting results: Complete analysis</a></li>
<li><a href="4-10-section5-10.html#section5-10"><span class="toc-section-number">4.10</span> Is cheating and lying related in students?</a></li>
<li><a href="4-11-section5-11.html#section5-11"><span class="toc-section-number">4.11</span> Analyzing a stratified random sample of California schools</a></li>
<li><a href="4-12-section5-12.html#section5-12"><span class="toc-section-number">4.12</span> Chapter summary</a></li>
<li><a href="4-13-section5-13.html#section5-13"><span class="toc-section-number">4.13</span> Summary of important R commands</a></li>
<li><a href="4-14-section5-14.html#section5-14"><span class="toc-section-number">4.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="5-chapter6.html#chapter6"><span class="toc-section-number">5</span> Correlation and Simple Linear Regression</a><ul>
<li><a href="5-1-section6-1.html#section6-1"><span class="toc-section-number">5.1</span> Relationships between two quantitative variables</a></li>
<li><a href="5-2-section6-2.html#section6-2"><span class="toc-section-number">5.2</span> Estimating the correlation coefficient</a></li>
<li><a href="5-3-section6-3.html#section6-3"><span class="toc-section-number">5.3</span> Relationships between variables by groups</a></li>
<li><a href="5-4-section6-4.html#section6-4"><span class="toc-section-number">5.4</span> Inference for the correlation coefficient</a></li>
<li><a href="5-5-section6-5.html#section6-5"><span class="toc-section-number">5.5</span> Are tree diameters related to tree heights?</a></li>
<li><a href="5-6-section6-6.html#section6-6"><span class="toc-section-number">5.6</span> Describing relationships with a regression model</a></li>
<li><a href="5-7-section6-7.html#section6-7"><span class="toc-section-number">5.7</span> Least Squares Estimation</a></li>
<li><a href="5-8-section6-8.html#section6-8"><span class="toc-section-number">5.8</span> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li><a href="5-9-section6-9.html#section6-9"><span class="toc-section-number">5.9</span> Outliers: leverage and influence</a></li>
<li><a href="5-10-section6-10.html#section6-10"><span class="toc-section-number">5.10</span> Residual diagnostics – setting the stage for inference</a></li>
<li><a href="5-11-section6-11.html#section6-11"><span class="toc-section-number">5.11</span> Old Faithful discharge and waiting times</a></li>
<li><a href="5-12-section6-12.html#section6-12"><span class="toc-section-number">5.12</span> Chapter summary</a></li>
<li><a href="5-13-section6-13.html#section6-13"><span class="toc-section-number">5.13</span> Summary of important R code</a></li>
<li><a href="5-14-section6-14.html#section6-14"><span class="toc-section-number">5.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="6-chapter7.html#chapter7"><span class="toc-section-number">6</span> Simple linear regression inference</a><ul>
<li><a href="6-1-section7-1.html#section7-1"><span class="toc-section-number">6.1</span> Model</a></li>
<li><a href="6-2-section7-2.html#section7-2"><span class="toc-section-number">6.2</span> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li><a href="6-3-section7-3.html#section7-3"><span class="toc-section-number">6.3</span> Bozeman temperature trend</a></li>
<li><a href="6-4-section7-4.html#section7-4"><span class="toc-section-number">6.4</span> Randomization-based inferences for the slope coefficient</a></li>
<li><a href="6-5-section7-5.html#section7-5"><span class="toc-section-number">6.5</span> Transformations part I: Linearizing relationships</a></li>
<li><a href="6-6-section7-6.html#section7-6"><span class="toc-section-number">6.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li><a href="6-7-section7-7.html#section7-7"><span class="toc-section-number">6.7</span> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li><a href="6-8-section7-8.html#section7-8"><span class="toc-section-number">6.8</span> Chapter summary</a></li>
<li><a href="6-9-section7-9.html#section7-9"><span class="toc-section-number">6.9</span> Summary of important R code</a></li>
<li><a href="6-10-section7-10.html#section7-10"><span class="toc-section-number">6.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="7-chapter8.html#chapter8"><span class="toc-section-number">7</span> Multiple linear regression</a><ul>
<li><a href="7-1-section8-1.html#section8-1"><span class="toc-section-number">7.1</span> Going from SLR to MLR</a></li>
<li><a href="7-2-section8-2.html#section8-2"><span class="toc-section-number">7.2</span> Validity conditions in MLR</a></li>
<li><a href="7-3-section8-3.html#section8-3"><span class="toc-section-number">7.3</span> Interpretation of MLR terms</a></li>
<li><a href="7-4-section8-4.html#section8-4"><span class="toc-section-number">7.4</span> Comparing multiple regression models</a></li>
<li><a href="7-5-section8-5.html#section8-5"><span class="toc-section-number">7.5</span> General recommendations for MLR interpretations and VIFs</a></li>
<li><a href="7-6-section8-6.html#section8-6"><span class="toc-section-number">7.6</span> MLR inference: Parameter inferences using the t-distribution</a></li>
<li><a href="7-7-section8-7.html#section8-7"><span class="toc-section-number">7.7</span> Overall F-test in multiple linear regression</a></li>
<li><a href="7-8-section8-8.html#section8-8"><span class="toc-section-number">7.8</span> Case study: First year college GPA and SATs</a></li>
<li><a href="7-9-section8-9.html#section8-9"><span class="toc-section-number">7.9</span> Different intercepts for different groups: MLR with indicator variables</a></li>
<li><a href="7-10-section8-10.html#section8-10"><span class="toc-section-number">7.10</span> Additive MLR with more than two groups: Headache example</a></li>
<li><a href="7-11-section8-11.html#section8-11"><span class="toc-section-number">7.11</span> Different slopes and different intercepts</a></li>
<li><a href="7-12-section8-12.html#section8-12"><span class="toc-section-number">7.12</span> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li><a href="7-13-section8-13.html#section8-13"><span class="toc-section-number">7.13</span> AICs for model selection</a></li>
<li><a href="7-14-section8-14.html#section8-14"><span class="toc-section-number">7.14</span> Case study: Forced expiratory volume model selection using AICs</a></li>
<li><a href="7-15-section8-15.html#section8-15"><span class="toc-section-number">7.15</span> Chapter summary</a></li>
<li><a href="7-16-section8-16.html#section8-16"><span class="toc-section-number">7.16</span> Summary of important R code</a></li>
<li><a href="7-17-section8-17.html#section8-17"><span class="toc-section-number">7.17</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="8-chapter9.html#chapter9"><span class="toc-section-number">8</span> Case studies</a><ul>
<li><a href="8-1-section9-1.html#section9-1"><span class="toc-section-number">8.1</span> Overview of material covered</a></li>
<li><a href="8-2-section9-2.html#section9-2"><span class="toc-section-number">8.2</span> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li><a href="8-3-section9-3.html#section9-3"><span class="toc-section-number">8.3</span> Ants learn to rely on more informative attributes during decision-making</a></li>
<li><a href="8-4-section9-4.html#section9-4"><span class="toc-section-number">8.4</span> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li><a href="8-5-section9-5.html#section9-5"><span class="toc-section-number">8.5</span> What do didgeridoos really do about sleepiness?</a></li>
<li><a href="8-6-section9-6.html#section9-6"><span class="toc-section-number">8.6</span> General summary</a></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="section6-6" class="section level2">
<h2><span class="header-section-number">5.6</span> Describing relationships with a regression model</h2>
<p>When the relationship appears to
be relatively linear, it makes sense to estimate and then interpret a line to
represent the relationship between the variables. This line is called a
<strong><em>regression line</em></strong> and involves finding a line that best fits (explains
variation in) the response variable for the
given values of the explanatory variable.

For regression, it matters which
variable you choose for <span class="math inline">\(x\)</span> and which you choose for <span class="math inline">\(y\)</span> – for correlation
it did not matter. This regression line describes the “effect” of <span class="math inline">\(x\)</span> on
<span class="math inline">\(y\)</span> and also provides an equation for predicting values of <span class="math inline">\(y\)</span> for given
values of <span class="math inline">\(x\)</span>. The <em>Beers</em> and <em>BAC</em> data provide a nice example to start
our exploration of regression models. The beer consumption is a clear
explanatory variable,
detectable in the story because (1) it was randomly assigned to subjects and
(2) basic science supports beer consumption amount being an explanatory
variable for <em>BAC</em>. In some situations, this will not be so clear, but look for
random assignment or scientific logic to guide your choices of variables as
explanatory or response<a href="#fn97" class="footnote-ref" id="fnref97"><sup>97</sup></a>. Regression lines are actually provided by
default in the <code>scatterplot</code> function with the <code>reg.line=T</code> option or just
omitting <code>reg.line=F</code> from the previous versions of the code since it is a
default option to provide the lines.</p>
<p>(ref:fig6-13) Scatterplot with estimated regression line for the <em>Beers</em> and
<em>BAC</em> data. Horizontal dashed lines for the predicted BAC for 4 and 5 beers consumed.</p>
<div class="sourceCode" id="cb542"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb542-1" title="1"><span class="kw">scatterplot</span>(BAC<span class="op">~</span>Beers, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,.<span class="dv">2</span>), <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">9</span>), <span class="dt">data=</span>BB,</a>
<a class="sourceLine" id="cb542-2" title="2">            <span class="dt">boxplot=</span>F, <span class="dt">main=</span><span class="st">&quot;Scatterplot with regression line&quot;</span>,</a>
<a class="sourceLine" id="cb542-3" title="3">            <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">smooth=</span>F)</a>
<a class="sourceLine" id="cb542-4" title="4"><span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">1</span><span class="op">:</span><span class="dv">9</span>, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>)</a>
<a class="sourceLine" id="cb542-5" title="5"><span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">c</span>(<span class="fl">0.05914</span>,<span class="fl">0.0771</span>), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure"><span id="fig:Figure6-13"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-13-1.png" alt="(ref:fig6-13)" width="576" />
<p class="caption">
Figure 1.112: (ref:fig6-13)
</p>
</div>
<p>The equation for a line is <span class="math inline">\(y=a+bx\)</span>, or maybe <span class="math inline">\(y=mx+b\)</span>. In the version
<span class="math inline">\(mx+b\)</span> you learned that <span class="math inline">\(m\)</span> is a slope coefficient that relates a
change in <span class="math inline">\(x\)</span> to changes in <span class="math inline">\(y\)</span> and that <span class="math inline">\(b\)</span> is a y-intercept (the
value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x\)</span> is 0). In Figure <a href="5-6-section6-6.html#fig:Figure6-13">1.112</a>, two extra
horizontal lines are added to help you see the defining characteristics of the line. The
slope, whatever letter you use, is the change in <span class="math inline">\(y\)</span> for a one-unit
increase in <span class="math inline">\(x\)</span>. Here, the slope is the change in <code>BAC</code> for a 1 beer
increase in <code>Beers</code>, such as the change from 4 to 5 beers. The
y-values (blue, dashed lines) for <code>Beers</code> = 4 and 5 go from 0.059 to
0.077. This means that for a 1 beer increase (+1 unit change in <span class="math inline">\(x\)</span>), the
<code>BAC</code> goes up by <span class="math inline">\(0.077-0.059=0.018\)</span> (+0.018 unit change in <span class="math inline">\(y\)</span>).
We can also try to find the y-intercept on the graph by looking for the
<code>BAC</code> level for 0 <code>Beers</code> consumed. The y-value (<code>BAC</code>) ends up
being around -0.01 if you extend the regression line to <code>Beers</code>=0.
You might assume that the <code>BAC</code> should be 0 for <code>Beers</code>=0 but the
researchers did not observe any students at 0 <code>Beers</code>, so we don’t
really know what the <code>BAC</code> might be at this value. We have to
use our line to <strong><em>predict</em></strong> this value. This ends up providing a
prediction below 0 – an impossible value for <em>BAC</em>. If the
y-intercept were positive, it would suggest that the students
has a <em>BAC</em> over 0 even without drinking. </p>
<p>The numbers reported were very
accurate because we weren’t using the plot alone to generate the
values – we
were using a linear model to estimate the equation to
describe the
relationship between <code>Beers</code> and <code>BAC</code>. In statistics, we estimate
“<span class="math inline">\(m\)</span>” and “<span class="math inline">\(b\)</span>”. We also write the equation starting with the y-intercept
and use slightly different notation that allows us to extend to more
complicated models with more variables.


Specifically, the estimated
regression equation is <span class="math inline">\(\hat{y} = b_0 + b_1x\)</span>, where</p>
<ul>
<li><p><span class="math inline">\(\hat{y}\)</span> is the estimated value of <span class="math inline">\(y\)</span> for a given <span class="math inline">\(x\)</span>,</p></li>
<li><p><span class="math inline">\(b_0\)</span> is the estimated y-intercept (predicted value of <span class="math inline">\(y\)</span> when
<span class="math inline">\(x\)</span> is 0),</p></li>
<li><p><span class="math inline">\(b_1\)</span> is the estimated slope coefficient, and</p></li>
<li><p><span class="math inline">\(x\)</span> is the explanatory variable.</p></li>
</ul>
<p>One of the differences between when you learned equations in algebra
classes and our
situation is that the line is not a perfect description of the relationship
between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> – it is an “on average” description and will usually
leave differences between the line and the observations, which we call
residuals <span class="math inline">\((e = y-\hat{y})\)</span>.

We worked with residuals in the ANOVA<a href="#fn98" class="footnote-ref" id="fnref98"><sup>98</sup></a>
material. The residuals describe the vertical distance in the scatterplot between
our model (regression line) and the actual observed data point. The lack of a
perfect fit of the line to the observations distinguishes statistical equations
from those you learned in math classes. The equations work the same, but we
have to modify interpretations of the coefficients to reflect this.</p>
<p>We also tie this estimated model to a theoretical or <strong><em>population
regression model</em></strong>:
</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1x_i+\varepsilon_i\]</span></p>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(y_i\)</span> is the observed response for the <span class="math inline">\(i^{th}\)</span> observation,</p></li>
<li><p><span class="math inline">\(x_i\)</span> is the observed value of the explanatory variable for the
<span class="math inline">\(i^{th}\)</span> observation,</p></li>
<li><p><span class="math inline">\(\beta_0 + \beta_1x_i\)</span> is the true mean function evaluated at <span class="math inline">\(x_i\)</span>,</p></li>
<li><p><span class="math inline">\(\beta_0\)</span> is the true (or population) y-intercept,</p></li>
<li><p><span class="math inline">\(\beta_1\)</span> is the true (or population) slope coefficient, and</p></li>
<li><p>the deviations, <span class="math inline">\(\varepsilon_i\)</span>, are assumed to be independent and
normally distributed with mean 0 and standard deviation <span class="math inline">\(\sigma\)</span> or,
more compactly, <span class="math inline">\(\varepsilon_i \sim N(0,\sigma^2)\)</span>.</p></li>
</ul>
<p>This presents another version of the linear model from Chapters <a href="1-chapter2.html#chapter2">1</a>, <a href="2-chapter3.html#chapter3"><strong>??</strong></a>,
and <a href="3-chapter4.html#chapter4"><strong>??</strong></a>, now with a
quantitative explanatory variable instead of categorical explanatory variable(s). This chapter
focuses mostly on the estimated regression coefficients, but remember that we
are doing statistics and our desire is to make inferences to a larger population.
So, estimated coefficients, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, are approximations to
theoretical coefficients, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. In other words,
<span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>
are the statistics that try to estimate the true population parameters
<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, respectively.</p>
<p>To get estimated regression coefficients, we use the <code>lm</code> function
and our standard <code>lm(y~x, data=...)</code> setup.

This is the same function
used to estimate our ANOVA models and much of this
will look familiar. In fact, the ties between ANOVA and regression are
deep and fundamental but not the topic of this section. For the <em>Beers</em>
and <em>BAC</em> example, the <strong><em>estimated regression coefficients</em></strong> can be
found from:</p>

<div class="sourceCode" id="cb543"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb543-1" title="1">m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(BAC<span class="op">~</span>Beers, <span class="dt">data=</span>BB)</a>
<a class="sourceLine" id="cb543-2" title="2">m1</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = BAC ~ Beers, data = BB)
## 
## Coefficients:
## (Intercept)        Beers  
##    -0.01270      0.01796</code></pre>
<p>More often, we will extract these from the <strong><em>coefficient table</em></strong> produced
by a model <code>summary</code>: </p>

<div class="sourceCode" id="cb545"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb545-1" title="1"><span class="kw">summary</span>(m1)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = BAC ~ Beers, data = BB)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.027118 -0.017350  0.001773  0.008623  0.041027 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -0.012701   0.012638  -1.005    0.332
## Beers        0.017964   0.002402   7.480 2.97e-06
## 
## Residual standard error: 0.02044 on 14 degrees of freedom
## Multiple R-squared:  0.7998, Adjusted R-squared:  0.7855 
## F-statistic: 55.94 on 1 and 14 DF,  p-value: 2.969e-06</code></pre>
<p>From either version of the output, you can find the estimated y-intercept
in the <code>(Intercept)</code> part of the output and the slope coefficient in the
<code>Beers</code> part of the output. So <span class="math inline">\(b_0 = -0.0127\)</span>, <span class="math inline">\(b_1=0.01796\)</span>, and
the <strong><em>estimated regression equation</em></strong> is</p>
<p><span class="math display">\[\widehat{\text{BAC}}_i = -0.0127 + 0.01796\cdot\text{Beers}_i.\]</span></p>
<p>This is the equation that was plotted in Figure <a href="5-6-section6-6.html#fig:Figure6-13">1.112</a>.
In writing out the equation, it is
good to replace <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> with the variable names to make the predictor
and response variables clear. <strong>If you prefer to write all equations with</strong>
<span class="math inline">\(\boldsymbol{x}\)</span> <strong>and</strong> <span class="math inline">\(\boldsymbol{y}\)</span><strong>, you need to define</strong>
<span class="math inline">\(\boldsymbol{x}\)</span> <strong>and</strong> <span class="math inline">\(\boldsymbol{y}\)</span> <strong>or else these equations are not
clearly defined.</strong></p>
<p>There is a general interpretation
for the slope coefficient that you will need to master. In general, we
interpret the slope coefficient as: </p>
<ul>
<li><strong>Slope interpretation (general):</strong> For a 1 <strong>[<em>unit of X</em>]</strong> increase in
<strong><em>X</em></strong>, we expect, <em>on average</em>, a <span class="math inline">\(\boldsymbol{b_1}\)</span> <strong>[<em>unit of Y</em>]</strong> change
in <strong><em>Y</em></strong>.</li>
</ul>
<p>Figure <a href="5-6-section6-6.html#fig:Figure6-14">1.113</a> can help you think about the
different sorts of slope coefficients we might need to interpret, both
providing changes in the response variable for 1 unit increases in the
predictor variable.</p>

<div class="figure"><span id="fig:Figure6-14"></span>
<img src="chapter6_files/image047.png" alt="Diagram of interpretation of slope coefficients." width="583" />
<p class="caption">
Figure 1.113: Diagram of interpretation of slope coefficients.
</p>
</div>
<p>Applied to this problem, for each additional 1 beer consumed, we expect
a 0.018 gram per dL change in the <em>BAC</em> <em>on average</em>. Using “change” in
the interpretation for what happened in the response
allows you to use the same template for the interpretation even with
negative slopes –
be careful about saying “decrease” when the slope is negative as you can create
a double-negative and end up implying an increase… Note also that you need to
carefully incorporate the units of <span class="math inline">\(x\)</span> and the units of <span class="math inline">\(y\)</span> to make the
interpretation clear. For example, if the change in <em>BAC</em> for 1 beer increase
is 0.018, then we could also modify the size of the change in <span class="math inline">\(x\)</span> to be a 10 beer increase and then the estimated
change in <em>BAC</em> is <span class="math inline">\(10*0.018 = 0.18\)</span> g/dL. Both are correct as long as you are
clear about the change in <span class="math inline">\(x\)</span> you are talking
about. Typically, we will just use the units used in the original variables and
only change the scale of “change in <span class="math inline">\(x\)</span>” when it provides an interpretation we
are particularly interested in.</p>
<p>Similarly, the general interpretation for a y-intercept is:
</p>
<ul>
<li><strong>Y-intercept interpretation (general):</strong> For <strong><em>X</em></strong>= 0 <strong>[<em>units of X</em>]</strong>,
we expect, on average, <span class="math inline">\(\boldsymbol{b_0}\)</span> <strong>[<em>units of Y</em>]</strong> <strong>in</strong> <strong><em>Y</em></strong>.</li>
</ul>
<p>Again, applied to the <em>BAC</em> data set: For 0 beers for <em>Beers</em> consumed,
we expect, on
average, -0.012 g/dL <em>BAC</em>. The y-intercept interpretation is often less
interesting than the slope interpretation but can be interesting in some
situations. Here, it is predicting average <em>BAC</em> for <code>Beers</code>=0, which
is a value outside the scope of the <span class="math inline">\(x\text{&#39;s}\)</span> (<em>Beers</em> was observed
between 1 and 9). Prediction outside the scope of the predictor values is
called <strong><em>extrapolation</em></strong>. Extrapolation is dangerous at best and
misleading at worst. That said, if you are asked to
interpret the y-intercept you should still interpret it, but it is also good to
note if it is outside of the region where we had observations on the
explanatory variable. Another example is useful for practicing how to do these
interpretations.</p>
<p>In the Australian Athlete data, we
saw a weak negative relationship between <em>Body Fat</em> (% body weight that
is fat) and <em>Hematocrit</em> (% red blood cells in the blood). The scatterplot
in Figure <a href="5-6-section6-6.html#fig:Figure6-15">1.114</a> shows just the
results for the female athletes along with the regression line which has a
negative slope coefficient. The estimated regression coefficients are found
using the <code>lm</code> function:
</p>
<div class="sourceCode" id="cb547"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb547-1" title="1">aisR2 &lt;-<span class="st"> </span>ais[<span class="op">-</span><span class="kw">c</span>(<span class="dv">56</span>,<span class="dv">166</span>), <span class="kw">c</span>(<span class="st">&quot;Ht&quot;</span>,<span class="st">&quot;Hc&quot;</span>,<span class="st">&quot;Bfat&quot;</span>,<span class="st">&quot;Sex&quot;</span>)]</a>
<a class="sourceLine" id="cb547-2" title="2">m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Hc<span class="op">~</span>Bfat, <span class="dt">data=</span><span class="kw">subset</span>(aisR2,Sex<span class="op">==</span><span class="dv">1</span>)) <span class="co">#Results for Females </span></a></code></pre></div>

<div class="sourceCode" id="cb548"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb548-1" title="1"><span class="kw">summary</span>(m2)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Hc ~ Bfat, data = subset(aisR2, Sex == 1))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.2399 -2.2132 -0.1061  1.8917  6.6453 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 42.01378    0.93269  45.046   &lt;2e-16
## Bfat        -0.08504    0.05067  -1.678   0.0965
## 
## Residual standard error: 2.598 on 97 degrees of freedom
## Multiple R-squared:  0.02822,    Adjusted R-squared:  0.0182 
## F-statistic: 2.816 on 1 and 97 DF,  p-value: 0.09653</code></pre>
<div class="sourceCode" id="cb550"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb550-1" title="1"><span class="kw">scatterplot</span>(Hc<span class="op">~</span>Bfat, <span class="dt">data=</span><span class="kw">subset</span>(aisR2,Sex<span class="op">==</span><span class="dv">1</span>), <span class="dt">smooth=</span>F,</a>
<a class="sourceLine" id="cb550-2" title="2">            <span class="dt">main=</span><span class="st">&quot;Scatterplot of Body Fat vs Hematocrit for Female Athletes&quot;</span>,</a>
<a class="sourceLine" id="cb550-3" title="3">            <span class="dt">ylab=</span><span class="st">&quot;Hc (% blood)&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Body fat (% weight)&quot;</span>)</a></code></pre></div>
<div class="figure"><span id="fig:Figure6-15"></span>
<img src="06-correlationAndSimpleLinearRegression_files/figure-html/Figure6-15-1.png" alt="Scatterplot of Hematocrit versus Body Fat for female athletes." width="576" />
<p class="caption">
Figure 1.114: Scatterplot of Hematocrit versus Body Fat for female athletes.
</p>
</div>
<p>Based on these results, the estimated regression equation is
<span class="math inline">\(\widehat{\text{Hc}}_i = 42.014 - 0.085\cdot\text{BodyFat}_i\)</span> with <span class="math inline">\(b_0 = 42.014\)</span>
and <span class="math inline">\(b_1 = 0.085\)</span>. The slope coefficient interpretation is: For a one
percent increase in body fat, we expect, on average, a -0.085% (blood) change
in Hematocrit for Australian female athletes. For the y-intercept, the
interpretation is: For a 0% body fat female athlete, we expect a Hematocrit of
42.014% on average. Again, this y-intercept involves extrapolation to a region
of <span class="math inline">\(x\)</span>’s that we did not observed. None of the athletes had body fat below 5% so we don’t know what would happen to
the hematocrit of an athlete that had no body fat except that it probably would not
continue to follow a linear relationship.</p>
</div>
<div class="footnotes">
<hr />
<ol start="97">
<li id="fn97"><p>Even with clear scientific logic, we sometimes make choices
to flip the model directions to facilitate different types of analyses. In
<span class="citation">Vsevolozhskaya et al. (<a href="#ref-Vsevol2014" role="doc-biblioref">2014</a>)</span> we looked at genomic differences based on obesity groups, even
though we were really interested in exploring how gene-level differences
explained differences in obesity.<a href="5-6-section6-6.html#fnref97" class="footnote-back">↩</a></p></li>
<li id="fn98"><p>The
residuals from these methods and ANOVA are the same because they all come
from linear models but are completely different from the standardized residuals
used in the Chi-square material in Chapter <a href="4-chapter5.html#chapter5"><strong>??</strong></a>.<a href="5-6-section6-6.html#fnref98" class="footnote-back">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="5-5-section6-5.html"><button class="btn btn-default">Previous</button></a>
<a href="5-7-section6-7.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
