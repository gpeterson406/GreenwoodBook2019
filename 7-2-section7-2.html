<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="7.2 Confidence interval and hypothesis tests for the slope and intercept | Intermediate Statistics with R" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="gpeterson406/Greenwood_Book" />

<meta name="author" content="Mark C Greenwood" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="7.2 Confidence interval and hypothesis tests for the slope and intercept | Intermediate Statistics with R">

<title>7.2 Confidence interval and hypothesis tests for the slope and intercept | Intermediate Statistics with R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li class="has-sub"><a href="1-chapter1.html#chapter1"><span class="toc-section-number">1</span> Preface</a><ul>
<li><a href="1-1-section1-1.html#section1-1"><span class="toc-section-number">1.1</span> Overview of methods</a></li>
<li><a href="1-2-section1-2.html#section1-2"><span class="toc-section-number">1.2</span> Getting started in R</a></li>
<li><a href="1-3-section1-3.html#section1-3"><span class="toc-section-number">1.3</span> Basic summary statistics, histograms, and boxplots using R</a></li>
<li><a href="1-4-section1-4.html#section1-4"><span class="toc-section-number">1.4</span> Chapter summary</a></li>
<li><a href="1-5-section1-5.html#section1-5"><span class="toc-section-number">1.5</span> Summary of important R code</a></li>
<li><a href="1-6-section1-6.html#section1-6"><span class="toc-section-number">1.6</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="2-chapter2.html#chapter2"><span class="toc-section-number">2</span> (R)e-Introduction to statistics</a><ul>
<li><a href="2-1-section2-1.html#section2-1"><span class="toc-section-number">2.1</span> Histograms, boxplots, and density curves</a></li>
<li><a href="2-2-section2-2.html#section2-2"><span class="toc-section-number">2.2</span> Pirate-plots</a></li>
<li><a href="2-3-section2-3.html#section2-3"><span class="toc-section-number">2.3</span> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li><a href="2-4-section2-4.html#section2-4"><span class="toc-section-number">2.4</span> Permutation testing for the two sample mean situation</a></li>
<li><a href="2-5-section2-5.html#section2-5"><span class="toc-section-number">2.5</span> Hypothesis testing (general)</a></li>
<li><a href="2-6-section2-6.html#section2-6"><span class="toc-section-number">2.6</span> Connecting randomization (nonparametric) and parametric tests</a></li>
<li><a href="2-7-section2-7.html#section2-7"><span class="toc-section-number">2.7</span> Second example of permutation tests</a></li>
<li><a href="2-8-section2-8.html#section2-8"><span class="toc-section-number">2.8</span> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li><a href="2-9-section2-9.html#section2-9"><span class="toc-section-number">2.9</span> Confidence intervals and bootstrapping</a></li>
<li><a href="2-10-section2-10.html#section2-10"><span class="toc-section-number">2.10</span> Bootstrap confidence intervals for difference in GPAs</a></li>
<li><a href="2-11-section2-11.html#section2-11"><span class="toc-section-number">2.11</span> Chapter summary</a></li>
<li><a href="2-12-section2-12.html#section2-12"><span class="toc-section-number">2.12</span> Summary of important R code</a></li>
<li><a href="2-13-section2-13.html#section2-13"><span class="toc-section-number">2.13</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="3-chapter3.html#chapter3"><span class="toc-section-number">3</span> One-Way ANOVA</a><ul>
<li><a href="3-1-section3-1.html#section3-1"><span class="toc-section-number">3.1</span> Situation</a></li>
<li><a href="3-2-section3-2.html#section3-2"><span class="toc-section-number">3.2</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li><a href="3-3-section3-3.html#section3-3"><span class="toc-section-number">3.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li><a href="3-4-section3-4.html#section3-4"><span class="toc-section-number">3.4</span> ANOVA model diagnostics including QQ-plots</a></li>
<li><a href="3-5-section3-5.html#section3-5"><span class="toc-section-number">3.5</span> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li><a href="3-6-section3-6.html#section3-6"><span class="toc-section-number">3.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li><a href="3-7-section3-7.html#section3-7"><span class="toc-section-number">3.7</span> Pair-wise comparisons for the Overtake data</a></li>
<li><a href="3-8-section3-8.html#section3-8"><span class="toc-section-number">3.8</span> Chapter summary</a></li>
<li><a href="3-9-section3-9.html#section3-9"><span class="toc-section-number">3.9</span> Summary of important R code</a></li>
<li><a href="3-10-section3-10.html#section3-10"><span class="toc-section-number">3.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="4-chapter4.html#chapter4"><span class="toc-section-number">4</span> Two-Way ANOVA</a><ul>
<li><a href="4-1-section4-1.html#section4-1"><span class="toc-section-number">4.1</span> Situation</a></li>
<li><a href="4-2-section4-2.html#section4-2"><span class="toc-section-number">4.2</span> Designing a two-way experiment and visualizing results</a></li>
<li><a href="4-3-section4-3.html#section4-3"><span class="toc-section-number">4.3</span> Two-Way ANOVA models and hypothesis tests</a></li>
<li><a href="4-4-section4-4.html#section4-4"><span class="toc-section-number">4.4</span> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li><a href="4-5-section4-5.html#section4-5"><span class="toc-section-number">4.5</span> Observational study example: The Psychology of Debt</a></li>
<li><a href="4-6-section4-6.html#section4-6"><span class="toc-section-number">4.6</span> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li><a href="4-7-section4-7.html#section4-7"><span class="toc-section-number">4.7</span> Chapter summary</a></li>
<li><a href="4-8-section4-8.html#section4-8"><span class="toc-section-number">4.8</span> Summary of important R code</a></li>
<li><a href="4-9-section4-9.html#section4-9"><span class="toc-section-number">4.9</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="5-chapter5.html#chapter5"><span class="toc-section-number">5</span> Chi-square tests</a><ul>
<li><a href="5-1-section5-1.html#section5-1"><span class="toc-section-number">5.1</span> Situation, contingency tables, and tableplots</a></li>
<li><a href="5-2-section5-2.html#section5-2"><span class="toc-section-number">5.2</span> Homogeneity test hypotheses</a></li>
<li><a href="5-3-section5-3.html#section5-3"><span class="toc-section-number">5.3</span> Independence test hypotheses</a></li>
<li><a href="5-4-section5-4.html#section5-4"><span class="toc-section-number">5.4</span> Models for R by C tables</a></li>
<li><a href="5-5-section5-5.html#section5-5"><span class="toc-section-number">5.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-6-section5-6.html#section5-6"><span class="toc-section-number">5.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-7-section5-7.html#section5-7"><span class="toc-section-number">5.7</span> Examining residuals for the source of differences</a></li>
<li><a href="5-8-section5-8.html#section5-8"><span class="toc-section-number">5.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li><a href="5-9-section5-9.html#section5-9"><span class="toc-section-number">5.9</span> Political party and voting results: Complete analysis</a></li>
<li><a href="5-10-section5-10.html#section5-10"><span class="toc-section-number">5.10</span> Is cheating and lying related in students?</a></li>
<li><a href="5-11-section5-11.html#section5-11"><span class="toc-section-number">5.11</span> Analyzing a stratified random sample of California schools</a></li>
<li><a href="5-12-section5-12.html#section5-12"><span class="toc-section-number">5.12</span> Chapter summary</a></li>
<li><a href="5-13-section5-13.html#section5-13"><span class="toc-section-number">5.13</span> Summary of important R commands</a></li>
<li><a href="5-14-section5-14.html#section5-14"><span class="toc-section-number">5.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="6-chapter6.html#chapter6"><span class="toc-section-number">6</span> Correlation and Simple Linear Regression</a><ul>
<li><a href="6-1-section6-1.html#section6-1"><span class="toc-section-number">6.1</span> Relationships between two quantitative variables</a></li>
<li><a href="6-2-section6-2.html#section6-2"><span class="toc-section-number">6.2</span> Estimating the correlation coefficient</a></li>
<li><a href="6-3-section6-3.html#section6-3"><span class="toc-section-number">6.3</span> Relationships between variables by groups</a></li>
<li><a href="6-4-section6-4.html#section6-4"><span class="toc-section-number">6.4</span> Inference for the correlation coefficient</a></li>
<li><a href="6-5-section6-5.html#section6-5"><span class="toc-section-number">6.5</span> Are tree diameters related to tree heights?</a></li>
<li><a href="6-6-section6-6.html#section6-6"><span class="toc-section-number">6.6</span> Describing relationships with a regression model</a></li>
<li><a href="6-7-section6-7.html#section6-7"><span class="toc-section-number">6.7</span> Least Squares Estimation</a></li>
<li><a href="6-8-section6-8.html#section6-8"><span class="toc-section-number">6.8</span> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li><a href="6-9-section6-9.html#section6-9"><span class="toc-section-number">6.9</span> Outliers: leverage and influence</a></li>
<li><a href="6-10-section6-10.html#section6-10"><span class="toc-section-number">6.10</span> Residual diagnostics – setting the stage for inference</a></li>
<li><a href="6-11-section6-11.html#section6-11"><span class="toc-section-number">6.11</span> Old Faithful discharge and waiting times</a></li>
<li><a href="6-12-section6-12.html#section6-12"><span class="toc-section-number">6.12</span> Chapter summary</a></li>
<li><a href="6-13-section6-13.html#section6-13"><span class="toc-section-number">6.13</span> Summary of important R code</a></li>
<li><a href="6-14-section6-14.html#section6-14"><span class="toc-section-number">6.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="7-chapter7.html#chapter7"><span class="toc-section-number">7</span> Simple linear regression inference</a><ul>
<li><a href="7-1-section7-1.html#section7-1"><span class="toc-section-number">7.1</span> Model</a></li>
<li><a href="7-2-section7-2.html#section7-2"><span class="toc-section-number">7.2</span> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li><a href="7-3-section7-3.html#section7-3"><span class="toc-section-number">7.3</span> Bozeman temperature trend</a></li>
<li><a href="7-4-section7-4.html#section7-4"><span class="toc-section-number">7.4</span> Randomization-based inferences for the slope coefficient</a></li>
<li><a href="7-5-section7-5.html#section7-5"><span class="toc-section-number">7.5</span> Transformations part I: Linearizing relationships</a></li>
<li><a href="7-6-section7-6.html#section7-6"><span class="toc-section-number">7.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li><a href="7-7-section7-7.html#section7-7"><span class="toc-section-number">7.7</span> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li><a href="7-8-section7-8.html#section7-8"><span class="toc-section-number">7.8</span> Chapter summary</a></li>
<li><a href="7-9-section7-9.html#section7-9"><span class="toc-section-number">7.9</span> Summary of important R code</a></li>
<li><a href="7-10-section7-10.html#section7-10"><span class="toc-section-number">7.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="8-chapter8.html#chapter8"><span class="toc-section-number">8</span> Multiple linear regression</a><ul>
<li><a href="8-1-section8-1.html#section8-1"><span class="toc-section-number">8.1</span> Going from SLR to MLR</a></li>
<li><a href="8-2-section8-2.html#section8-2"><span class="toc-section-number">8.2</span> Validity conditions in MLR</a></li>
<li><a href="8-3-section8-3.html#section8-3"><span class="toc-section-number">8.3</span> Interpretation of MLR terms</a></li>
<li><a href="8-4-section8-4.html#section8-4"><span class="toc-section-number">8.4</span> Comparing multiple regression models</a></li>
<li><a href="8-5-section8-5.html#section8-5"><span class="toc-section-number">8.5</span> General recommendations for MLR interpretations and VIFs</a></li>
<li><a href="8-6-section8-6.html#section8-6"><span class="toc-section-number">8.6</span> MLR inference: Parameter inferences using the t-distribution</a></li>
<li><a href="8-7-section8-7.html#section8-7"><span class="toc-section-number">8.7</span> Overall F-test in multiple linear regression</a></li>
<li><a href="8-8-section8-8.html#section8-8"><span class="toc-section-number">8.8</span> Case study: First year college GPA and SATs</a></li>
<li><a href="8-9-section8-9.html#section8-9"><span class="toc-section-number">8.9</span> Different intercepts for different groups: MLR with indicator variables</a></li>
<li><a href="8-10-section8-10.html#section8-10"><span class="toc-section-number">8.10</span> Additive MLR with more than two groups: Headache example</a></li>
<li><a href="8-11-section8-11.html#section8-11"><span class="toc-section-number">8.11</span> Different slopes and different intercepts</a></li>
<li><a href="8-12-section8-12.html#section8-12"><span class="toc-section-number">8.12</span> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li><a href="8-13-section8-13.html#section8-13"><span class="toc-section-number">8.13</span> AICs for model selection</a></li>
<li><a href="8-14-section8-14.html#section8-14"><span class="toc-section-number">8.14</span> Case study: Forced expiratory volume model selection using AICs</a></li>
<li><a href="8-15-section8-15.html#section8-15"><span class="toc-section-number">8.15</span> Chapter summary</a></li>
<li><a href="8-16-section8-16.html#section8-16"><span class="toc-section-number">8.16</span> Summary of important R code</a></li>
<li><a href="8-17-section8-17.html#section8-17"><span class="toc-section-number">8.17</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="9-chapter9.html#chapter9"><span class="toc-section-number">9</span> Case studies</a><ul>
<li><a href="9-1-section9-1.html#section9-1"><span class="toc-section-number">9.1</span> Overview of material covered</a></li>
<li><a href="9-2-section9-2.html#section9-2"><span class="toc-section-number">9.2</span> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li><a href="9-3-section9-3.html#section9-3"><span class="toc-section-number">9.3</span> Ants learn to rely on more informative attributes during decision-making</a></li>
<li><a href="9-4-section9-4.html#section9-4"><span class="toc-section-number">9.4</span> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li><a href="9-5-section9-5.html#section9-5"><span class="toc-section-number">9.5</span> What do didgeridoos really do about sleepiness?</a></li>
<li><a href="9-6-section9-6.html#section9-6"><span class="toc-section-number">9.6</span> General summary</a></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="section7-2" class="section level2">
<h2><span class="header-section-number">7.2</span> Confidence interval and hypothesis tests for the slope and intercept</h2>

<p>Our inference techniques will resemble previous material with an
interest in forming confidence intervals and
doing hypothesis testing, although the interpretation of confidence intervals
for slope coefficients take some extra care. Remember that the general form of
any parametric   confidence interval is</p>
<p><span class="math display">\[\text{estimate} \mp t^*\text{SE}_{estimate},\]</span></p>
<p>so we need to obtain the appropriate standard error for regression model
coefficients and the degrees of freedom to define the <span class="math inline">\(t\)</span>-distribution
to look up <span class="math inline">\(t^*\)</span> multiplier.

We will find the <span class="math inline">\(\text{SE}_{b_0}\)</span> and <span class="math inline">\(\text{SE}_{b_1}\)</span>
in the model summary. The degrees of freedom for the <span class="math inline">\(t\)</span>-distribution
in simple linear regression are <span class="math inline">\(\mathbf{df=n-2}\)</span>.

Putting this
together, the confidence interval for the true y-intercept, <span class="math inline">\(\beta_0\)</span>, is
<span class="math inline">\(\mathbf{b_0 \mp t^*_{n-2}}\textbf{SE}_{\mathbf{b_0}}\)</span> although this
confidence interval is rarely of interest. The confidence interval
that is almost always of interest is for the true slope coefficient,
<span class="math inline">\(\beta_1\)</span>, that is <span class="math inline">\(\mathbf{b_1 \mp t^*_{n-2}}\textbf{SE}_{\mathbf{b_1}}\)</span>.
The slope confidence interval is used to do two
things: (1) inference for the amount of change in the mean of <span class="math inline">\(y\)</span> for a unit
change in <span class="math inline">\(x\)</span> in the population and (2) to potentially do hypothesis testing by
checking whether 0 is in the CI or not. The sketch in Figure <a href="7-2-section7-2.html#fig:Figure7-4">2.128</a>
illustrates the roles of the
CI for the slope in terms of determining where the population slope, <span class="math inline">\(\beta_1\)</span>, coefficient
might be – centered at the sample slope coefficient – our best guess for the
true slope. This sketch also informs an <strong><em>interpretation of the slope coefficient
confidence interval</em></strong>:</p>
<p>(ref:fig7-4) Graphic illustrating the confidence interval for a slope
coefficient for a 1 unit increase in <span class="math inline">\(x\)</span>.</p>
<div class="figure"><span id="fig:Figure7-4"></span>
<img src="chapter7_files/image045.png" alt="(ref:fig7-4)" width="210" />
<p class="caption">
Figure 2.128: (ref:fig7-4)
</p>
</div>
<blockquote>
<p>For a 1 <strong>[<em>units of X</em>]</strong> increase in <strong>X</strong>, we are ___ % confident
that the <strong>true change in the mean of</strong> <strong><em>Y</em></strong> will be between <strong>LL</strong>
and <strong>UL</strong> <strong>[<em>units of Y</em>]</strong>.</p>
</blockquote>
<p></p>
<p>In this interpretation, LL and UL are the calculated lower and upper
limits of the confidence interval. This builds
on our previous interpretation of the slope coefficient, adding in the
information about pinning down the true change (population change) in the mean
of the response variable for a difference of 1 unit in the <span class="math inline">\(x\)</span>-direction. The interpretation of the y-intercept CI is:
</p>
<blockquote>
<p>For an <strong><em>x</em></strong> of 0 <strong>[<em>units of X</em>]</strong>, we are 95% confident that
the true mean of <strong><em>Y</em></strong> will be between <strong>LL</strong> and <strong>UL</strong>
<strong>[<em>units of Y</em>]</strong>.</p>
</blockquote>
<p>This is really only interesting if the value of <span class="math inline">\(x=0\)</span> is interesting –
we’ll see a method for generating CIs
for the true mean at potentially more interesting values of <span class="math inline">\(x\)</span> in
Section <a href="7-7-section7-7.html#section7-7">7.7</a>. To trust the results from these
confidence intervals, it is critical that any issues with the regression validity conditions are minor. </p>
<p>The only hypothesis test of interest in this situation is for the slope
coefficient.

To develop the hypotheses of interest in SLR, note the effect
of having <span class="math inline">\(\beta_1=0\)</span> in the mean of the regression equation,
<span class="math inline">\(\mu_{y_i} = \beta_0 + \beta_1x_i = \beta_0 + 0x_i = \beta_0\)</span>.
This is the “intercept-only” or “mean-only” model that suggests that
the mean of <span class="math inline">\(y\)</span> does not vary with different values of <span class="math inline">\(x\)</span> as it is always
<span class="math inline">\(\beta_0\)</span>. We saw this model in the ANOVA material as the reduced model when the
null hypothesis of no difference in the true means across the groups was true.
Here, this is the same as saying that there is no linear relationship between <span class="math inline">\(x\)</span>
and <span class="math inline">\(y\)</span>, or that <span class="math inline">\(x\)</span> is of no use in predicting <span class="math inline">\(y\)</span>, or that we make the same
prediction for <span class="math inline">\(y\)</span> for every value of <span class="math inline">\(x\)</span>. Thus</p>
<p><span class="math display">\[\boldsymbol{H_0: \beta_1=0}\]</span></p>
<p>is a test for <strong>no linear relationship between</strong> <span class="math inline">\(\mathbf{x}\)</span> <strong>and</strong> <span class="math inline">\(\mathbf{y}\)</span>
<strong>in the population</strong>. The alternative of <span class="math inline">\(\boldsymbol{H_A: \beta_1\ne 0}\)</span>,
that there is <strong>some</strong> linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> in the population,
is our main test of interest in these situations. It is also possible to test
greater than or less than alternatives in certain situations.</p>
<p>Test statistics for regression coefficients are developed, if we can trust our assumptions, using the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom.

The <span class="math inline">\(t\)</span>-test statistic is generally</p>
<p><span class="math display">\[t=\frac{b_i}{\text{SE}_{b_i}}\]</span></p>
<p>with the main interest in the test for <span class="math inline">\(\beta_1\)</span> based on <span class="math inline">\(b_1\)</span> initially.
The p-value would be calculated using the two-tailed area from the
<span class="math inline">\(t_{n-2}\)</span> distribution calculated using the <code>pt</code> function.

The p-value
to test these hypotheses is also provided
in the model summary as we will see below. </p>
<p>The greater than or
less than alternatives can have interesting interpretations in certain
situations. For example, the greater than alternative
<span class="math inline">\(\left(\boldsymbol{H_A: \beta_1 &gt; 0}\right)\)</span> tests an alternative of a
positive linear relationship, with the p-value extracted just from the
right tail of the same <span class="math inline">\(t\)</span>-distribution. This could be
used when a researcher would only find a result “interesting” if a positive
relationship is detected, such as in the study of tree height and tree diameter
where a researcher might be justified in deciding to test only for a positive
linear relationship. Similarly, the left-tailed alternative is also possible,
<span class="math inline">\(\boldsymbol{H_A: \beta_1 &lt; 0}\)</span>. To get one-tailed p-values from two-tailed
results (the default), first check that the observed test statistic is
in the direction of the alternative (<span class="math inline">\(t&gt;0\)</span> for <span class="math inline">\(H_A:\beta_1&gt;0\)</span> or <span class="math inline">\(t&lt;0\)</span>
for <span class="math inline">\(H_A:\beta_1&lt;0\)</span>).

<strong>If these conditions are met, then the p-value for
the one-sided test from the two-sided version is found by dividing the
reported p-value by 2</strong>. If <span class="math inline">\(t&gt;0\)</span> for <span class="math inline">\(H_A:\beta_1&gt;0\)</span> or <span class="math inline">\(t&lt;0\)</span>
for <span class="math inline">\(H_A:\beta_1&lt;0\)</span> are not met, then the p-value would be greater than
0.5 and it would be easiest to look it up directly using <code>pt</code> using the tail area direction in the direction of the alternative.
</p>
<p>We can revisit a couple of examples for a last time with these ideas in
hand to complete the analyses.</p>
<blockquote>
<p>For the <em>Beers, BAC</em> data, the 95% confidence for the true slope
coefficient, <span class="math inline">\(\beta_1\)</span>, is</p>
</blockquote>
<p><span class="math display">\[\begin{array}{rl}
\boldsymbol{b_1 \mp t^*_{n-2}} \textbf{SE}_{\boldsymbol{b_1}}
&amp; \boldsymbol{= 0.01796 \mp 2.144787 * 0.002402} \\
&amp; \boldsymbol{= 0.01796 \mp 0.00515} \\
&amp; \boldsymbol{\rightarrow (0.0128, 0.0231).}
\end{array}\]</span></p>
<p>You can find the components of this
calculation in the model summary and from <code>qt(0.975, df=n-2)</code> which was
2.145 for the <span class="math inline">\(t^*\)</span>-multiplier. Be careful not to use the <span class="math inline">\(t\)</span>-value of
7.48 in the model summary to
make confidence intervals – that is the test statistic used below. The related
calculations are shown at the bottom of the following code:</p>
<div class="sourceCode" id="cb575"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb575-1" title="1">m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(BAC<span class="op">~</span>Beers, <span class="dt">data=</span>BB)</a></code></pre></div>
<div class="sourceCode" id="cb576"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb576-1" title="1"><span class="kw">summary</span>(m1)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = BAC ~ Beers, data = BB)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.027118 -0.017350  0.001773  0.008623  0.041027 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -0.012701   0.012638  -1.005    0.332
## Beers        0.017964   0.002402   7.480 2.97e-06
## 
## Residual standard error: 0.02044 on 14 degrees of freedom
## Multiple R-squared:  0.7998, Adjusted R-squared:  0.7855 
## F-statistic: 55.94 on 1 and 14 DF,  p-value: 2.969e-06</code></pre>

<div class="sourceCode" id="cb578"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb578-1" title="1"><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">14</span>) <span class="co"># t* multiplier for 95% CI</span></a></code></pre></div>
<pre><code>## [1] 2.144787</code></pre>
<div class="sourceCode" id="cb580"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb580-1" title="1"><span class="fl">0.017964</span> <span class="op">+</span><span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">14</span>)<span class="op">*</span><span class="fl">0.002402</span></a></code></pre></div>
<pre><code>## [1] 0.01281222 0.02311578</code></pre>
<div class="sourceCode" id="cb582"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb582-1" title="1"><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">14</span>)<span class="op">*</span><span class="fl">0.002402</span></a></code></pre></div>
<pre><code>## [1] 0.005151778</code></pre>

<p>We can also get the confidence interval
directly from the <code>confint</code> function run on our regression model,
saving some calculation
effort and providing both the CI for the y-intercept and the slope coefficient. </p>
<div class="sourceCode" id="cb584"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb584-1" title="1"><span class="kw">confint</span>(m1)</a></code></pre></div>
<pre><code>##                   2.5 %     97.5 %
## (Intercept) -0.03980535 0.01440414
## Beers        0.01281262 0.02311490</code></pre>
<p>We interpret the 95% CI for the slope coefficient as follows: For a
1 <strong>beer</strong> increase in number of beers consumed, we are 95% confident that
the <strong>true</strong> change in the <strong>mean</strong> <em>BAC</em> will be between 0.0128 and 0.0231
g/dL. While the estimated slope is our best guess of the impacts of an extra
beer consumed based on our sample, this CI provides information about the likely
range of potential impacts on the mean in the population. It also could be used
to test the two-sided hypothesis test and would suggest strong evidence against
the null hypothesis since the confidence interval does not contain 0, but its main use is to quantify where we think the true slope coefficient resides.</p>
<p>The width of the CI, interpreted loosely as the precision of the estimated slope, is
impacted by the variability of
the observations around the estimated regression line, the overall sample size,
and the positioning of the <span class="math inline">\(x\)</span>-observations. Basically all those aspects relate
to how “clearly” known the regression line is and that determines the estimated
precision in the slope. For example, the more variability around the line that
is present, the more uncertainty there is about the correct line to use
(Least Squares (LS) can
still find an estimated line but there are other lines that might be “close” to
its optimizing choice). Similarly, more observations help us get a better
estimate of the mean – an idea that permeates all statistical methods. Finally,
the location of <span class="math inline">\(x\)</span>-values can impact the precision in a slope coefficient. We’ll
revisit this in the context of <strong><em>multi-collinearity</em></strong> in the next chapter,
and often we have no control of <span class="math inline">\(x\)</span>-values, but just note that different
patterns of <span class="math inline">\(x\)</span>-values can lead to different precision of estimated slope
coefficients<a href="#fn101" class="footnote-ref" id="fnref101"><sup>101</sup></a>.</p>
<p>For hypothesis testing, we will almost always stick with two-sided tests
in regression modeling as it is a more conservative approach and does not require
us to have an expectation of a direction for relationships <em>a priori</em>. In this
example, the null hypothesis for the slope coefficient is that there is no
linear relationship between <em>Beers</em> and <em>BAC</em> in the population. The
alternative hypothesis is that there is
some linear relationship between <em>Beers</em> and <em>BAC</em> in the population. The
test statistic is <span class="math inline">\(t=0.01796/0.002402 =7.48\)</span> which, if model assumptions hold,
follows a <span class="math inline">\(t(14)\)</span> distribution under the null hypothesis. The model summary provides the calculation
of the test statistic
and the two-sided test p-value of <span class="math inline">\(2.97\text{e-6} = 0.00000297\)</span>. So we
would just report
“p-value &lt; 0.0001”. This suggests that there is very strong evidence against the null hypothesis of no linear
relationship between <em>Beers</em> and <em>BAC</em> in the population, so we would conclude that there is a linear relationship between them. Because of the
random assignment, we can also say that drinking beers causes changes in BAC
but, because the sample was made up of volunteers, we cannot infer that these results
would hold in the general population of OSU students or more generally.</p>
<p>There are also results for the y-intercept in the output. The 95% CI is
from -0.0398 to 0.0144, that the true mean <em>BAC</em> for a 0 beer consuming
subject is between
-0.0398 to 0.01445. This is really not a big surprise but possibly is
comforting to know that these results would not show much evidence against the null hypothesis
that the true mean <em>BAC</em> for 0 <em>Beers</em> is 0. Finding little evidence of a
difference from 0 makes sense and makes the estimated y-intercept of -0.013 not
so problematic. In other situations, the results for the y-intercept may be
more illogical but this will often be because the y-intercept is extrapolating
far beyond the scope of observations. The y-intercept’s main function in
regression models is to be at the right level for the slope to “work” to make a
line that describes the responses and thus is usually of lesser interest even though it plays an important role in the model.</p>
<p>As a second example, we can revisit modeling the <em>Hematocrit</em> of female
Australian athletes as a function of <em>body fat %</em>. The sample size is <span class="math inline">\(n=99\)</span>
so the <em>df</em> are 97 in any <span class="math inline">\(t\)</span>-distributions. In Chapter <a href="6-chapter6.html#chapter6"><strong>??</strong></a>, the
relationship between <em>Hematocrit</em> and <em>body fat %</em> for females appeared to
be a weak negative linear
association. The 95% confidence interval for the slope is -0.186 to 0.0155. For
a 1% increase in body fat %, we are 95% confident that the change in the true
mean Hematocrit is between -0.186 and 0.0155% of blood. This suggests that we
would find little evidence against the null hypothesis of no linear relationship because this CI contains 0. In fact the p-value is 0.0965 which is larger than 0.05
and so provides a consistent conclusion with using the 95% confidence interval
to perform a hypothesis test. Either way, we would conclude that there is not strong evidence against the null hypothesis but there is some evidence against it with a p-value of that size since more extreme results are somewhat common but still fairly rare if we assume the null is true. If you think p-values around 0.10 provide moderate evidence, you might have a different opinion about
the evidence against the null hypothesis here. For this reason, we sometimes
interpret this sort of marginal result as having some or marginal evidence against the null
but certainly would never say that this presents strong evidence.

</p>
<div class="sourceCode" id="cb586"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb586-1" title="1"><span class="kw">library</span>(alr3)</a>
<a class="sourceLine" id="cb586-2" title="2"><span class="kw">data</span>(ais)</a>
<a class="sourceLine" id="cb586-3" title="3"><span class="kw">library</span>(tibble)</a>
<a class="sourceLine" id="cb586-4" title="4">ais &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(ais)</a>
<a class="sourceLine" id="cb586-5" title="5">aisR2 &lt;-<span class="st"> </span>ais[<span class="op">-</span><span class="kw">c</span>(<span class="dv">56</span>,<span class="dv">166</span>), <span class="kw">c</span>(<span class="st">&quot;Ht&quot;</span>,<span class="st">&quot;Hc&quot;</span>,<span class="st">&quot;Bfat&quot;</span>,<span class="st">&quot;Sex&quot;</span>)]</a>
<a class="sourceLine" id="cb586-6" title="6">m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Hc<span class="op">~</span>Bfat, <span class="dt">data=</span><span class="kw">subset</span>(aisR2,Sex<span class="op">==</span><span class="dv">1</span>)) <span class="co"># Results for Females </span></a>
<a class="sourceLine" id="cb586-7" title="7"><span class="kw">summary</span>(m2)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Hc ~ Bfat, data = subset(aisR2, Sex == 1))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.2399 -2.2132 -0.1061  1.8917  6.6453 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 42.01378    0.93269  45.046   &lt;2e-16
## Bfat        -0.08504    0.05067  -1.678   0.0965
## 
## Residual standard error: 2.598 on 97 degrees of freedom
## Multiple R-squared:  0.02822,    Adjusted R-squared:  0.0182 
## F-statistic: 2.816 on 1 and 97 DF,  p-value: 0.09653</code></pre>
<div class="sourceCode" id="cb588"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb588-1" title="1"><span class="kw">confint</span>(m2)</a></code></pre></div>
<pre><code>##                  2.5 %      97.5 %
## (Intercept) 40.1626516 43.86490713
## Bfat        -0.1856071  0.01553165</code></pre>
<p>One more worked example is provided from the Montana fire data. In this
example pay particular attention
to how we are handling the units of the response variable, log-hectares, and to
the changes to doing inferences with a 99% confidence
level CI, and where you can find the needed results in the following output:</p>
<div class="sourceCode" id="cb590"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb590-1" title="1">mtfires &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/climateR2.csv&quot;</span>)</a>
<a class="sourceLine" id="cb590-2" title="2">mtfires<span class="op">$</span>loghectares &lt;-<span class="st"> </span><span class="kw">log</span>(mtfires<span class="op">$</span>hectares)</a>
<a class="sourceLine" id="cb590-3" title="3">fire1 &lt;-<span class="st"> </span><span class="kw">lm</span>(loghectares<span class="op">~</span>Temperature, <span class="dt">data=</span>mtfires)</a>
<a class="sourceLine" id="cb590-4" title="4"><span class="kw">summary</span>(fire1)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = loghectares ~ Temperature, data = mtfires)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0822 -0.9549  0.1210  1.0007  2.4728 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -69.7845    12.3132  -5.667 1.26e-05
## Temperature   1.3884     0.2165   6.412 2.35e-06
## 
## Residual standard error: 1.476 on 21 degrees of freedom
## Multiple R-squared:  0.6619, Adjusted R-squared:  0.6458 
## F-statistic: 41.12 on 1 and 21 DF,  p-value: 2.347e-06</code></pre>
<div class="sourceCode" id="cb592"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb592-1" title="1"><span class="kw">confint</span>(fire1, <span class="dt">level=</span><span class="fl">0.99</span>)</a></code></pre></div>
<pre><code>##                    0.5 %     99.5 %
## (Intercept) -104.6477287 -34.921286
## Temperature    0.7753784   2.001499</code></pre>
<div class="sourceCode" id="cb594"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb594-1" title="1"><span class="kw">qt</span>(<span class="fl">0.995</span>, <span class="dt">df=</span><span class="dv">21</span>)</a></code></pre></div>
<pre><code>## [1] 2.83136</code></pre>
<ul>
<li><p>Based on the estimated regression model, we can say that if the
average temperature is 0, we expect that, on average, the log-area
burned would be -69.8 log-hectares.</p></li>
<li><p>From the regression model summary, <span class="math inline">\(b_1=1.39\)</span> with
<span class="math inline">\(\text{SE}_{b_1}=0.2165\)</span> and <span class="math inline">\(\mathbf{t=6.41}\)</span>.</p></li>
<li><p>There were <span class="math inline">\(n=23\)</span> measurements taken, so <span class="math inline">\(\mathbf{df=n-2=23-3=21}\)</span>.</p></li>
<li><p>Suppose that we want to test for a linear relationship between
temperature and log-hectares burned:</p>
<p><span class="math display">\[H_0: \beta_1=0\]</span></p>
<ul>
<li>In words, the true slope coefficient between <em>Temperature</em> and
<em>log-area burned</em> is 0 OR there is no linear relationship between
<em>Temperature</em> and <em>log-area burned</em> in the population.</li>
</ul>
<p><span class="math display">\[H_A: \beta_1\ne 0\]</span></p>
<ul>
<li>In words, the alternative states that the true slope coefficient
between <em>Temperature</em> and <em>log-area burned</em> is not 0 OR there is a
linear relationship between <em>Temperature</em> and <em>log-area burned</em> in
the population.</li>
</ul></li>
</ul>
<p>Test statistic: <span class="math inline">\(t = 1.39/0.217 = 6.41\)</span></p>
<ul>
<li>Assuming the null hypothesis to be true (no linear relationship), the
<span class="math inline">\(t\)</span>-statistic follows a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2 = 23-2=21\)</span> degrees of
freedom (or simply <span class="math inline">\(t_{21}\)</span>).</li>
</ul>
<p>p-value:</p>
<ul>
<li><p>From the model summary, the <strong>p-value is</strong> <span class="math inline">\(\mathbf{2.35*10^{-6}}\)</span></p>
<ul>
<li>Interpretation: There is less than a 0.01% chance that we would
observe slope coefficient like we did or something more extreme
(greater than 1.39 log(hectares)/<span class="math inline">\(^\circ F\)</span>) if there were in fact
no linear relationship between temperature (<span class="math inline">\(^\circ F\)</span>) and log-area
burned (log-hectares) in the population.</li>
</ul></li>
</ul>
<p>Conclusion: There is very strong evidence against the null hypothesis of no
linear relationship, so we would conclude that there
is, in fact, a linear relationship between Temperature and log(Hectares)
burned.</p>
<p>Scope of Inference: Since we have a time series of results, our inferences pertain to the
results we could have observed for these years but not for years we did not
observe – so just for the true slope for this sample of years. Because we can’t
randomly assign the amount of area burned, we cannot make causal inferences –
there are many reasons why both the average temperature and area burned would
vary together that would not involve a direct connection between them.</p>
<p><span class="math display">\[\text{99}\% \text{ CI for } \beta_1: \boldsymbol{b_1 \mp
t^*_{n-2}}\textbf{SE}_{\boldsymbol{b_1}} \rightarrow 1.39 \mp 2.831\bullet 0.217
\rightarrow (0.78, 2.00)\]</span></p>
<p>Interpretation of 99% CI for slope coefficient:</p>
<ul>
<li>For a 1 degree F increase in <em>Temperature</em>, we are 99% confident that the
change in the true mean log-area burned is between 0.78 and 2.00 log(Hectares).</li>
</ul>
<p>Another way to interpret this is:</p>
<ul>
<li>For a 1 degree F increase in <em>Temperature</em>, we are 99% confident that
the mean Area Burned will change by between 0.78 and 2.00 log(Hectares)
<strong>in the population</strong>.</li>
</ul>
<p>Also <span class="math inline">\(R^2\)</span> is 66.2%, which tells us that <em>Temperature</em> explains 66.2%
of the variation in <em>log(Hectares) burned</em>. Or that the linear regression
model built using <em>Temperature</em> explains 66.2% of the variation in yearly
<em>log(Hectares) burned</em> so this model explains quite a bit but not all the variation in the responses.</p>
</div>
<div class="footnotes">
<hr />
<ol start="101">
<li id="fn101"><p>There is an area of statistical research on how to
optimally choose <span class="math inline">\(x\)</span>-values to
get the most precise estimate of a slope coefficient. In observational studies
we have to deal with whatever pattern of <span class="math inline">\(x\text{&#39;s}\)</span> we ended up with. If you can
choose, generate an even spread of <span class="math inline">\(x\text{&#39;s}\)</span> over some range of
interest similar to what was used in the <em>Beers</em> vs <em>BAC</em> study to provide
the best distribution of values to discover the relationship across the
selected range of <span class="math inline">\(x\)</span>-values.<a href="7-2-section7-2.html#fnref101" class="footnote-back">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="7-1-section7-1.html"><button class="btn btn-default">Previous</button></a>
<a href="7-3-section7-3.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
