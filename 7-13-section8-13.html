<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="7.13 AICs for model selection | Intermediate Statistics with R" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="gpeterson406/Greenwood_Book" />

<meta name="author" content="Mark C Greenwood" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="7.13 AICs for model selection | Intermediate Statistics with R">

<title>7.13 AICs for model selection | Intermediate Statistics with R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li class="has-sub"><a href="acknowledgments.html#acknowledgments">Acknowledgments</a><ul>
<li><a href="0-1-section1-5.html#section1-5"><span class="toc-section-number">0.1</span> Summary of important R code</a></li>
</ul></li>
<li class="has-sub"><a href="1-chapter2.html#chapter2"><span class="toc-section-number">1</span> (R)e-Introduction to statistics</a><ul>
<li><a href="1-1-section2-1.html#section2-1"><span class="toc-section-number">1.1</span> Histograms, boxplots, and density curves</a></li>
<li><a href="1-2-section2-2.html#section2-2"><span class="toc-section-number">1.2</span> Pirate-plots</a></li>
<li><a href="1-3-section2-3.html#section2-3"><span class="toc-section-number">1.3</span> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li><a href="1-4-section2-4.html#section2-4"><span class="toc-section-number">1.4</span> Permutation testing for the two sample mean situation</a></li>
<li><a href="1-5-section2-5.html#section2-5"><span class="toc-section-number">1.5</span> Hypothesis testing (general)</a></li>
<li><a href="1-6-section2-6.html#section2-6"><span class="toc-section-number">1.6</span> Connecting randomization (nonparametric) and parametric tests</a></li>
<li><a href="1-7-section2-7.html#section2-7"><span class="toc-section-number">1.7</span> Second example of permutation tests</a></li>
<li><a href="1-8-section2-8.html#section2-8"><span class="toc-section-number">1.8</span> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li><a href="1-9-section2-9.html#section2-9"><span class="toc-section-number">1.9</span> Confidence intervals and bootstrapping</a></li>
<li><a href="1-10-section2-10.html#section2-10"><span class="toc-section-number">1.10</span> Bootstrap confidence intervals for difference in GPAs</a></li>
<li><a href="1-11-section2-11.html#section2-11"><span class="toc-section-number">1.11</span> Chapter summary</a></li>
<li><a href="1-12-section2-12.html#section2-12"><span class="toc-section-number">1.12</span> Summary of important R code</a></li>
<li><a href="1-13-section2-13.html#section2-13"><span class="toc-section-number">1.13</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="2-chapter3.html#chapter3"><span class="toc-section-number">2</span> One-Way ANOVA</a><ul>
<li><a href="2-1-section3-1.html#section3-1"><span class="toc-section-number">2.1</span> Situation</a></li>
<li><a href="2-2-section3-2.html#section3-2"><span class="toc-section-number">2.2</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li><a href="2-3-section3-3.html#section3-3"><span class="toc-section-number">2.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li><a href="2-4-section3-4.html#section3-4"><span class="toc-section-number">2.4</span> ANOVA model diagnostics including QQ-plots</a></li>
<li><a href="2-5-section3-5.html#section3-5"><span class="toc-section-number">2.5</span> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li><a href="2-6-section3-6.html#section3-6"><span class="toc-section-number">2.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li><a href="2-7-section3-7.html#section3-7"><span class="toc-section-number">2.7</span> Pair-wise comparisons for the Overtake data</a></li>
<li><a href="2-8-section3-8.html#section3-8"><span class="toc-section-number">2.8</span> Chapter summary</a></li>
<li><a href="2-9-section3-9.html#section3-9"><span class="toc-section-number">2.9</span> Summary of important R code</a></li>
<li><a href="2-10-section3-10.html#section3-10"><span class="toc-section-number">2.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="3-chapter4.html#chapter4"><span class="toc-section-number">3</span> Two-Way ANOVA</a><ul>
<li><a href="3-1-section4-1.html#section4-1"><span class="toc-section-number">3.1</span> Situation</a></li>
<li><a href="3-2-section4-2.html#section4-2"><span class="toc-section-number">3.2</span> Designing a two-way experiment and visualizing results</a></li>
<li><a href="3-3-section4-3.html#section4-3"><span class="toc-section-number">3.3</span> Two-Way ANOVA models and hypothesis tests</a></li>
<li><a href="3-4-section4-4.html#section4-4"><span class="toc-section-number">3.4</span> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li><a href="3-5-section4-5.html#section4-5"><span class="toc-section-number">3.5</span> Observational study example: The Psychology of Debt</a></li>
<li><a href="3-6-section4-6.html#section4-6"><span class="toc-section-number">3.6</span> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li><a href="3-7-section4-7.html#section4-7"><span class="toc-section-number">3.7</span> Chapter summary</a></li>
<li><a href="3-8-section4-8.html#section4-8"><span class="toc-section-number">3.8</span> Summary of important R code</a></li>
<li><a href="3-9-section4-9.html#section4-9"><span class="toc-section-number">3.9</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="4-chapter5.html#chapter5"><span class="toc-section-number">4</span> Chi-square tests</a><ul>
<li><a href="4-1-section5-1.html#section5-1"><span class="toc-section-number">4.1</span> Situation, contingency tables, and tableplots</a></li>
<li><a href="4-2-section5-2.html#section5-2"><span class="toc-section-number">4.2</span> Homogeneity test hypotheses</a></li>
<li><a href="4-3-section5-3.html#section5-3"><span class="toc-section-number">4.3</span> Independence test hypotheses</a></li>
<li><a href="4-4-section5-4.html#section5-4"><span class="toc-section-number">4.4</span> Models for R by C tables</a></li>
<li><a href="4-5-section5-5.html#section5-5"><span class="toc-section-number">4.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="4-6-section5-6.html#section5-6"><span class="toc-section-number">4.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="4-7-section5-7.html#section5-7"><span class="toc-section-number">4.7</span> Examining residuals for the source of differences</a></li>
<li><a href="4-8-section5-8.html#section5-8"><span class="toc-section-number">4.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li><a href="4-9-section5-9.html#section5-9"><span class="toc-section-number">4.9</span> Political party and voting results: Complete analysis</a></li>
<li><a href="4-10-section5-10.html#section5-10"><span class="toc-section-number">4.10</span> Is cheating and lying related in students?</a></li>
<li><a href="4-11-section5-11.html#section5-11"><span class="toc-section-number">4.11</span> Analyzing a stratified random sample of California schools</a></li>
<li><a href="4-12-section5-12.html#section5-12"><span class="toc-section-number">4.12</span> Chapter summary</a></li>
<li><a href="4-13-section5-13.html#section5-13"><span class="toc-section-number">4.13</span> Summary of important R commands</a></li>
<li><a href="4-14-section5-14.html#section5-14"><span class="toc-section-number">4.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="5-chapter6.html#chapter6"><span class="toc-section-number">5</span> Correlation and Simple Linear Regression</a><ul>
<li><a href="5-1-section6-1.html#section6-1"><span class="toc-section-number">5.1</span> Relationships between two quantitative variables</a></li>
<li><a href="5-2-section6-2.html#section6-2"><span class="toc-section-number">5.2</span> Estimating the correlation coefficient</a></li>
<li><a href="5-3-section6-3.html#section6-3"><span class="toc-section-number">5.3</span> Relationships between variables by groups</a></li>
<li><a href="5-4-section6-4.html#section6-4"><span class="toc-section-number">5.4</span> Inference for the correlation coefficient</a></li>
<li><a href="5-5-section6-5.html#section6-5"><span class="toc-section-number">5.5</span> Are tree diameters related to tree heights?</a></li>
<li><a href="5-6-section6-6.html#section6-6"><span class="toc-section-number">5.6</span> Describing relationships with a regression model</a></li>
<li><a href="5-7-section6-7.html#section6-7"><span class="toc-section-number">5.7</span> Least Squares Estimation</a></li>
<li><a href="5-8-section6-8.html#section6-8"><span class="toc-section-number">5.8</span> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li><a href="5-9-section6-9.html#section6-9"><span class="toc-section-number">5.9</span> Outliers: leverage and influence</a></li>
<li><a href="5-10-section6-10.html#section6-10"><span class="toc-section-number">5.10</span> Residual diagnostics – setting the stage for inference</a></li>
<li><a href="5-11-section6-11.html#section6-11"><span class="toc-section-number">5.11</span> Old Faithful discharge and waiting times</a></li>
<li><a href="5-12-section6-12.html#section6-12"><span class="toc-section-number">5.12</span> Chapter summary</a></li>
<li><a href="5-13-section6-13.html#section6-13"><span class="toc-section-number">5.13</span> Summary of important R code</a></li>
<li><a href="5-14-section6-14.html#section6-14"><span class="toc-section-number">5.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="6-chapter7.html#chapter7"><span class="toc-section-number">6</span> Simple linear regression inference</a><ul>
<li><a href="6-1-section7-1.html#section7-1"><span class="toc-section-number">6.1</span> Model</a></li>
<li><a href="6-2-section7-2.html#section7-2"><span class="toc-section-number">6.2</span> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li><a href="6-3-section7-3.html#section7-3"><span class="toc-section-number">6.3</span> Bozeman temperature trend</a></li>
<li><a href="6-4-section7-4.html#section7-4"><span class="toc-section-number">6.4</span> Randomization-based inferences for the slope coefficient</a></li>
<li><a href="6-5-section7-5.html#section7-5"><span class="toc-section-number">6.5</span> Transformations part I: Linearizing relationships</a></li>
<li><a href="6-6-section7-6.html#section7-6"><span class="toc-section-number">6.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li><a href="6-7-section7-7.html#section7-7"><span class="toc-section-number">6.7</span> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li><a href="6-8-section7-8.html#section7-8"><span class="toc-section-number">6.8</span> Chapter summary</a></li>
<li><a href="6-9-section7-9.html#section7-9"><span class="toc-section-number">6.9</span> Summary of important R code</a></li>
<li><a href="6-10-section7-10.html#section7-10"><span class="toc-section-number">6.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="7-chapter8.html#chapter8"><span class="toc-section-number">7</span> Multiple linear regression</a><ul>
<li><a href="7-1-section8-1.html#section8-1"><span class="toc-section-number">7.1</span> Going from SLR to MLR</a></li>
<li><a href="7-2-section8-2.html#section8-2"><span class="toc-section-number">7.2</span> Validity conditions in MLR</a></li>
<li><a href="7-3-section8-3.html#section8-3"><span class="toc-section-number">7.3</span> Interpretation of MLR terms</a></li>
<li><a href="7-4-section8-4.html#section8-4"><span class="toc-section-number">7.4</span> Comparing multiple regression models</a></li>
<li><a href="7-5-section8-5.html#section8-5"><span class="toc-section-number">7.5</span> General recommendations for MLR interpretations and VIFs</a></li>
<li><a href="7-6-section8-6.html#section8-6"><span class="toc-section-number">7.6</span> MLR inference: Parameter inferences using the t-distribution</a></li>
<li><a href="7-7-section8-7.html#section8-7"><span class="toc-section-number">7.7</span> Overall F-test in multiple linear regression</a></li>
<li><a href="7-8-section8-8.html#section8-8"><span class="toc-section-number">7.8</span> Case study: First year college GPA and SATs</a></li>
<li><a href="7-9-section8-9.html#section8-9"><span class="toc-section-number">7.9</span> Different intercepts for different groups: MLR with indicator variables</a></li>
<li><a href="7-10-section8-10.html#section8-10"><span class="toc-section-number">7.10</span> Additive MLR with more than two groups: Headache example</a></li>
<li><a href="7-11-section8-11.html#section8-11"><span class="toc-section-number">7.11</span> Different slopes and different intercepts</a></li>
<li><a href="7-12-section8-12.html#section8-12"><span class="toc-section-number">7.12</span> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li><a href="7-13-section8-13.html#section8-13"><span class="toc-section-number">7.13</span> AICs for model selection</a></li>
<li><a href="7-14-section8-14.html#section8-14"><span class="toc-section-number">7.14</span> Case study: Forced expiratory volume model selection using AICs</a></li>
<li><a href="7-15-section8-15.html#section8-15"><span class="toc-section-number">7.15</span> Chapter summary</a></li>
<li><a href="7-16-section8-16.html#section8-16"><span class="toc-section-number">7.16</span> Summary of important R code</a></li>
<li><a href="7-17-section8-17.html#section8-17"><span class="toc-section-number">7.17</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="8-chapter9.html#chapter9"><span class="toc-section-number">8</span> Case studies</a><ul>
<li><a href="8-1-section9-1.html#section9-1"><span class="toc-section-number">8.1</span> Overview of material covered</a></li>
<li><a href="8-2-section9-2.html#section9-2"><span class="toc-section-number">8.2</span> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li><a href="8-3-section9-3.html#section9-3"><span class="toc-section-number">8.3</span> Ants learn to rely on more informative attributes during decision-making</a></li>
<li><a href="8-4-section9-4.html#section9-4"><span class="toc-section-number">8.4</span> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li><a href="8-5-section9-5.html#section9-5"><span class="toc-section-number">8.5</span> What do didgeridoos really do about sleepiness?</a></li>
<li><a href="8-6-section9-6.html#section9-6"><span class="toc-section-number">8.6</span> General summary</a></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="section8-13" class="section level2">
<h2><span class="header-section-number">7.13</span> AICs for model selection</h2>
<p>There are a variety of techniques for selecting among a set of potential models
or refining an initially fit MLR
model. Hypothesis testing can be used (in the case where we have nested models either by adding or deleting a single term at a time)
or comparisons of adjusted <strong><em>R</em></strong><sup>2</sup>
across different potential models (which is valid for nested or non-nested
model comparisons).

Diagnostics should play a role in the models considered and
in selecting among models that might appear to be similar on a model comparison
criterion.
In this section, a new model selection method is introduced that has stronger
theoretical underpinnings, a slightly more interpretable scale, and, often,
better performance in picking an optimal<a href="#fn134" class="footnote-ref" id="fnref134"><sup>134</sup></a>
model than the <strong><em>adjusted</em></strong> <strong><em>R</em></strong><sup>2</sup>. The measure is called the
<strong><em>AIC</em></strong> (Akaike’s An Information Criterion<a href="#fn135" class="footnote-ref" id="fnref135"><sup>135</sup></a>,
<span class="citation">(Akaike <a href="#ref-Akaike1974" role="doc-biblioref">1974</a>)</span>. It is extremely popular, but sometimes misused, in some fields
such as Ecology and has been applied in almost every other potential
application area where statistical models can be compared. <span class="citation">Burnham and Anderson (<a href="#ref-Burnham2002" role="doc-biblioref">2002</a>)</span>
have been responsible for popularizing the use of AIC for model
selection, especially in Ecology. The <strong>AIC is an estimate of the distance
(or discrepancy or divergence) between a candidate model and the true model,
on a log-scale</strong>, based on a measure called the Kullback-Leibler divergence. The
models that are closer (have a smaller distance) to the truth are better and we
can compare how close two models are to the truth, picking the one that has a
smaller distance (smaller AIC) as better. The AIC includes a component that is
on the log-scale, so negative values are possible and you should not be
disturbed if you are comparing large magnitude negative numbers – just pick the
model with the smallest AIC score.</p>
<p>The AIC is optimized (smallest) for a model that contains the optimal
balance of
simplicity of the
model with quality of fit to the observations. Scientists are driven to
different degrees by what is called the <strong><em>principle of parsimony</em></strong>: that
<strong>simpler explanations (models) are better if everything else is equal or even
close to equal</strong>. In this case, it would
mean that if two models are similarly good on AIC, then select the simpler of
the two models since it is more likely to be correct in general than the more
complicated model. The AIC is calculated as <span class="math inline">\(AIC=-2log(Likelihood)+2m\)</span>, where
the <strong><em>Likelihood</em></strong> provides a measure of fit of the model (we let R
calculate it for us) and gets smaller for better fitting models and
<span class="math inline">\(m\)</span> = (number of estimated <span class="math inline">\(\beta\text{&#39;s}+1\)</span>). The value <span class="math inline">\(m\)</span> is called the
<em>model degrees of freedom</em> for AIC calculations and relates to how
many total parameters are estimated.

Note that it is a different measure of
<em>degrees of freedom</em> than used in ANOVA <span class="math inline">\(F\)</span>-tests. The main things to understand
about the formula for the AIC is that as <span class="math inline">\(m\)</span> increases, the AIC will go up and
that as the fit improves, the <em>likelihood</em> will increase (so -2<em>log-likelihood</em> will get smaller)<a href="#fn136" class="footnote-ref" id="fnref136"><sup>136</sup></a>.</p>
<p>There are some facets of this discussion to keep in mind when comparing
models. More complicated models always fit better (we saw this for the
<strong><em>R</em></strong><sup>2</sup> measure, as the proportion of variation explained always goes
up if more “stuff” is put into the model even if the “stuff” isn’t useful). The
AIC resembles the adjusted <strong><em>R</em></strong><sup>2</sup> in that it incorporates the count
of the number of parameters estimated. This allows the AIC to make sure that
enough extra variability is explained in the responses to justify making the
model more complicated (increasing <span class="math inline">\(m\)</span>). The optimal model on AIC has to balance
adding complexity and increasing quality of the fit. Since this measure
provides an estimate of the distance or discrepancy to the “true model”, the
model with the smallest value “wins” – it is top-ranked on the AIC. Note that
the <strong>top-ranked AIC model</strong> will often <strong>not be the best fitting</strong> model
since the best fitting model is always the most complicated model considered.
The top AIC model is the one that is estimated to be closest to the truth,
where the truth is still unknown…</p>
<p>To help with interpreting the scale of AICs, they are often reported in a table
sorted from smallest to largest values with the AIC and the “delta AIC” or,
simply, <span class="math inline">\(\Delta\text{AIC}\)</span> reported. The</p>
<p><span class="math display">\[\Delta\text{AIC}=\text{AIC}_{\text{model}} - \text{AIC}_{\text{topModel}}\]</span></p>
<p>and so provides a value of 0 for the top-ranked AIC model and a measure of how
much worse on the AIC scale the other models are. A rule of thumb is that a 2
unit difference on AICs <span class="math inline">\((\Delta\text{AIC}=2)\)</span> is decent evidence of a
difference in the models and more than 4 units <span class="math inline">\((\Delta\text{AIC}&gt;4)\)</span> is a
really big difference. This is more based on experience than a distinct reason
or theoretical result but seems to provide reasonable results in most situations.
Often researchers will consider any models within 2 AIC units of the top model
<span class="math inline">\((\Delta\text{AIC}&lt;2)\)</span> as indistinguishable on
AICs and so either select the simplest model of the choices or report all the
models with similar “support”, allowing the reader to explore the suite of
similarly supported potential models . It is important to remember that if you
search across too many models, even with the AIC to support your model
comparisons, you might find a spuriously top model. Individual results that are
found by exploring many tests or models have higher chances to be <strong><em>spurious</em></strong>
and results found in this manner are difficult to <strong><em>reproduce</em></strong> when
someone repeats a similar study<a href="#fn137" class="footnote-ref" id="fnref137"><sup>137</sup></a>.
For these reasons, there is a set of general recommendations that have been
developed for using AICs:</p>
<ul>
<li><p>Consider a suite of models (often pre-specified and based on prior research in
the area of interest) and find the models with the top (in other words,
smallest) AIC results.</p>
<ul>
<li>The suite of candidate models need to contain at least some good models.
Selecting the best of a set of BAD models only puts you at the top of
$%#%-mountain, which is not necessarily a good thing.</li>
</ul></li>
<li><p>Report a table with the models considered, sorted from smallest to largest
AICs (<span class="math inline">\(\Delta\text{AICs}\)</span> from smaller to larger) that includes a count of
number of parameters estimated<a href="#fn138" class="footnote-ref" id="fnref138"><sup>138</sup></a>, the
AICs, and <span class="math inline">\(\Delta\text{AICs}\)</span>.</p>
<ul>
<li>Remember to incorporate the mean-only model in the model selection results. This allows you to compare the top model to one that does not contain any predictors.</li>
</ul></li>
<li><p>Interpret the top model or top models if a few are close on the AIC-scale to
the top model.</p></li>
<li><p><strong>DO NOT REPORT P-VALUES OR CALL TERMS “SIGNIFICANT” when models were selected
using AICs.</strong> </p>
<ul>
<li>Hypothesis testing and AIC model selection are not compatible philosophies
and testing in models selected by AICs invalidates the tests as they have
inflated Type I error rates.


The AIC results are your “evidence” – you don’t need anything else. If you wanted to report p-values, use them to select your model.</li>
</ul></li>
<li><p>You can describe variables as “important” or “useful” and report confidence
intervals to aid in interpretation of the terms in the selected model(s)
but need to avoid performing hypothesis tests with the confidence intervals.</p></li>
<li><p>Remember that the selected model is not the “true” model – it is only the
best model <em>according to AIC</em> among the set of models <em>you provided</em>.</p></li>
<li><p>Model assumptions need to be met to use AICs. They assume that the model is
specified correctly up to possibly comparing different predictor variables. Perform diagnostic checks on your initial model and the top model. </p></li>
</ul>
<p>When working with AICs, there are two options. Fit the models of interest and
then run the <code>AIC</code> function on each model. This can be tedious, especially
when we have many possible models to consider. We can make
it easy to fit all the potential candidate models that are implied by a
complicated starting model by using the <code>dredge</code> function from the <code>MuMIn</code>
package <span class="citation">(Barton <a href="#ref-R-MuMIn" role="doc-biblioref">2019</a>)</span>.

The name (dredge) actually speaks to what <strong><em>fitting
all possible models</em></strong> really engages – what is called <strong><em>data dredging</em></strong>.
The term is meant to
refer to considering way too many models for your data set, probably finding
something good from the process, but maybe identifying something spurious since
you looked at so many models. Note that if you take a hypothesis testing
approach where you plan to remove any terms with large p-values in this same
situation, you are really considering all possible models as well because you
could have removed some or all model components.

Methods that consider all
possible models are probably best used in exploratory analyses where you do not
know if any or all terms should be important. If you have more specific
research questions, then you probably should try to focus on comparisons of models
that help you directly answer those questions, either with AIC or p-value methods.</p>
<p>The <code>dredge</code> function provides an automated method of assessing all possible
simpler models based on an initial (full) model. It generates a table of AIC
results, <span class="math inline">\(\Delta\text{AICs}\)</span>, and also shows when various predictors are in or
out of the model for all reduced models possible from an initial model. For
quantitative predictors, the estimated slope is reported when that predictor is
in the model. For categorical variables and interactions with them, it just
puts a “+” in the table to let you know that the term is in the models. Note
that you must run the <code>options(na.action = "na.fail")</code> code to get <code>dredge</code>
to work.</p>
<p>To explore the AICs and compare their results to the adjusted <strong><em>R</em></strong><sup>2</sup>
that we used before for model selection, we can revisit the <em>Snow Depth</em> data set
with related results found in Section <a href="7-4-section8-4.html#section8-4">7.4</a> and
Table <a href="7-4-section8-4.html#tab:Table8-1">1.13</a>. In that situation we were considering
a “full” model that included <em>Elevation</em>, <em>Min.Temp</em>, and <em>Max.Temp</em> as potential
predictor variables after removing two influential points.   And we considered all
possible reduced models from that “full”<a href="#fn139" class="footnote-ref" id="fnref139"><sup>139</sup></a> model. Note
that the <code>dredge</code> output adds one more model that adjusted <strong><em>R</em></strong><sup>2</sup>
can’t consider – the mean-only model that contains no predictor variables. In
the following output it is the last model in the output (worst ranked on AIC).
Including the mean-only model in these results helps us “prove” that there is
support for having something in the model, but only if there is better support for
other models than this simplest possible model.</p>
<p>In reading <code>dredge</code> output<a href="#fn140" class="footnote-ref" id="fnref140"><sup>140</sup></a> as it is constructed here, the models are sorted by
top to bottom AIC values (smallest AIC to largest). The column <code>delta</code> is for
the <span class="math inline">\(\Delta\text{AICs}\)</span> and shows a 0 for the first row, which is the
top-ranked AIC model. Here it is for the model with <em>Elevation</em> and <em>Max.Temp</em> but
not including <em>Min.Temp</em>. This was also the top ranked model from adjusted
<strong><em>R</em></strong><sup>2</sup>, which is reproduced in the <code>adjRsq</code> column. The <code>AIC</code>
is calculated using the previous formula based on the <code>df</code> and <code>logLik</code>
columns. The <code>df</code> is also a useful column for comparing models as it helps
you see how complex each model is. For example, the top model used up 4
<em>model df</em> (three <span class="math inline">\(\beta\text{&#39;s}\)</span> and the residual error variance) and the
most complex model that included four predictor variables used up 5 <em>model df</em>.</p>

<div class="sourceCode" id="cb805"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb805-1" title="1"><span class="kw">library</span>(MuMIn)</a>
<a class="sourceLine" id="cb805-2" title="2"><span class="kw">options</span>(<span class="dt">na.action =</span> <span class="st">&quot;na.fail&quot;</span>) <span class="co">#Must run this code once to use dredge</span></a>
<a class="sourceLine" id="cb805-3" title="3">snotel2R&lt;-snotel2[<span class="op">-</span><span class="kw">c</span>(<span class="dv">9</span>,<span class="dv">22</span>),]; m6 &lt;-<span class="st"> </span><span class="kw">lm</span>(Snow.Depth<span class="op">~</span>Elevation<span class="op">+</span>Min.Temp<span class="op">+</span>Max.Temp, <span class="dt">data=</span>snotel2R)</a>
<a class="sourceLine" id="cb805-4" title="4"><span class="kw">dredge</span>(m6, <span class="dt">rank=</span><span class="st">&quot;AIC&quot;</span>,</a>
<a class="sourceLine" id="cb805-5" title="5">       <span class="dt">extra =</span> <span class="kw">c</span>(<span class="st">&quot;R^2&quot;</span>, <span class="dt">adjRsq=</span><span class="cf">function</span>(x) <span class="kw">summary</span>(x)<span class="op">$</span>adj.r.squared))</a></code></pre></div>
<pre><code>## Global model call: lm(formula = Snow.Depth ~ Elevation + Min.Temp + Max.Temp, data = snotel2R)
## ---
## Model selection table 
##     (Int)     Elv Max.Tmp Min.Tmp    R^2 adjRsq df   logLik   AIC delta
## 4 -167.50 0.02408  1.2530         0.8495 0.8344  4  -80.855 169.7  0.00
## 8 -213.30 0.02686  1.2430  0.9843 0.8535 0.8304  5  -80.541 171.1  1.37
## 2  -80.41 0.01791                 0.8087 0.7996  3  -83.611 173.2  3.51
## 6 -130.70 0.02098          1.0660 0.8134 0.7948  4  -83.322 174.6  4.93
## 5  179.60                 -5.0090 0.6283 0.6106  3  -91.249 188.5 18.79
## 7  178.60         -0.2687 -4.6240 0.6308 0.5939  4  -91.170 190.3 20.63
## 3  119.50         -2.1800         0.4131 0.3852  3  -96.500 199.0 29.29
## 1   40.21                         0.0000 0.0000  2 -102.630 209.3 39.55
##   weight
## 4  0.568
## 8  0.286
## 2  0.098
## 6  0.048
## 5  0.000
## 7  0.000
## 3  0.000
## 1  0.000
## Models ranked by AIC(x)</code></pre>

<p>There are two models that are clearly favored over the others with
<span class="math inline">\(\Delta\text{AICs}\)</span> for the model with <em>Elevation</em> and <em>Max.Temp</em> of 0 and for
the model with all three predictors of 1.37. The <span class="math inline">\(\Delta\text{AIC}\)</span> for the
third ranked model (contains just <em>Elevation</em>) is 3.51 suggesting clear support
for the top model over this. The
difference between the second and third ranked models also provides relatively
strong support for the more complex model over the model with just <em>Elevation</em>.
And the mean-only model had a <span class="math inline">\(\Delta\text{AIC}\)</span> of nearly 40 – suggesting
extremely strong support for the top model versus using no predictors. So we
have pretty clear support for models that include the <em>Elevation</em> and <em>Max.Temp</em>
variables (in both top models) and some support for also including the
<em>Min.Temp</em>, but the top model did not require its inclusion.</p>
<p>We could add further explorations of the term-plots and confidence
intervals for
the slopes from the
top or, here, possibly top two models. We would not spend any time with
p-values since we already used the AIC to assess importance of the model
components and they are invalid if we model select prior to reporting them. We
can quickly compare the slopes for variables that are shared in the two models
since they are both quantitative variables using the output. It is interesting
that the <em>Elevation</em> and <em>Max.Temp</em> slopes change little with the inclusion of
<em>Min.Temp</em> in moving from the top to second ranked model (0.02408 to 0.0286 and
1.253 to 1.243).</p>
<p>This was an observational study and so we can’t consider causal
inferences here
as discussed previously. Generally, the use of AICs does not preclude making
causal statements but if you have randomized assignment of levels of an
explanatory variable, it is more philosophically consistent to use hypothesis
testing methods in that setting. If you went to the effort to impose the levels
of a treatment on the subjects, it also makes sense to see if the differences
created are beyond what you might expect by chance if the treatment didn’t
matter.</p>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Akaike1974">
<p>Akaike, Hirotugu. 1974. “A New Look at the Statistical Model Identification.” <em>IEEE Transactions on Automatic Control</em> 19: 716–23.</p>
</div>
<div id="ref-R-MuMIn">
<p>Barton, Kamil. 2019. <em>MuMIn: Multi-Model Inference</em>. <a href="https://CRAN.R-project.org/package=MuMIn">https://CRAN.R-project.org/package=MuMIn</a>.</p>
</div>
<div id="ref-Burnham2002">
<p>Burnham, Kenneth P., and David R. Anderson. 2002. <em>Model Selection and Multimodel Inference</em>. NY: Springer.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="134">
<li id="fn134"><p>In most situations, it would be crazy
to assume that the true model for a
process has been obtained so we can never pick the “correct” model. In fact, we
won’t even know if we are picking a “good” model, but just the best from a set
of the candidate models. But we can study the general performance of methods
using simulations where we know the true model and the AIC has some useful
properties in identifying the correct model when it is in the candidate set of
models. No such similar theory exists for the adjusted <strong><em>R</em></strong><sup>2</sup>.<a href="7-13-section8-13.html#fnref134" class="footnote-back">↩</a></p></li>
<li id="fn135"><p>Most people now call this
Akaike’s (pronounced <strong>ah-kah-ee-kay</strong>) Information Criterion, but he used the
AIC nomenclature to mean An Information Criterion – he was not so vain as to
name the method after himself in the original paper that proposed it.<a href="7-13-section8-13.html#fnref135" class="footnote-back">↩</a></p></li>
<li id="fn136"><p>More details on these components of the methods will be
left for more advanced material - we will focus on an introduction to using the AIC measure here.<a href="7-13-section8-13.html#fnref136" class="footnote-back">↩</a></p></li>
<li id="fn137"><p>Reproducibility ideas are used in statistics
first by making data and code used available to others (like all the code and
data in this book) and second by trying to use methods that when others perform
similar studies they will find similar results (from Physics, think of the famous
cold-fusion experiments <a href="http://en.wikipedia.org/wiki/Cold_fusion" class="uri">http://en.wikipedia.org/wiki/Cold_fusion</a>.)<a href="7-13-section8-13.html#fnref137" class="footnote-back">↩</a></p></li>
<li id="fn138"><p>Although sometimes excluded, the count of
parameters should include counting the residual variance as a parameter.<a href="7-13-section8-13.html#fnref138" class="footnote-back">↩</a></p></li>
<li id="fn139"><p>We put quotes on “full” or sometimes
call it the “fullish” model because we could always add more to the model, like
interactions or other explanatory variables. So we rarely have a completely full model but
we do have our “most complicated that we are considering” model.<a href="7-13-section8-13.html#fnref139" class="footnote-back">↩</a></p></li>
<li id="fn140"><p>The options in <code>extra=...</code> are to get extra information displayed that you do not necessarily need. You can simply run <code>dredge(m6,rank="AIC")</code> to get just the AIC results.<a href="7-13-section8-13.html#fnref140" class="footnote-back">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="7-12-section8-12.html"><button class="btn btn-default">Previous</button></a>
<a href="7-14-section8-14.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
