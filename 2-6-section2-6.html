<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="2.6 Connecting randomization (nonparametric) and parametric tests | Intermediate Statistics with R" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="gpeterson406/Greenwood_Book" />

<meta name="author" content="Mark C Greenwood" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="2.6 Connecting randomization (nonparametric) and parametric tests | Intermediate Statistics with R">

<title>2.6 Connecting randomization (nonparametric) and parametric tests | Intermediate Statistics with R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li class="has-sub"><a href="1-chapter1.html#chapter1"><span class="toc-section-number">1</span> Preface</a><ul>
<li><a href="1-1-section1-1.html#section1-1"><span class="toc-section-number">1.1</span> Overview of methods</a></li>
<li><a href="1-2-section1-2.html#section1-2"><span class="toc-section-number">1.2</span> Getting started in R</a></li>
<li><a href="1-3-section1-3.html#section1-3"><span class="toc-section-number">1.3</span> Basic summary statistics, histograms, and boxplots using R</a></li>
<li><a href="1-4-section1-4.html#section1-4"><span class="toc-section-number">1.4</span> Chapter summary</a></li>
<li><a href="1-5-section1-5.html#section1-5"><span class="toc-section-number">1.5</span> Summary of important R code</a></li>
<li><a href="1-6-section1-6.html#section1-6"><span class="toc-section-number">1.6</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="2-chapter2.html#chapter2"><span class="toc-section-number">2</span> (R)e-Introduction to statistics</a><ul>
<li><a href="2-1-section2-1.html#section2-1"><span class="toc-section-number">2.1</span> Histograms, boxplots, and density curves</a></li>
<li><a href="2-2-section2-2.html#section2-2"><span class="toc-section-number">2.2</span> Pirate-plots</a></li>
<li><a href="2-3-section2-3.html#section2-3"><span class="toc-section-number">2.3</span> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li><a href="2-4-section2-4.html#section2-4"><span class="toc-section-number">2.4</span> Permutation testing for the two sample mean situation</a></li>
<li><a href="2-5-section2-5.html#section2-5"><span class="toc-section-number">2.5</span> Hypothesis testing (general)</a></li>
<li><a href="2-6-section2-6.html#section2-6"><span class="toc-section-number">2.6</span> Connecting randomization (nonparametric) and parametric tests</a></li>
<li><a href="2-7-section2-7.html#section2-7"><span class="toc-section-number">2.7</span> Second example of permutation tests</a></li>
<li><a href="2-8-section2-8.html#section2-8"><span class="toc-section-number">2.8</span> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li><a href="2-9-section2-9.html#section2-9"><span class="toc-section-number">2.9</span> Confidence intervals and bootstrapping</a></li>
<li><a href="2-10-section2-10.html#section2-10"><span class="toc-section-number">2.10</span> Bootstrap confidence intervals for difference in GPAs</a></li>
<li><a href="2-11-section2-11.html#section2-11"><span class="toc-section-number">2.11</span> Chapter summary</a></li>
<li><a href="2-12-section2-12.html#section2-12"><span class="toc-section-number">2.12</span> Summary of important R code</a></li>
<li><a href="2-13-section2-13.html#section2-13"><span class="toc-section-number">2.13</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="3-chapter3.html#chapter3"><span class="toc-section-number">3</span> One-Way ANOVA</a><ul>
<li><a href="3-1-section3-1.html#section3-1"><span class="toc-section-number">3.1</span> Situation</a></li>
<li><a href="3-2-section3-2.html#section3-2"><span class="toc-section-number">3.2</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li><a href="3-3-section3-3.html#section3-3"><span class="toc-section-number">3.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li><a href="3-4-section3-4.html#section3-4"><span class="toc-section-number">3.4</span> ANOVA model diagnostics including QQ-plots</a></li>
<li><a href="3-5-section3-5.html#section3-5"><span class="toc-section-number">3.5</span> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li><a href="3-6-section3-6.html#section3-6"><span class="toc-section-number">3.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li><a href="3-7-section3-7.html#section3-7"><span class="toc-section-number">3.7</span> Pair-wise comparisons for the Overtake data</a></li>
<li><a href="3-8-section3-8.html#section3-8"><span class="toc-section-number">3.8</span> Chapter summary</a></li>
<li><a href="3-9-section3-9.html#section3-9"><span class="toc-section-number">3.9</span> Summary of important R code</a></li>
<li><a href="3-10-section3-10.html#section3-10"><span class="toc-section-number">3.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="4-chapter4.html#chapter4"><span class="toc-section-number">4</span> Two-Way ANOVA</a><ul>
<li><a href="4-1-section4-1.html#section4-1"><span class="toc-section-number">4.1</span> Situation</a></li>
<li><a href="4-2-section4-2.html#section4-2"><span class="toc-section-number">4.2</span> Designing a two-way experiment and visualizing results</a></li>
<li><a href="4-3-section4-3.html#section4-3"><span class="toc-section-number">4.3</span> Two-Way ANOVA models and hypothesis tests</a></li>
<li><a href="4-4-section4-4.html#section4-4"><span class="toc-section-number">4.4</span> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li><a href="4-5-section4-5.html#section4-5"><span class="toc-section-number">4.5</span> Observational study example: The Psychology of Debt</a></li>
<li><a href="4-6-section4-6.html#section4-6"><span class="toc-section-number">4.6</span> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li><a href="4-7-section4-7.html#section4-7"><span class="toc-section-number">4.7</span> Chapter summary</a></li>
<li><a href="4-8-section4-8.html#section4-8"><span class="toc-section-number">4.8</span> Summary of important R code</a></li>
<li><a href="4-9-section4-9.html#section4-9"><span class="toc-section-number">4.9</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="5-chapter5.html#chapter5"><span class="toc-section-number">5</span> Chi-square tests</a><ul>
<li><a href="5-1-section5-1.html#section5-1"><span class="toc-section-number">5.1</span> Situation, contingency tables, and tableplots</a></li>
<li><a href="5-2-section5-2.html#section5-2"><span class="toc-section-number">5.2</span> Homogeneity test hypotheses</a></li>
<li><a href="5-3-section5-3.html#section5-3"><span class="toc-section-number">5.3</span> Independence test hypotheses</a></li>
<li><a href="5-4-section5-4.html#section5-4"><span class="toc-section-number">5.4</span> Models for R by C tables</a></li>
<li><a href="5-5-section5-5.html#section5-5"><span class="toc-section-number">5.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-6-section5-6.html#section5-6"><span class="toc-section-number">5.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-7-section5-7.html#section5-7"><span class="toc-section-number">5.7</span> Examining residuals for the source of differences</a></li>
<li><a href="5-8-section5-8.html#section5-8"><span class="toc-section-number">5.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li><a href="5-9-section5-9.html#section5-9"><span class="toc-section-number">5.9</span> Political party and voting results: Complete analysis</a></li>
<li><a href="5-10-section5-10.html#section5-10"><span class="toc-section-number">5.10</span> Is cheating and lying related in students?</a></li>
<li><a href="5-11-section5-11.html#section5-11"><span class="toc-section-number">5.11</span> Analyzing a stratified random sample of California schools</a></li>
<li><a href="5-12-section5-12.html#section5-12"><span class="toc-section-number">5.12</span> Chapter summary</a></li>
<li><a href="5-13-section5-13.html#section5-13"><span class="toc-section-number">5.13</span> Summary of important R commands</a></li>
<li><a href="5-14-section5-14.html#section5-14"><span class="toc-section-number">5.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="6-chapter6.html#chapter6"><span class="toc-section-number">6</span> Correlation and Simple Linear Regression</a><ul>
<li><a href="6-1-section6-1.html#section6-1"><span class="toc-section-number">6.1</span> Relationships between two quantitative variables</a></li>
<li><a href="6-2-section6-2.html#section6-2"><span class="toc-section-number">6.2</span> Estimating the correlation coefficient</a></li>
<li><a href="6-3-section6-3.html#section6-3"><span class="toc-section-number">6.3</span> Relationships between variables by groups</a></li>
<li><a href="6-4-section6-4.html#section6-4"><span class="toc-section-number">6.4</span> Inference for the correlation coefficient</a></li>
<li><a href="6-5-section6-5.html#section6-5"><span class="toc-section-number">6.5</span> Are tree diameters related to tree heights?</a></li>
<li><a href="6-6-section6-6.html#section6-6"><span class="toc-section-number">6.6</span> Describing relationships with a regression model</a></li>
<li><a href="6-7-section6-7.html#section6-7"><span class="toc-section-number">6.7</span> Least Squares Estimation</a></li>
<li><a href="6-8-section6-8.html#section6-8"><span class="toc-section-number">6.8</span> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li><a href="6-9-section6-9.html#section6-9"><span class="toc-section-number">6.9</span> Outliers: leverage and influence</a></li>
<li><a href="6-10-section6-10.html#section6-10"><span class="toc-section-number">6.10</span> Residual diagnostics – setting the stage for inference</a></li>
<li><a href="6-11-section6-11.html#section6-11"><span class="toc-section-number">6.11</span> Old Faithful discharge and waiting times</a></li>
<li><a href="6-12-section6-12.html#section6-12"><span class="toc-section-number">6.12</span> Chapter summary</a></li>
<li><a href="6-13-section6-13.html#section6-13"><span class="toc-section-number">6.13</span> Summary of important R code</a></li>
<li><a href="6-14-section6-14.html#section6-14"><span class="toc-section-number">6.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="7-chapter7.html#chapter7"><span class="toc-section-number">7</span> Simple linear regression inference</a><ul>
<li><a href="7-1-section7-1.html#section7-1"><span class="toc-section-number">7.1</span> Model</a></li>
<li><a href="7-2-section7-2.html#section7-2"><span class="toc-section-number">7.2</span> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li><a href="7-3-section7-3.html#section7-3"><span class="toc-section-number">7.3</span> Bozeman temperature trend</a></li>
<li><a href="7-4-section7-4.html#section7-4"><span class="toc-section-number">7.4</span> Randomization-based inferences for the slope coefficient</a></li>
<li><a href="7-5-section7-5.html#section7-5"><span class="toc-section-number">7.5</span> Transformations part I: Linearizing relationships</a></li>
<li><a href="7-6-section7-6.html#section7-6"><span class="toc-section-number">7.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li><a href="7-7-section7-7.html#section7-7"><span class="toc-section-number">7.7</span> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li><a href="7-8-section7-8.html#section7-8"><span class="toc-section-number">7.8</span> Chapter summary</a></li>
<li><a href="7-9-section7-9.html#section7-9"><span class="toc-section-number">7.9</span> Summary of important R code</a></li>
<li><a href="7-10-section7-10.html#section7-10"><span class="toc-section-number">7.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="8-chapter8.html#chapter8"><span class="toc-section-number">8</span> Multiple linear regression</a><ul>
<li><a href="8-1-section8-1.html#section8-1"><span class="toc-section-number">8.1</span> Going from SLR to MLR</a></li>
<li><a href="8-2-section8-2.html#section8-2"><span class="toc-section-number">8.2</span> Validity conditions in MLR</a></li>
<li><a href="8-3-section8-3.html#section8-3"><span class="toc-section-number">8.3</span> Interpretation of MLR terms</a></li>
<li><a href="8-4-section8-4.html#section8-4"><span class="toc-section-number">8.4</span> Comparing multiple regression models</a></li>
<li><a href="8-5-section8-5.html#section8-5"><span class="toc-section-number">8.5</span> General recommendations for MLR interpretations and VIFs</a></li>
<li><a href="8-6-section8-6.html#section8-6"><span class="toc-section-number">8.6</span> MLR inference: Parameter inferences using the t-distribution</a></li>
<li><a href="8-7-section8-7.html#section8-7"><span class="toc-section-number">8.7</span> Overall F-test in multiple linear regression</a></li>
<li><a href="8-8-section8-8.html#section8-8"><span class="toc-section-number">8.8</span> Case study: First year college GPA and SATs</a></li>
<li><a href="8-9-section8-9.html#section8-9"><span class="toc-section-number">8.9</span> Different intercepts for different groups: MLR with indicator variables</a></li>
<li><a href="8-10-section8-10.html#section8-10"><span class="toc-section-number">8.10</span> Additive MLR with more than two groups: Headache example</a></li>
<li><a href="8-11-section8-11.html#section8-11"><span class="toc-section-number">8.11</span> Different slopes and different intercepts</a></li>
<li><a href="8-12-section8-12.html#section8-12"><span class="toc-section-number">8.12</span> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li><a href="8-13-section8-13.html#section8-13"><span class="toc-section-number">8.13</span> AICs for model selection</a></li>
<li><a href="8-14-section8-14.html#section8-14"><span class="toc-section-number">8.14</span> Case study: Forced expiratory volume model selection using AICs</a></li>
<li><a href="8-15-section8-15.html#section8-15"><span class="toc-section-number">8.15</span> Chapter summary</a></li>
<li><a href="8-16-section8-16.html#section8-16"><span class="toc-section-number">8.16</span> Summary of important R code</a></li>
<li><a href="8-17-section8-17.html#section8-17"><span class="toc-section-number">8.17</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="9-chapter9.html#chapter9"><span class="toc-section-number">9</span> Case studies</a><ul>
<li><a href="9-1-section9-1.html#section9-1"><span class="toc-section-number">9.1</span> Overview of material covered</a></li>
<li><a href="9-2-section9-2.html#section9-2"><span class="toc-section-number">9.2</span> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li><a href="9-3-section9-3.html#section9-3"><span class="toc-section-number">9.3</span> Ants learn to rely on more informative attributes during decision-making</a></li>
<li><a href="9-4-section9-4.html#section9-4"><span class="toc-section-number">9.4</span> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li><a href="9-5-section9-5.html#section9-5"><span class="toc-section-number">9.5</span> What do didgeridoos really do about sleepiness?</a></li>
<li><a href="9-6-section9-6.html#section9-6"><span class="toc-section-number">9.6</span> General summary</a></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="section2-6" class="section level2">
<h2><span class="header-section-number">2.6</span> Connecting randomization (nonparametric) and parametric tests</h2>
<p>In developing statistical inference techniques, we need to define the test
statistic, <span class="math inline">\(T\)</span>, that measures the quantity of interest. To compare the means of
two groups, a statistic is needed
that measures their differences. In general, for comparing two groups, the
choice is simple – a difference in the means often works well and is a
natural choice. There are other options such as tracking the ratio of means or
possibly the difference in medians. Instead of just using the difference in the
means, we also could “standardize” the difference in the means by dividing by
an appropriate quantity that reflects the variation in the difference in the
means. All of these are valid and can sometimes provide similar results - it
ends up that there are many possibilities for testing using the randomization
(nonparametric)

techniques introduced previously. Parametric

statistical
methods focus on means because the statistical theory surrounding means is
quite a bit easier (not easy, just easier) than other options. There are
just a couple of test statistics that you can use and end up with named
distributions to use for generating inferences. Randomization techniques allow
inference for other quantities (such as ratios of means or differences in medians) but our focus here will be on using
randomization for inferences on means to see the similarities with the more
traditional parametric procedures used in these situations.</p>
<p>In two-sample mean situations, instead of working just with the
difference in the means, we often calculate a test statistic that is called the
<strong><em>equal variance two-independent samples t-statistic</em></strong>. The test statistic is</p>
<p><span class="math display">\[t = \frac{\bar{x}_1 - \bar{x}_2}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}},\]</span></p>
<p>where <span class="math inline">\(s_1^2\)</span> and <span class="math inline">\(s_2^2\)</span> are the sample variances for the two groups, <span class="math inline">\(n_1\)</span> and
<span class="math inline">\(n_2\)</span> are the sample sizes for the two groups, and the
<strong><em>pooled sample standard deviation</em></strong>,</p>
<p><span class="math display">\[s_p = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}.\]</span></p>
<p>The <span class="math inline">\(t\)</span>-statistic keeps the important comparison between the means in the
numerator that we used before and standardizes (re-scales) that difference so
that <span class="math inline">\(t\)</span> will follow a <span class="math inline">\(t\)</span>-distribution

(a parametric

“named” distribution) if
certain assumptions are met. But first we should see if standardizing the
difference in the means had an impact on our permutation test

results. It ends up that, while not too obvious, the <code>summary</code> of the <code>lm</code> we fit earlier contains this test statistic<a href="#fn35" class="footnote-ref" id="fnref35"><sup>35</sup></a>. Instead
of using the second model coefficient that estimates the difference in the means of the groups, we will extract the test statistic from the table of summary output that is in the <code>coef</code> object in the summary – using <code>$</code> to reference the <code>coef</code> information only. In the <code>coef</code> object in the summary, results related to the <code>ConditionCommute</code> are again useful for the comparison of two groups.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" title="1"><span class="kw">summary</span>(lm1)<span class="op">$</span>coef</a></code></pre></div>
<pre><code>##                   Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)      135.80000   8.862996 15.322133 3.832161e-15
## Conditioncommute -25.93333  12.534169 -2.069011 4.788928e-02</code></pre>
<p>The first column of numbers contains the estimated difference in the sample means (-25.933 here) that was used before. The next column is the <code>Std. Error</code> column that contains the standard error (SE) of the estimated difference in the means, which is <span class="math inline">\(s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\)</span> and also the denominator used to form the t-test statistic (12.53 here). It will be a common theme in this material to take the ratio of the estimate (-25.933) to its SE (12.53) to generate test statistics, which provides -2.07 – this is the “standardized” estimate of the difference in the means. It is also a test statistic (<span class="math inline">\(T\)</span>) that we can use in a permutation test. This value is in the second row and third column of <code>summary(lm1)$coef</code> and to extract it the bracket notation is again employed. Specifically we want to extract <code>summary(lm1)$coef[2,3]</code> and using it and its permuted data equivalents to calculate a p-value. Since we are doing a two-sided
test, the code resembles the permutation test code in Section <a href="2-4-section2-4.html#section2-4">2.4</a>
with the new <span class="math inline">\(t\)</span>-statistic replacing the difference in the sample means that we
used before.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb101-1" title="1">Tobs &lt;-<span class="st"> </span><span class="kw">summary</span>(lm1)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb101-2" title="2">Tobs</a></code></pre></div>
<pre><code>## [1] -2.069011</code></pre>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb103-1" title="1">B &lt;-<span class="st"> </span><span class="dv">1000</span></a>
<a class="sourceLine" id="cb103-2" title="2"><span class="kw">set.seed</span>(<span class="dv">406</span>)</a>
<a class="sourceLine" id="cb103-3" title="3">Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</a>
<a class="sourceLine" id="cb103-4" title="4"><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</a>
<a class="sourceLine" id="cb103-5" title="5">  lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span><span class="kw">shuffle</span>(Condition), <span class="dt">data=</span>dsample)</a>
<a class="sourceLine" id="cb103-6" title="6">  Tstar[b] &lt;-<span class="st"> </span><span class="kw">summary</span>(lmP)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb103-7" title="7">}</a>
<a class="sourceLine" id="cb103-8" title="8"><span class="kw">pdata</span>(<span class="kw">abs</span>(Tstar), <span class="kw">abs</span>(Tobs), <span class="dt">lower.tail=</span>F)</a></code></pre></div>
<pre><code>## [1] 0.041</code></pre>
<p>The permutation distribution

in Figure <a href="2-6-section2-6.html#fig:Figure2-12">2.13</a> looks
similar to the previous results with slightly different x-axis scaling. The
observed <span class="math inline">\(t\)</span>-statistic was <span class="math inline">\(-2.07\)</span> and the proportion of permuted results that
were as or more extreme than the observed result
was 0.041. This difference is due to a different set of random permutations
being selected. If you run permutation code, you will often get slightly
different results each time you run it. If you are uncomfortable with the
variation in the results, you can run more than <em>B</em> = 1,000 permutations (say
10,000) and the variability in the resulting p-values will be reduced further.
Usually this uncertainty will not cause any substantive problems – but do not
be surprised if your results vary if you use different random number seeds.</p>

<div class="figure"><span id="fig:Figure2-12"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-12-1.png" alt="Permutation distribution of the \(t\)-statistic." width="960" />
<p class="caption">
Figure 2.13: Permutation distribution of the <span class="math inline">\(t\)</span>-statistic.
</p>
</div>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb105-1" title="1"><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)</a>
<a class="sourceLine" id="cb105-2" title="2"><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a>
<a class="sourceLine" id="cb105-3" title="3"><span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</a>
<a class="sourceLine" id="cb105-4" title="4"><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<p>The parametric version

of these results is based on using what is called
the <strong><em>two-independent sample t-test</em></strong>. There are actually two versions of this
test, one that assumes that variances are equal in the groups and one that
does not. There is a rule of thumb that if the <strong>ratio of the larger standard
deviation over the smaller standard deviation is less than 2, the equal variance
procedure is OK</strong>. It ends up that this assumption
is less important if the sample sizes in the groups are approximately equal
and more important if the groups contain different numbers of observations. In
comparing the two potential test statistics, the procedure that assumes equal
variances has a complicated denominator (see the formula above for <span class="math inline">\(t\)</span>
involving <span class="math inline">\(s_p\)</span>) but a simple formula for <strong><em>degrees of freedom</em></strong> (<strong><em>df</em></strong>)


for the <span class="math inline">\(t\)</span>-distribution (<span class="math inline">\(df=n_1+n_2-2\)</span>) that approximates the
distribution of the test statistic, <span class="math inline">\(t\)</span>, under the null hypothesis. The
procedure that assumes unequal variances has a simpler test statistic and a
very complicated degrees of freedom formula. The equal variance procedure is
equivalent to the methods we will consider in Chapters
<a href="3-chapter3.html#chapter3"><strong>??</strong></a> and <a href="4-chapter4.html#chapter4"><strong>??</strong></a> so that
will be our focus for the two group problem and is what we get when using the <code>lm</code> model to estimate the differences in the group means. The unequal variance version of the two-sample t-test is available in the <code>t.test</code> function if needed.</p>

<div class="figure"><span id="fig:Figure2-13"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-13-1.png" alt="Plots of \(t\)-distributions with 2, 10, and 20 degrees of freedom and a normal distribution (dashed line). Note how the \(t\)-distributions get closer to the normal distribution as the degrees of freedom increase and at 20 degrees of freedom, the \(t\)-distribution almost matches a standard normal curve." width="672" />
<p class="caption">
Figure 2.14: Plots of <span class="math inline">\(t\)</span>-distributions with 2, 10, and 20 degrees of freedom and a normal distribution (dashed line). Note how the <span class="math inline">\(t\)</span>-distributions get closer to the normal distribution as the degrees of freedom increase and at 20 degrees of freedom, the <span class="math inline">\(t\)</span>-distribution <em>almost</em> matches a standard normal curve.
</p>
</div>
<p>If the assumptions for the equal variance <span class="math inline">\(t\)</span>-test and the null
hypothesis are true, then the sampling distribution of the test statistic should
follow a <span class="math inline">\(t\)</span>-distribution

with <span class="math inline">\(n_1+n_2-2\)</span> degrees of freedom (so the total sample size, <span class="math inline">\(n\)</span>, minus 2). The <strong><em>t-distribution</em></strong> is a bell-shaped curve that is more spread out for smaller
values of degrees of freedom as shown in Figure <a href="2-6-section2-6.html#fig:Figure2-13">2.14</a>. The
<span class="math inline">\(t\)</span>-distribution looks more and more like a <strong><em>standard normal distribution</em></strong>

(<span class="math inline">\(N(0,1)\)</span>) as the degrees of freedom increase.</p>
<p>To get the p-value for the parametric <span class="math inline">\(t\)</span>-test,

we need to calculate the
test statistic and <span class="math inline">\(df\)</span>, then look up the areas in the tails of the
<span class="math inline">\(t\)</span>-distribution

relative to the observed <span class="math inline">\(t\)</span>-statistic. We’ll learn how to use
R to do this below, but for now we will allow the <code>summary</code> of the <code>lm</code> function to take
care of this. In the <code>ConditionCommute</code> row of the summary and the <code>Pr(&gt;|t|)</code> column, we can find the p-value associated with the test statistic. We can either calculate the degrees of freedom for the <span class="math inline">\(t\)</span>-distribution using <span class="math inline">\(n_1+n_2-2 = 15+15-2 = 28\)</span> or explore the full suite of the model summary that is repeated below. In the first row below the <code>ConditionCommute</code> row, it reports “… 28 degrees of freedom” and these are the same <span class="math inline">\(df\)</span> that are needed to report and look up for any of the <span class="math inline">\(t\)</span>-statistics in the model summary.</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb106-1" title="1"><span class="kw">summary</span>(lm1)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Distance ~ Condition, data = dsample)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -63.800 -21.850   4.133  15.150  72.200 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)       135.800      8.863  15.322 3.83e-15
## Conditioncommute  -25.933     12.534  -2.069   0.0479
## 
## Residual standard error: 34.33 on 28 degrees of freedom
## Multiple R-squared:  0.1326, Adjusted R-squared:  0.1016 
## F-statistic: 4.281 on 1 and 28 DF,  p-value: 0.04789</code></pre>
<p>So the parametric <span class="math inline">\(t\)</span>-test gives a p-value of 0.0479 from a test statistic of
-2.07. The p-value is very similar to the two permutation results found before. The
reason for this similarity is that the permutation distribution looks like a <span class="math inline">\(t\)</span>-distribution with 28 degrees
of freedom. Figure <a href="2-6-section2-6.html#fig:Figure2-14">2.15</a> shows how similar the two distributions
happened to be here, where the only difference in shape is near the peak of the distributions with a slight difference of the permutation distribution to shift to the right.</p>

<div class="figure"><span id="fig:Figure2-14"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-14-1.png" alt="Plot of permutation and \(t\)-distribution with \(df=28\). Note the close match in the two distributions, especially in the tails of the distributions where we are obtaining the p-values." width="576" />
<p class="caption">
Figure 2.15: Plot of permutation and <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df=28\)</span>. Note the close match in the two distributions, especially in the tails of the distributions where we are obtaining the p-values.
</p>
</div>
<p>In your previous statistics course, you might have used an applet or
a table to find p-values such as what was provided in the previous R output.
When not directly provided in the output of a function, R can be used to look up
p-values<a href="#fn36" class="footnote-ref" id="fnref36"><sup>36</sup></a> from
named distributions such as the <span class="math inline">\(t\)</span>-distribution. In this case, the distribution
of the test statistic under the null hypothesis is a <span class="math inline">\(t(28)\)</span> or a <span class="math inline">\(t\)</span> with 28
degrees of freedom. The <code>pt</code> function is used to get p-values from the

<span class="math inline">\(t\)</span>-distribution in the same manner that <code>pdata</code> could help us to find p-values
from the permutation distribution.

We need to provide the <code>df=...</code> and specify
the tail of the distribution of interest using the <code>lower.tail</code> option along
with the cutoff of interest. If we want the area to the left of -2.07:</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" title="1"><span class="kw">pt</span>(<span class="op">-</span><span class="fl">2.069</span>, <span class="dt">df=</span><span class="dv">28</span>, <span class="dt">lower.tail=</span>T)</a></code></pre></div>
<pre><code>## [1] 0.02394519</code></pre>
<p>And we can double it to get the p-value that was in the output, because
the <span class="math inline">\(t\)</span>-distribution is symmetric:
</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb110-1" title="1"><span class="dv">2</span><span class="op">*</span><span class="kw">pt</span>(<span class="op">-</span><span class="fl">2.069</span>, <span class="dt">df=</span><span class="dv">28</span>, <span class="dt">lower.tail=</span>T)</a></code></pre></div>
<pre><code>## [1] 0.04789038</code></pre>
<p>More generally, we could always make the test statistic positive using the
absolute value (<code>abs</code>), find the area to the right of it (<code>lower.tail=F</code>), and then double that for a
two-sided test p-value:</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb112-1" title="1"><span class="dv">2</span><span class="op">*</span><span class="kw">pt</span>(<span class="kw">abs</span>(<span class="op">-</span><span class="fl">2.069</span>), <span class="dt">df=</span><span class="dv">28</span>, <span class="dt">lower.tail=</span>F)</a></code></pre></div>
<pre><code>## [1] 0.04789038</code></pre>
<p>Permutation distributions  do not need to match the named
parametric distribution

to work correctly, although this happened in the previous example.
The parametric approach, the <span class="math inline">\(t\)</span>-test, requires certain conditions to be true (or at least not be clearly violated)
for the sampling distribution of the
statistic to follow the named distribution and provide accurate p-values. The
conditions for the t-test are:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Independent observations</strong>:

Each observation obtained is unrelated to all other
observations. To assess this, consider whether anything in the data collection
might lead to clustered or related observations that are un-related to the
differences in the groups. For example, was the same person measured more than
once<a href="#fn37" class="footnote-ref" id="fnref37"><sup>37</sup></a>?</p></li>
<li><p><strong>Equal variances</strong> in the groups (because we used a procedure that assumes
equal variances! – there is another procedure that allows you to relax this
assumption if needed…). To assess this, compare the standard deviations and
variability in the pirate-plots  and see if they look noticeably different. Be
particularly critical of this assessment if the sample sizes differ greatly
between groups.</p></li>
<li><p><strong>Normal distributions</strong> of the observations in each group. We’ll learn more
diagnostics later, but the pirate-plots are a good place to start to
help you look for potential skew or outliers. If you find
skew and/or outliers, that would suggest a problem with the assumption of
normality as normal distributions
  
are symmetric and extreme observations occur
very rarely.</p></li>
</ol>
<p>For the permutation test,  we relax the third condition and replace it with:</p>
<ol start="3" style="list-style-type: decimal">
<li><strong><em>Similar distributions for the groups:</em></strong> The permutation approach
allows valid inferences as long as the two groups have similar shapes and only
possibly differ in their centers. In other words, the distributions need not
look normal for the procedure to work well, but they do need to look similar.
</li>
</ol>
<p>In the bicycle overtake study, we are not able to assume that the independent
observation condition is met because of multiple measurements taken on the same ride. The fact that the same rider was used for all observations is not really a violation of independence here because there was only one subject used. If multiple subjects had been used, then that also could present a violation of the independence assumption. This violation is important to note as the inferences may not be correct due to the violation of this assumption and more sophisticated statistical methods would be needed to complete this analysis correctly. The equal variance condition does not appear to be violated. The standard deviations are 28.4 vs 39.4, so this difference is not “large”
according to the rule of thumb noted above (ratio of SDs is about 1.4). There is also little evidence in the pirate-plots to suggest a violation of the normality condition for each of the groups (Figure <a href="2-2-section2-2.html#fig:Figure2-6">2.6</a>). Additionally, the shapes look similar for the two groups so we also could feel comfortable using the permutation approach based on
its version of condition (3) above. Note that when assessing assumptions, it is important to never state that assumptions are met – we never know the truth and can only look at the information in the sample to look for evidence of problems with particular conditions. Violations of those conditions suggest a need for either more sophisticated statistical tools<a href="#fn38" class="footnote-ref" id="fnref38"><sup>38</sup></a> or possibly transformations of the response variable (discussed in <a href="7-chapter7.html#chapter7"><strong>??</strong></a>).</p>
<p>The permutation approach is resistant

to impacts of violations of the
normality assumption. It is not resistant to impacts of violations of any of
the other assumptions.

In fact, it can be quite sensitive to unequal variances
as it will detect differences in the variances of the groups instead of
differences in the means. Its scope of inference is the same as the parametric
approach.  It also provides similarly inaccurate conclusions in the presence
of non-independent observations as for the parametric approach. In this
example, we discover that parametric and permutation approaches provide very
similar inferences, but both are subject to concerns related to violations of the independent observations condition. And we haven’t directly addressed the size and direction of the differences, which is addressed in the coming discussion of confidence intervals.</p>
<p>For comparison, we can also explore the original data set of all <span class="math inline">\(n=1,636\)</span> observations for the two outfits. The estimated difference in the means is -3.003 cm (<em>commute</em> minus <em>casual</em>), the standard error is 1.472, the <span class="math inline">\(t\)</span>-statistic is -2.039 and using a <span class="math inline">\(t\)</span>-distribution with 1634 <span class="math inline">\(df\)</span>, the p-value is 0.0416. The estimated difference in the means is much smaller but the p-value is similar to the results for the sub-sample we analyzed. The SE is much smaller with the large sample size which corresponds to having higher power to detect smaller differences.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb114-1" title="1">lm_all &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>ddsub)</a>
<a class="sourceLine" id="cb114-2" title="2"><span class="kw">summary</span>(lm_all)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Distance ~ Condition, data = ddsub)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -106.608  -17.608    0.389   16.392  127.389 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)       117.611      1.066 110.357   &lt;2e-16
## Conditioncommute   -3.003      1.472  -2.039   0.0416
## 
## Residual standard error: 29.75 on 1634 degrees of freedom
## Multiple R-squared:  0.002539,   Adjusted R-squared:  0.001929 
## F-statistic:  4.16 on 1 and 1634 DF,  p-value: 0.04156</code></pre>
<p>The permutations take a little more computing power with almost two thousand observations to shuffle, but this is manageable on a modern laptop as it only has to be completed once to fill in the distribution of the test statistic under 1,000 shuffles. And the p-value obtained is a close match to the parametric result at 0.045 for the permutation version and 0.042 for the parametric approach. So we would get similar inferences for strength of evidence against the null with either the smaller data set or the full data set but the estimated size of the differences is quite a bit different. It is important to note that other random samples from the larger data set would give different p-values and this one happened to match the larger set more closely than one might expect in general.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb116-1" title="1">Tobs &lt;-<span class="st"> </span><span class="kw">summary</span>(lm_all)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb116-2" title="2">Tobs</a></code></pre></div>
<pre><code>## [1] -2.039491</code></pre>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb118-1" title="1">B &lt;-<span class="st"> </span><span class="dv">1000</span></a>
<a class="sourceLine" id="cb118-2" title="2"><span class="kw">set.seed</span>(<span class="dv">406</span>)</a>
<a class="sourceLine" id="cb118-3" title="3">Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)</a>
<a class="sourceLine" id="cb118-4" title="4"><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){</a>
<a class="sourceLine" id="cb118-5" title="5">  lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span><span class="kw">shuffle</span>(Condition), <span class="dt">data=</span>ddsub)</a>
<a class="sourceLine" id="cb118-6" title="6">  Tstar[b] &lt;-<span class="st"> </span><span class="kw">summary</span>(lmP)<span class="op">$</span>coef[<span class="dv">2</span>,<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb118-7" title="7">}</a>
<a class="sourceLine" id="cb118-8" title="8"><span class="kw">pdata</span>(<span class="kw">abs</span>(Tstar), <span class="kw">abs</span>(Tobs), <span class="dt">lower.tail=</span>F)</a></code></pre></div>
<pre><code>## [1] 0.045</code></pre>

<div class="figure"><span id="fig:Figure2-15"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-15-1.png" alt="Permutation distribution of the \(t\)-statistic for \(n=1,636\) overtake data set." width="960" />
<p class="caption">
Figure 2.16: Permutation distribution of the <span class="math inline">\(t\)</span>-statistic for <span class="math inline">\(n=1,636\)</span> overtake data set.
</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="35">
<li id="fn35"><p>The <code>t.test</code> function with the <code>var.equal=T</code> option is the more direct route to calculating this statistic (here that would be <code>t.test(Distance~Condition, data=dsamp, var.equal=T)</code>), but since we can get the result of interest by fitting a linear model, we will use that approach.<a href="2-6-section2-6.html#fnref35" class="footnote-back">↩</a></p></li>
<li id="fn36"><p>On exams, you might be asked to describe the area of interest, sketch a
picture of the area of interest, and/or note the distribution you would use. Make sure you think about what you are trying to do here as much as learning the mechanics of how to get p-values from R.<a href="2-6-section2-6.html#fnref36" class="footnote-back">↩</a></p></li>
<li id="fn37"><p>In some studies, the same subject is measured in both conditions and
this violates the assumptions of this procedure.<a href="2-6-section2-6.html#fnref37" class="footnote-back">↩</a></p></li>
<li id="fn38"><p>At this level, it is critical to learn the tools and learn where they might provide inaccurate inferences. If you explore more advanced statistical resources, you will encounter methods that can allow you to obtain valid inferences in even more scenarios.<a href="2-6-section2-6.html#fnref38" class="footnote-back">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="2-5-section2-5.html"><button class="btn btn-default">Previous</button></a>
<a href="2-7-section2-7.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
