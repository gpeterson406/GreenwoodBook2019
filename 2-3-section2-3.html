<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="2.3 Models, hypotheses, and permutations for the two sample mean situation | Intermediate Statistics with R" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="gpeterson406/Greenwood_Book" />

<meta name="author" content="Mark C Greenwood" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="2.3 Models, hypotheses, and permutations for the two sample mean situation | Intermediate Statistics with R">

<title>2.3 Models, hypotheses, and permutations for the two sample mean situation | Intermediate Statistics with R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li class="has-sub"><a href="1-chapter1.html#chapter1"><span class="toc-section-number">1</span> Preface</a><ul>
<li><a href="1-1-section1-1.html#section1-1"><span class="toc-section-number">1.1</span> Overview of methods</a></li>
<li><a href="1-2-section1-2.html#section1-2"><span class="toc-section-number">1.2</span> Getting started in R</a></li>
<li><a href="1-3-section1-3.html#section1-3"><span class="toc-section-number">1.3</span> Basic summary statistics, histograms, and boxplots using R</a></li>
<li><a href="1-4-section1-4.html#section1-4"><span class="toc-section-number">1.4</span> Chapter summary</a></li>
<li><a href="1-5-section1-5.html#section1-5"><span class="toc-section-number">1.5</span> Summary of important R code</a></li>
<li><a href="1-6-section1-6.html#section1-6"><span class="toc-section-number">1.6</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="2-chapter2.html#chapter2"><span class="toc-section-number">2</span> (R)e-Introduction to statistics</a><ul>
<li><a href="2-1-section2-1.html#section2-1"><span class="toc-section-number">2.1</span> Histograms, boxplots, and density curves</a></li>
<li><a href="2-2-section2-2.html#section2-2"><span class="toc-section-number">2.2</span> Pirate-plots</a></li>
<li><a href="2-3-section2-3.html#section2-3"><span class="toc-section-number">2.3</span> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li><a href="2-4-section2-4.html#section2-4"><span class="toc-section-number">2.4</span> Permutation testing for the two sample mean situation</a></li>
<li><a href="2-5-section2-5.html#section2-5"><span class="toc-section-number">2.5</span> Hypothesis testing (general)</a></li>
<li><a href="2-6-section2-6.html#section2-6"><span class="toc-section-number">2.6</span> Connecting randomization (nonparametric) and parametric tests</a></li>
<li><a href="2-7-section2-7.html#section2-7"><span class="toc-section-number">2.7</span> Second example of permutation tests</a></li>
<li><a href="2-8-section2-8.html#section2-8"><span class="toc-section-number">2.8</span> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li><a href="2-9-section2-9.html#section2-9"><span class="toc-section-number">2.9</span> Confidence intervals and bootstrapping</a></li>
<li><a href="2-10-section2-10.html#section2-10"><span class="toc-section-number">2.10</span> Bootstrap confidence intervals for difference in GPAs</a></li>
<li><a href="2-11-section2-11.html#section2-11"><span class="toc-section-number">2.11</span> Chapter summary</a></li>
<li><a href="2-12-section2-12.html#section2-12"><span class="toc-section-number">2.12</span> Summary of important R code</a></li>
<li><a href="2-13-section2-13.html#section2-13"><span class="toc-section-number">2.13</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="3-chapter3.html#chapter3"><span class="toc-section-number">3</span> One-Way ANOVA</a><ul>
<li><a href="3-1-section3-1.html#section3-1"><span class="toc-section-number">3.1</span> Situation</a></li>
<li><a href="3-2-section3-2.html#section3-2"><span class="toc-section-number">3.2</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li><a href="3-3-section3-3.html#section3-3"><span class="toc-section-number">3.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li><a href="3-4-section3-4.html#section3-4"><span class="toc-section-number">3.4</span> ANOVA model diagnostics including QQ-plots</a></li>
<li><a href="3-5-section3-5.html#section3-5"><span class="toc-section-number">3.5</span> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li><a href="3-6-section3-6.html#section3-6"><span class="toc-section-number">3.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li><a href="3-7-section3-7.html#section3-7"><span class="toc-section-number">3.7</span> Pair-wise comparisons for the Overtake data</a></li>
<li><a href="3-8-section3-8.html#section3-8"><span class="toc-section-number">3.8</span> Chapter summary</a></li>
<li><a href="3-9-section3-9.html#section3-9"><span class="toc-section-number">3.9</span> Summary of important R code</a></li>
<li><a href="3-10-section3-10.html#section3-10"><span class="toc-section-number">3.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="4-chapter4.html#chapter4"><span class="toc-section-number">4</span> Two-Way ANOVA</a><ul>
<li><a href="4-1-section4-1.html#section4-1"><span class="toc-section-number">4.1</span> Situation</a></li>
<li><a href="4-2-section4-2.html#section4-2"><span class="toc-section-number">4.2</span> Designing a two-way experiment and visualizing results</a></li>
<li><a href="4-3-section4-3.html#section4-3"><span class="toc-section-number">4.3</span> Two-Way ANOVA models and hypothesis tests</a></li>
<li><a href="4-4-section4-4.html#section4-4"><span class="toc-section-number">4.4</span> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li><a href="4-5-section4-5.html#section4-5"><span class="toc-section-number">4.5</span> Observational study example: The Psychology of Debt</a></li>
<li><a href="4-6-section4-6.html#section4-6"><span class="toc-section-number">4.6</span> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li><a href="4-7-section4-7.html#section4-7"><span class="toc-section-number">4.7</span> Chapter summary</a></li>
<li><a href="4-8-section4-8.html#section4-8"><span class="toc-section-number">4.8</span> Summary of important R code</a></li>
<li><a href="4-9-section4-9.html#section4-9"><span class="toc-section-number">4.9</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="5-chapter5.html#chapter5"><span class="toc-section-number">5</span> Chi-square tests</a><ul>
<li><a href="5-1-section5-1.html#section5-1"><span class="toc-section-number">5.1</span> Situation, contingency tables, and tableplots</a></li>
<li><a href="5-2-section5-2.html#section5-2"><span class="toc-section-number">5.2</span> Homogeneity test hypotheses</a></li>
<li><a href="5-3-section5-3.html#section5-3"><span class="toc-section-number">5.3</span> Independence test hypotheses</a></li>
<li><a href="5-4-section5-4.html#section5-4"><span class="toc-section-number">5.4</span> Models for R by C tables</a></li>
<li><a href="5-5-section5-5.html#section5-5"><span class="toc-section-number">5.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-6-section5-6.html#section5-6"><span class="toc-section-number">5.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-7-section5-7.html#section5-7"><span class="toc-section-number">5.7</span> Examining residuals for the source of differences</a></li>
<li><a href="5-8-section5-8.html#section5-8"><span class="toc-section-number">5.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li><a href="5-9-section5-9.html#section5-9"><span class="toc-section-number">5.9</span> Political party and voting results: Complete analysis</a></li>
<li><a href="5-10-section5-10.html#section5-10"><span class="toc-section-number">5.10</span> Is cheating and lying related in students?</a></li>
<li><a href="5-11-section5-11.html#section5-11"><span class="toc-section-number">5.11</span> Analyzing a stratified random sample of California schools</a></li>
<li><a href="5-12-section5-12.html#section5-12"><span class="toc-section-number">5.12</span> Chapter summary</a></li>
<li><a href="5-13-section5-13.html#section5-13"><span class="toc-section-number">5.13</span> Summary of important R commands</a></li>
<li><a href="5-14-section5-14.html#section5-14"><span class="toc-section-number">5.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="6-chapter6.html#chapter6"><span class="toc-section-number">6</span> Correlation and Simple Linear Regression</a><ul>
<li><a href="6-1-section6-1.html#section6-1"><span class="toc-section-number">6.1</span> Relationships between two quantitative variables</a></li>
<li><a href="6-2-section6-2.html#section6-2"><span class="toc-section-number">6.2</span> Estimating the correlation coefficient</a></li>
<li><a href="6-3-section6-3.html#section6-3"><span class="toc-section-number">6.3</span> Relationships between variables by groups</a></li>
<li><a href="6-4-section6-4.html#section6-4"><span class="toc-section-number">6.4</span> Inference for the correlation coefficient</a></li>
<li><a href="6-5-section6-5.html#section6-5"><span class="toc-section-number">6.5</span> Are tree diameters related to tree heights?</a></li>
<li><a href="6-6-section6-6.html#section6-6"><span class="toc-section-number">6.6</span> Describing relationships with a regression model</a></li>
<li><a href="6-7-section6-7.html#section6-7"><span class="toc-section-number">6.7</span> Least Squares Estimation</a></li>
<li><a href="6-8-section6-8.html#section6-8"><span class="toc-section-number">6.8</span> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li><a href="6-9-section6-9.html#section6-9"><span class="toc-section-number">6.9</span> Outliers: leverage and influence</a></li>
<li><a href="6-10-section6-10.html#section6-10"><span class="toc-section-number">6.10</span> Residual diagnostics – setting the stage for inference</a></li>
<li><a href="6-11-section6-11.html#section6-11"><span class="toc-section-number">6.11</span> Old Faithful discharge and waiting times</a></li>
<li><a href="6-12-section6-12.html#section6-12"><span class="toc-section-number">6.12</span> Chapter summary</a></li>
<li><a href="6-13-section6-13.html#section6-13"><span class="toc-section-number">6.13</span> Summary of important R code</a></li>
<li><a href="6-14-section6-14.html#section6-14"><span class="toc-section-number">6.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="7-chapter7.html#chapter7"><span class="toc-section-number">7</span> Simple linear regression inference</a><ul>
<li><a href="7-1-section7-1.html#section7-1"><span class="toc-section-number">7.1</span> Model</a></li>
<li><a href="7-2-section7-2.html#section7-2"><span class="toc-section-number">7.2</span> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li><a href="7-3-section7-3.html#section7-3"><span class="toc-section-number">7.3</span> Bozeman temperature trend</a></li>
<li><a href="7-4-section7-4.html#section7-4"><span class="toc-section-number">7.4</span> Randomization-based inferences for the slope coefficient</a></li>
<li><a href="7-5-section7-5.html#section7-5"><span class="toc-section-number">7.5</span> Transformations part I: Linearizing relationships</a></li>
<li><a href="7-6-section7-6.html#section7-6"><span class="toc-section-number">7.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li><a href="7-7-section7-7.html#section7-7"><span class="toc-section-number">7.7</span> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li><a href="7-8-section7-8.html#section7-8"><span class="toc-section-number">7.8</span> Chapter summary</a></li>
<li><a href="7-9-section7-9.html#section7-9"><span class="toc-section-number">7.9</span> Summary of important R code</a></li>
<li><a href="7-10-section7-10.html#section7-10"><span class="toc-section-number">7.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="8-chapter8.html#chapter8"><span class="toc-section-number">8</span> Multiple linear regression</a><ul>
<li><a href="8-1-section8-1.html#section8-1"><span class="toc-section-number">8.1</span> Going from SLR to MLR</a></li>
<li><a href="8-2-section8-2.html#section8-2"><span class="toc-section-number">8.2</span> Validity conditions in MLR</a></li>
<li><a href="8-3-section8-3.html#section8-3"><span class="toc-section-number">8.3</span> Interpretation of MLR terms</a></li>
<li><a href="8-4-section8-4.html#section8-4"><span class="toc-section-number">8.4</span> Comparing multiple regression models</a></li>
<li><a href="8-5-section8-5.html#section8-5"><span class="toc-section-number">8.5</span> General recommendations for MLR interpretations and VIFs</a></li>
<li><a href="8-6-section8-6.html#section8-6"><span class="toc-section-number">8.6</span> MLR inference: Parameter inferences using the t-distribution</a></li>
<li><a href="8-7-section8-7.html#section8-7"><span class="toc-section-number">8.7</span> Overall F-test in multiple linear regression</a></li>
<li><a href="8-8-section8-8.html#section8-8"><span class="toc-section-number">8.8</span> Case study: First year college GPA and SATs</a></li>
<li><a href="8-9-section8-9.html#section8-9"><span class="toc-section-number">8.9</span> Different intercepts for different groups: MLR with indicator variables</a></li>
<li><a href="8-10-section8-10.html#section8-10"><span class="toc-section-number">8.10</span> Additive MLR with more than two groups: Headache example</a></li>
<li><a href="8-11-section8-11.html#section8-11"><span class="toc-section-number">8.11</span> Different slopes and different intercepts</a></li>
<li><a href="8-12-section8-12.html#section8-12"><span class="toc-section-number">8.12</span> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li><a href="8-13-section8-13.html#section8-13"><span class="toc-section-number">8.13</span> AICs for model selection</a></li>
<li><a href="8-14-section8-14.html#section8-14"><span class="toc-section-number">8.14</span> Case study: Forced expiratory volume model selection using AICs</a></li>
<li><a href="8-15-section8-15.html#section8-15"><span class="toc-section-number">8.15</span> Chapter summary</a></li>
<li><a href="8-16-section8-16.html#section8-16"><span class="toc-section-number">8.16</span> Summary of important R code</a></li>
<li><a href="8-17-section8-17.html#section8-17"><span class="toc-section-number">8.17</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="9-chapter9.html#chapter9"><span class="toc-section-number">9</span> Case studies</a><ul>
<li><a href="9-1-section9-1.html#section9-1"><span class="toc-section-number">9.1</span> Overview of material covered</a></li>
<li><a href="9-2-section9-2.html#section9-2"><span class="toc-section-number">9.2</span> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li><a href="9-3-section9-3.html#section9-3"><span class="toc-section-number">9.3</span> Ants learn to rely on more informative attributes during decision-making</a></li>
<li><a href="9-4-section9-4.html#section9-4"><span class="toc-section-number">9.4</span> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li><a href="9-5-section9-5.html#section9-5"><span class="toc-section-number">9.5</span> What do didgeridoos really do about sleepiness?</a></li>
<li><a href="9-6-section9-6.html#section9-6"><span class="toc-section-number">9.6</span> General summary</a></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="section2-3" class="section level2">
<h2><span class="header-section-number">2.3</span> Models, hypotheses, and permutations for the two sample mean situation</h2>

<p>There appears to be some evidence that the <em>casual</em> clothing group is
getting higher average overtake distances than
the <em>commute</em> group of observations, but we want to try to make sure that the difference is
real – that there is evidence to reject the assumption that the means
are the same “in the population”. First, a <strong><em>null hypothesis</em></strong><a href="#fn22" class="footnote-ref" id="fnref22"><sup>22</sup></a> which
defines a <strong><em>null model</em></strong><a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a>

needs to be determined in terms of <strong><em>parameters</em></strong> (the true values in
the population). The research question should help you determine the form of the
hypotheses for the assumed population. In the two independent sample mean
problem, the interest is in testing a null hypothesis of <span class="math inline">\(H_0: \mu_1 = \mu_2\)</span>
versus the alternative hypothesis of <span class="math inline">\(H_A: \mu_1 \ne \mu_2\)</span>, where
<span class="math inline">\(\mu_1\)</span> is the parameter for the true mean of the first group and <span class="math inline">\(\mu_2\)</span>
is the parameter for the true mean of the second group. The alternative
hypothesis involves assuming a statistical model

for the <span class="math inline">\(i^{th}\ (i=1,\ldots,n_j)\)</span>
response from the <span class="math inline">\(j^{th}\ (j=1,2)\)</span> group, <span class="math inline">\(\boldsymbol{y}_{ij}\)</span>, that
involves modeling it as <span class="math inline">\(y_{ij} = \mu_j + \varepsilon_{ij}\)</span>,
where we assume that <span class="math inline">\(\varepsilon_{ij} \sim N(0,\sigma^2)\)</span>. For the moment,
focus on the models that either assume the means are the same (null) or
different (alternative),

which imply:</p>
<ul>
<li><p>Null Model: <span class="math inline">\(y_{ij} = \mu + \varepsilon_{ij}\)</span> There is <strong>no</strong>
difference in <strong>true</strong> means for the two groups.</p></li>
<li><p>Alternative Model: <span class="math inline">\(y_{ij} = \mu_j + \varepsilon_{ij}\)</span> There is <strong>a</strong>
difference in <strong>true</strong> means for the two groups.</p></li>
</ul>
<p>Suppose we are considering the alternative model for the 4<sup>th</sup>
observation (<span class="math inline">\(i=4\)</span>) from the second group (<span class="math inline">\(j=2\)</span>), then the model for
this observation is <span class="math inline">\(y_{42} = \mu_2 +\varepsilon_{42}\)</span>, that defines the
response as coming from the true mean for the second group plus a
random error term for that observation, <span class="math inline">\(\varepsilon_{42}\)</span>. For, say, the
5<sup>th</sup> observation from the first group (<span class="math inline">\(j=1\)</span>), the model is
<span class="math inline">\(y_{51} = \mu_1 +\varepsilon_{51}\)</span>. If we were working with the null model,
the mean is always the same (<span class="math inline">\(\mu\)</span>) – the group specified does not change
the mean we use for that observation, so the model for <span class="math inline">\(y_{42}\)</span> would be <span class="math inline">\(\mu +\varepsilon_{42}\)</span>.</p>
<p>It can be helpful to think about the null and alternative models graphically.

By assuming the null hypothesis is true (means are equal) and that the random
errors around the mean follow a normal distribution,

we assume that the truth
is as displayed in the left panel of Figure <a href="2-3-section2-3.html#fig:Figure2-7">2.7</a> – two
normal distributions with the same mean and variability. The alternative
model allows the two groups to potentially have different means, such as
those displayed in the right panel of Figure <a href="2-3-section2-3.html#fig:Figure2-7">2.7</a> where the
second group has a larger mean. Note that in this scenario, we assume that
the observations all came from the same distribution except that they had
different means. Depending on the statistical procedure we are using, we
basically are going to assume that the observations (<span class="math inline">\(y_{ij}\)</span>) either were
generated as samples from the null or alternative model. You can imagine
drawing observations at random from the pictured distributions. For hypothesis
testing, the null model

is assumed to be true and then the unusualness of
the actual result is assessed relative to that assumption. In hypothesis
testing, we have to decide if we have enough evidence to reject the assumption
that the null model (or hypothesis) is true. If we reject the null hypothesis,
then we would conclude that the other model considered (the alternative
model)

is more reasonable. The researchers obviously would have hoped to
encounter some sort of noticeable difference in the distances for the
different outfits and have been able to find enough evidence to reject the null
model where the groups “look the same”.</p>

<div class="figure"><span id="fig:Figure2-7"></span>
<img src="chapter2_files/image015.png" alt="Illustration of the assumed situations under the null (left) and a single possibility that could occur if the alternative were true (right) and the true means were different. There are an infinite number of ways to make a plot like the right panel that satisfies the alternative hypothesis." width="480" />
<p class="caption">
Figure 2.7: Illustration of the assumed situations under the null (left) and a single possibility that could occur if the alternative were true (right) and the true means were different. There are an infinite number of ways to make a plot like the right panel that satisfies the alternative hypothesis.
</p>
</div>
<p>In statistical inference, null hypotheses (and their
implied models) are set
up as “straw men” with every interest in rejecting them even though we assume
they are true to be able to assess the evidence <span class="math inline">\(\underline{\text{against them}}\)</span>.
Consider the original study design here, the outfits were randomly assigned to
the rides. If the null hypothesis were true, then we would have no difference
in the population means of the groups. And this would apply if we had done a
different random assignment  of the outfits. So let’s try this:
assume that the null hypothesis is true and randomly re-assign the treatments
(outfits) to the observations that were obtained. In other words, keep the
<em>Distance</em> results the same and shuffle the group labels randomly. The
technical term for this is doing a <strong><em>permutation</em></strong>  (a random shuffling of
a grouping variable relative to the observed responses). If the null is true
and the means
in the two groups are the same, then we should be able to re-shuffle the
groups to the observed <em>Distance</em> values and get results similar to those we
actually observed. If the null is false and the means are really different in
the two groups, then what we observed should differ from what we get under
other random permutations. The differences between the two groups should be
more noticeable in the observed data set than in (most) of the shuffled data
sets. It helps to see an example of a permutation of the labels to understand
what this means here.</p>
<p>The data set we are working with is a little on the large size, especially to explore individual observations. So for the moment we are going to work with a random sample of 30 of the <span class="math inline">\(n=1,636\)</span> observations in <code>ddsub</code>, fifteen from each group, that are generated using the <code>sample</code> function. To do this<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a>, we will use the <code>sample</code> function  twice - once to sample from the subsetted <em>commute</em> observations (creating the <code>s1</code> data set) and once to sample from the <em>casual</em> ones (creating <code>s2</code>). A new function for us, called <code>rbind</code> , is used to bind the rows together — much like pasting a chunk of rows below another chunk in a spreadsheet program. This operation only works if the columns all have the same names and meanings both for <code>rbind</code> and in a spreadsheet. Together this code creates the <code>dsample</code> data set that we will analyze below and compare to results from the full data set. The sample means are now 135.8 and 109.87 cm for <em>casual</em> and <em>commute</em> groups, respectively, and so the difference in the sample means has increased in magnitude to -25.93 cm (commute - casual). This difference would vary based on the different random samples from the larger data set, but for the moment pretend this was the entire data set that the researchers had collected and that we want to try to find evidence against the null hypothesis that the true means are the same in these two groups.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb63-1" title="1"><span class="kw">set.seed</span>(<span class="dv">9432</span>)</a>
<a class="sourceLine" id="cb63-2" title="2">s1 &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">subset</span>(ddsub, Condition <span class="op">%in%</span><span class="st"> &quot;commute&quot;</span>), <span class="dt">size=</span><span class="dv">15</span>)</a>
<a class="sourceLine" id="cb63-3" title="3">s2 &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">subset</span>(ddsub, Condition <span class="op">%in%</span><span class="st"> &quot;casual&quot;</span>), <span class="dt">size=</span><span class="dv">15</span>)</a>
<a class="sourceLine" id="cb63-4" title="4">dsample &lt;-<span class="st"> </span><span class="kw">rbind</span>(s1, s2)</a>
<a class="sourceLine" id="cb63-5" title="5"><span class="kw">mean</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>dsample)</a></code></pre></div>
<pre><code>##   casual  commute 
## 135.8000 109.8667</code></pre>
<p>In order to assess evidence against the null hypothesis of no difference, we want to permute the group labels versus the observations. In the <code>mosaic</code> package, the <code>shuffle</code> function allows us to easily perform
a  permutation<a href="#fn25" class="footnote-ref" id="fnref25"><sup>25</sup></a>. One permutation of the
treatment labels is provided in the <code>PermutedCondition</code> variable below. Note
that the <code>Distances</code> are held in the same place while the group labels are shuffled.</p>

<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb65-1" title="1">Perm1 &lt;-<span class="st"> </span><span class="kw">with</span>(dsample, <span class="kw">tibble</span>(Distance, Condition, <span class="dt">PermutedCondition=</span><span class="kw">shuffle</span>(Condition)))</a>
<a class="sourceLine" id="cb65-2" title="2"><span class="co">#To force the tibble to print out all rows in data set - not used often</span></a>
<a class="sourceLine" id="cb65-3" title="3"><span class="kw">data.frame</span>(Perm1) </a></code></pre></div>
<pre><code>##    Distance Condition PermutedCondition
## 1       168   commute           commute
## 2       137   commute           commute
## 3        80   commute            casual
## 4       107   commute           commute
## 5       104   commute            casual
## 6        60   commute            casual
## 7        88   commute           commute
## 8       126   commute           commute
## 9       115   commute            casual
## 10      120   commute            casual
## 11      146   commute           commute
## 12      113   commute            casual
## 13       89   commute           commute
## 14       77   commute           commute
## 15      118   commute            casual
## 16      148    casual            casual
## 17      114    casual            casual
## 18      124    casual           commute
## 19      115    casual            casual
## 20      102    casual            casual
## 21       77    casual            casual
## 22       72    casual           commute
## 23      193    casual           commute
## 24      111    casual           commute
## 25      161    casual            casual
## 26      208    casual           commute
## 27      179    casual            casual
## 28      143    casual           commute
## 29      144    casual           commute
## 30      146    casual            casual</code></pre>
<p>If you count up the number of subjects in each group by counting the number
of times each label (commute, casual) occurs, it is the same in both the
<code>Condition</code> and <code>PermutedCondition</code> columns (15 each). Permutations involve randomly
re-ordering the values of a variable – here the <code>Condition</code> group labels – without
changing the content of the variable.

This result can also be generated using
what is called <strong><em>sampling without replacement</em></strong>:  sequentially select <span class="math inline">\(n\)</span> labels
from the original variable (<em>Condition</em>), removing each observed label and making sure that each of the
original <code>Condition</code> labels is selected once and only once. The new, randomly
selected order of selected labels provides the permuted labels. Stepping
through the process helps to understand how it works: after the initial random
sample of one label, there would <span class="math inline">\(n - 1\)</span> choices possible; on the <span class="math inline">\(n^{th}\)</span>
selection, there would only be one label remaining to select. This makes sure
that all original labels are re-used but that the order is random. Sampling
without replacement is like picking names out of a hat, one-at-a-time, and not
putting the names back in after they are selected. It is an exhaustive process
for all the original observations. <strong><em>Sampling with replacement</em></strong>,  in contrast,
involves sampling from the specified list with each observation having an equal
chance of selection for each sampled observation – in other words, observations
can be selected more than once. This is like picking <span class="math inline">\(n\)</span> names out of a hat that
contains <span class="math inline">\(n\)</span> names, except that every time a name is selected, it goes back into
the hat – we’ll use this technique in Section <a href="2-8-section2-8.html#section2-8">2.8</a>
to do what is called <strong><em>bootstrapping</em></strong>.

Both sampling mechanisms can be
used to generate inferences but each has particular situations
where they are most useful. For hypothesis testing,

we will use permutations 
(sampling without replacement) as its mechanism most closely matches the null hypotheses we will be testing.</p>
<p>The comparison of the pirate-plots  between the real <span class="math inline">\(n=30\)</span> data set and permuted version is what is really interesting (Figure <a href="2-3-section2-3.html#fig:Figure2-8">2.8</a>). The
original difference in the sample means of the two groups was -25.93 cm (<em>commute</em> - <em>casual</em>). The sample means are the <strong><em>statistics</em></strong>
that estimate the parameters for the true means of the two groups and the difference in the sample means is a way to create a single number that tracks a quantity directly related to the difference between the null and alternative models. In the
permuted data set, the difference in the means is 12.07 cm in the opposite
direction (the <em>commute</em> group had a higher mean than <em>casual</em> in the permuted data).</p>

<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb67-1" title="1"><span class="kw">mean</span>(Distance<span class="op">~</span>PermutedCondition, <span class="dt">data=</span>Perm1)</a></code></pre></div>
<pre><code>##   casual  commute 
## 116.8000 128.8667</code></pre>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb69-1" title="1"><span class="kw">diffmean</span>(Distance<span class="op">~</span>PermutedCondition, <span class="dt">data=</span>Perm1)</a></code></pre></div>
<pre><code>## diffmean 
## 12.06667</code></pre>

<div class="figure"><span id="fig:Figure2-8"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-8-1.png" alt="Pirate-plots of Distance responses versus actual treatment groups and permuted groups. Note how the responses are the same but that they are shuffled between the two groups differently in the permuted data set. With the smaller sample size, the 95% confidence intervals are more clearly visible than with the original large data set." width="672" />
<p class="caption">
Figure 2.8: Pirate-plots of Distance responses versus actual treatment groups and permuted groups. Note how the responses are the same but that they are shuffled between the two groups differently in the permuted data set. With the smaller sample size, the 95% confidence intervals are more clearly visible than with the original large data set.
</p>
</div>
<p>The <code>diffmean</code> function is a simple way to get the differences in the means, but we can also start to learn about using the <code>lm</code>  function - that will be used for every chapter except for Chapter <a href="5-chapter5.html#chapter5"><strong>??</strong></a>. The <code>lm</code> stands for <strong><em>linear model</em></strong>  and, as we will see moving forward, encompasses a wide array of different models and scenarios. Here we will consider among its simplest usage<a href="#fn26" class="footnote-ref" id="fnref26"><sup>26</sup></a> to be able to estimate the difference in the mean of two groups. Notationally, it is very similar to other functions we have considered, <code>lm(y ~ x, data=...)</code> where <code>y</code> is the response variable and <code>x</code> is the explanatory variable. Here that is <code>lm(Distance~Condition, data=dsample)</code> with <code>Condition</code> defined as a factor variable. With linear models, we will need to interrogate them to obtain a variety of useful information and our first “interrogation” function is usually the <code>summary</code> function. To use it, it is best to have stored the model into an object, something like <code>lm1</code>, and then we can apply the <code>summary()</code>  function to the stored model object to get a suite of output:</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb71-1" title="1">lm1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span>Condition, <span class="dt">data=</span>dsample)</a>
<a class="sourceLine" id="cb71-2" title="2"><span class="kw">summary</span>(lm1)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Distance ~ Condition, data = dsample)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -63.800 -21.850   4.133  15.150  72.200 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)       135.800      8.863  15.322 3.83e-15
## Conditioncommute  -25.933     12.534  -2.069   0.0479
## 
## Residual standard error: 34.33 on 28 degrees of freedom
## Multiple R-squared:  0.1326, Adjusted R-squared:  0.1016 
## F-statistic: 4.281 on 1 and 28 DF,  p-value: 0.04789</code></pre>
<p>This output is explored more in Chapter <a href="3-chapter3.html#chapter3"><strong>??</strong></a>, but for the moment, focus on the row labeled as <code>Conditioncommute</code> in the middle of the output. In the first (<code>Estimate</code>) column, there is -25.933. This is a number we saw before – it is the difference in the sample means between <em>commute</em> and <em>casual</em> (<em>commute</em> - <em>casual</em>). When <code>lm</code> denotes a category in the row of the output (here <code>commute</code>), it is trying to indicate that the information to follow relates to the difference between this category and a baseline or reference category (here <code>casual</code>). The first (<code>(Intercept)</code>) row also contains a number we have seen before: - 135.8 is the sample mean for the <em>casual</em> group. So the <code>lm</code> is generating a coefficient for the mean of one of the groups and another as the difference in the two groups<a href="#fn27" class="footnote-ref" id="fnref27"><sup>27</sup></a>. In developing a test to assess evidence against the null hypothesis, we will focus on the difference in the sample means. So we want to be able to extract that number from this large suite of information. It ends up that we can apply the <code>coef</code>  function to <code>lm</code> models and then access that second coefficient using the bracket notation. Specifically:</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb73-1" title="1"><span class="kw">coef</span>(lm1)[<span class="dv">2</span>]</a></code></pre></div>
<pre><code>## Conditioncommute 
##        -25.93333</code></pre>
<p>This is the same result as using the <code>diffmean</code> function, so either could be used here. The estimated difference in the sample means in the permuted data set of 12.07 cm is available with:</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb75-1" title="1">lmP &lt;-<span class="st"> </span><span class="kw">lm</span>(Distance<span class="op">~</span>PermutedCondition, <span class="dt">data=</span>Perm1)</a>
<a class="sourceLine" id="cb75-2" title="2"><span class="kw">coef</span>(lmP)[<span class="dv">2</span>]</a></code></pre></div>
<pre><code>## PermutedConditioncommute 
##                 12.06667</code></pre>
<p>Comparing the pirate-plots and the estimated difference in the sample means suggests that the observed difference was larger than what we got
when we did a single permutation . Conceptually, permuting observations between group labels is
consistent with the null hypothesis – this is a technique to generate results
that we might have gotten if the null hypothesis were true since the responses
are the same in the two groups if the null is true. We just need to repeat the
permutation process many times and track how unusual our observed result is
relative to this distribution of potential responses if the null were true.
If the observed differences are unusual relative to the results under
permutations, then there is evidence against the null hypothesis, and we can conclude,
in the direction of the alternative hypothesis, that the
true means differ. If the observed differences are similar to (or at least not
unusual relative to) what we get under random shuffling under the null model,
we would have a tough time concluding that there is any real difference between
the groups based on our observed data set. This is formalized using the <strong><em>p-value</em></strong> as a measure of the strength of evidence against the null hypothesis and how we use it.</p>
</div>
<div class="footnotes">
<hr />
<ol start="22">
<li id="fn22"><p>The
hypothesis of no difference that is typically generated in the hopes of
being rejected in favor of the alternative hypothesis, which contains the sort
of difference that is of interest in the application.<a href="2-3-section2-3.html#fnref22" class="footnote-back">↩</a></p></li>
<li id="fn23"><p>The null model is the statistical model that
is implied by the chosen null hypothesis. Here, a null hypothesis of no
difference translates to having a model with the same mean for both groups.<a href="2-3-section2-3.html#fnref23" class="footnote-back">↩</a></p></li>
<li id="fn24"><p>While note required, we often set our random number seed using the <code>set.seed</code> function so that when we re-run code with randomization in it we get the same results. <a href="2-3-section2-3.html#fnref24" class="footnote-back">↩</a></p></li>
<li id="fn25"><p>We’ll see the <code>shuffle</code> function in a more common usage below;
while the code to generate <code>Perm1</code> is provided, it isn’t something to worry
about right now.<a href="2-3-section2-3.html#fnref25" class="footnote-back">↩</a></p></li>
<li id="fn26"><p>This is a bit like getting a new convertible sports car and driving it to the grocery store - there might be better ways to get groceries, but we want to drive our new car as soon as we get it.<a href="2-3-section2-3.html#fnref26" class="footnote-back">↩</a></p></li>
<li id="fn27"><p>This will be formalized and explained more in the next chapter when we encounter more than two groups in these same models. For now, it is recommended to start with the sample means from <code>favstats</code> for the two groups and then use that to sort out which direction the differencing was done in the <code>lm</code> output.<a href="2-3-section2-3.html#fnref27" class="footnote-back">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="2-2-section2-2.html"><button class="btn btn-default">Previous</button></a>
<a href="2-4-section2-4.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
