<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="1.5 Hypothesis testing (general) | Intermediate Statistics with R" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="gpeterson406/Greenwood_Book" />

<meta name="author" content="Mark C Greenwood" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="1.5 Hypothesis testing (general) | Intermediate Statistics with R">

<title>1.5 Hypothesis testing (general) | Intermediate Statistics with R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li class="has-sub"><a href="acknowledgments.html#acknowledgments">Acknowledgments</a><ul>
<li><a href="0-1-section1-5.html#section1-5"><span class="toc-section-number">0.1</span> Summary of important R code</a></li>
</ul></li>
<li class="has-sub"><a href="1-chapter2.html#chapter2"><span class="toc-section-number">1</span> (R)e-Introduction to statistics</a><ul>
<li><a href="1-1-section2-1.html#section2-1"><span class="toc-section-number">1.1</span> Histograms, boxplots, and density curves</a></li>
<li><a href="1-2-section2-2.html#section2-2"><span class="toc-section-number">1.2</span> Pirate-plots</a></li>
<li><a href="1-3-section2-3.html#section2-3"><span class="toc-section-number">1.3</span> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li><a href="1-4-section2-4.html#section2-4"><span class="toc-section-number">1.4</span> Permutation testing for the two sample mean situation</a></li>
<li><a href="1-5-section2-5.html#section2-5"><span class="toc-section-number">1.5</span> Hypothesis testing (general)</a></li>
<li><a href="1-6-section2-6.html#section2-6"><span class="toc-section-number">1.6</span> Connecting randomization (nonparametric) and parametric tests</a></li>
<li><a href="1-7-section2-7.html#section2-7"><span class="toc-section-number">1.7</span> Second example of permutation tests</a></li>
<li><a href="1-8-section2-8.html#section2-8"><span class="toc-section-number">1.8</span> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li><a href="1-9-section2-9.html#section2-9"><span class="toc-section-number">1.9</span> Confidence intervals and bootstrapping</a></li>
<li><a href="1-10-section2-10.html#section2-10"><span class="toc-section-number">1.10</span> Bootstrap confidence intervals for difference in GPAs</a></li>
<li><a href="1-11-section2-11.html#section2-11"><span class="toc-section-number">1.11</span> Chapter summary</a></li>
<li><a href="1-12-section2-12.html#section2-12"><span class="toc-section-number">1.12</span> Summary of important R code</a></li>
<li><a href="1-13-section2-13.html#section2-13"><span class="toc-section-number">1.13</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="2-chapter3.html#chapter3"><span class="toc-section-number">2</span> One-Way ANOVA</a><ul>
<li><a href="2-1-section3-1.html#section3-1"><span class="toc-section-number">2.1</span> Situation</a></li>
<li><a href="2-2-section3-2.html#section3-2"><span class="toc-section-number">2.2</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li><a href="2-3-section3-3.html#section3-3"><span class="toc-section-number">2.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li><a href="2-4-section3-4.html#section3-4"><span class="toc-section-number">2.4</span> ANOVA model diagnostics including QQ-plots</a></li>
<li><a href="2-5-section3-5.html#section3-5"><span class="toc-section-number">2.5</span> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li><a href="2-6-section3-6.html#section3-6"><span class="toc-section-number">2.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li><a href="2-7-section3-7.html#section3-7"><span class="toc-section-number">2.7</span> Pair-wise comparisons for the Overtake data</a></li>
<li><a href="2-8-section3-8.html#section3-8"><span class="toc-section-number">2.8</span> Chapter summary</a></li>
<li><a href="2-9-section3-9.html#section3-9"><span class="toc-section-number">2.9</span> Summary of important R code</a></li>
<li><a href="2-10-section3-10.html#section3-10"><span class="toc-section-number">2.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="3-chapter4.html#chapter4"><span class="toc-section-number">3</span> Two-Way ANOVA</a><ul>
<li><a href="3-1-section4-1.html#section4-1"><span class="toc-section-number">3.1</span> Situation</a></li>
<li><a href="3-2-section4-2.html#section4-2"><span class="toc-section-number">3.2</span> Designing a two-way experiment and visualizing results</a></li>
<li><a href="3-3-section4-3.html#section4-3"><span class="toc-section-number">3.3</span> Two-Way ANOVA models and hypothesis tests</a></li>
<li><a href="3-4-section4-4.html#section4-4"><span class="toc-section-number">3.4</span> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li><a href="3-5-section4-5.html#section4-5"><span class="toc-section-number">3.5</span> Observational study example: The Psychology of Debt</a></li>
<li><a href="3-6-section4-6.html#section4-6"><span class="toc-section-number">3.6</span> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li><a href="3-7-section4-7.html#section4-7"><span class="toc-section-number">3.7</span> Chapter summary</a></li>
<li><a href="3-8-section4-8.html#section4-8"><span class="toc-section-number">3.8</span> Summary of important R code</a></li>
<li><a href="3-9-section4-9.html#section4-9"><span class="toc-section-number">3.9</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="4-chapter5.html#chapter5"><span class="toc-section-number">4</span> Chi-square tests</a><ul>
<li><a href="4-1-section5-1.html#section5-1"><span class="toc-section-number">4.1</span> Situation, contingency tables, and tableplots</a></li>
<li><a href="4-2-section5-2.html#section5-2"><span class="toc-section-number">4.2</span> Homogeneity test hypotheses</a></li>
<li><a href="4-3-section5-3.html#section5-3"><span class="toc-section-number">4.3</span> Independence test hypotheses</a></li>
<li><a href="4-4-section5-4.html#section5-4"><span class="toc-section-number">4.4</span> Models for R by C tables</a></li>
<li><a href="4-5-section5-5.html#section5-5"><span class="toc-section-number">4.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="4-6-section5-6.html#section5-6"><span class="toc-section-number">4.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="4-7-section5-7.html#section5-7"><span class="toc-section-number">4.7</span> Examining residuals for the source of differences</a></li>
<li><a href="4-8-section5-8.html#section5-8"><span class="toc-section-number">4.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li><a href="4-9-section5-9.html#section5-9"><span class="toc-section-number">4.9</span> Political party and voting results: Complete analysis</a></li>
<li><a href="4-10-section5-10.html#section5-10"><span class="toc-section-number">4.10</span> Is cheating and lying related in students?</a></li>
<li><a href="4-11-section5-11.html#section5-11"><span class="toc-section-number">4.11</span> Analyzing a stratified random sample of California schools</a></li>
<li><a href="4-12-section5-12.html#section5-12"><span class="toc-section-number">4.12</span> Chapter summary</a></li>
<li><a href="4-13-section5-13.html#section5-13"><span class="toc-section-number">4.13</span> Summary of important R commands</a></li>
<li><a href="4-14-section5-14.html#section5-14"><span class="toc-section-number">4.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="5-chapter6.html#chapter6"><span class="toc-section-number">5</span> Correlation and Simple Linear Regression</a><ul>
<li><a href="5-1-section6-1.html#section6-1"><span class="toc-section-number">5.1</span> Relationships between two quantitative variables</a></li>
<li><a href="5-2-section6-2.html#section6-2"><span class="toc-section-number">5.2</span> Estimating the correlation coefficient</a></li>
<li><a href="5-3-section6-3.html#section6-3"><span class="toc-section-number">5.3</span> Relationships between variables by groups</a></li>
<li><a href="5-4-section6-4.html#section6-4"><span class="toc-section-number">5.4</span> Inference for the correlation coefficient</a></li>
<li><a href="5-5-section6-5.html#section6-5"><span class="toc-section-number">5.5</span> Are tree diameters related to tree heights?</a></li>
<li><a href="5-6-section6-6.html#section6-6"><span class="toc-section-number">5.6</span> Describing relationships with a regression model</a></li>
<li><a href="5-7-section6-7.html#section6-7"><span class="toc-section-number">5.7</span> Least Squares Estimation</a></li>
<li><a href="5-8-section6-8.html#section6-8"><span class="toc-section-number">5.8</span> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li><a href="5-9-section6-9.html#section6-9"><span class="toc-section-number">5.9</span> Outliers: leverage and influence</a></li>
<li><a href="5-10-section6-10.html#section6-10"><span class="toc-section-number">5.10</span> Residual diagnostics – setting the stage for inference</a></li>
<li><a href="5-11-section6-11.html#section6-11"><span class="toc-section-number">5.11</span> Old Faithful discharge and waiting times</a></li>
<li><a href="5-12-section6-12.html#section6-12"><span class="toc-section-number">5.12</span> Chapter summary</a></li>
<li><a href="5-13-section6-13.html#section6-13"><span class="toc-section-number">5.13</span> Summary of important R code</a></li>
<li><a href="5-14-section6-14.html#section6-14"><span class="toc-section-number">5.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="6-chapter7.html#chapter7"><span class="toc-section-number">6</span> Simple linear regression inference</a><ul>
<li><a href="6-1-section7-1.html#section7-1"><span class="toc-section-number">6.1</span> Model</a></li>
<li><a href="6-2-section7-2.html#section7-2"><span class="toc-section-number">6.2</span> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li><a href="6-3-section7-3.html#section7-3"><span class="toc-section-number">6.3</span> Bozeman temperature trend</a></li>
<li><a href="6-4-section7-4.html#section7-4"><span class="toc-section-number">6.4</span> Randomization-based inferences for the slope coefficient</a></li>
<li><a href="6-5-section7-5.html#section7-5"><span class="toc-section-number">6.5</span> Transformations part I: Linearizing relationships</a></li>
<li><a href="6-6-section7-6.html#section7-6"><span class="toc-section-number">6.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li><a href="6-7-section7-7.html#section7-7"><span class="toc-section-number">6.7</span> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li><a href="6-8-section7-8.html#section7-8"><span class="toc-section-number">6.8</span> Chapter summary</a></li>
<li><a href="6-9-section7-9.html#section7-9"><span class="toc-section-number">6.9</span> Summary of important R code</a></li>
<li><a href="6-10-section7-10.html#section7-10"><span class="toc-section-number">6.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="7-chapter8.html#chapter8"><span class="toc-section-number">7</span> Multiple linear regression</a><ul>
<li><a href="7-1-section8-1.html#section8-1"><span class="toc-section-number">7.1</span> Going from SLR to MLR</a></li>
<li><a href="7-2-section8-2.html#section8-2"><span class="toc-section-number">7.2</span> Validity conditions in MLR</a></li>
<li><a href="7-3-section8-3.html#section8-3"><span class="toc-section-number">7.3</span> Interpretation of MLR terms</a></li>
<li><a href="7-4-section8-4.html#section8-4"><span class="toc-section-number">7.4</span> Comparing multiple regression models</a></li>
<li><a href="7-5-section8-5.html#section8-5"><span class="toc-section-number">7.5</span> General recommendations for MLR interpretations and VIFs</a></li>
<li><a href="7-6-section8-6.html#section8-6"><span class="toc-section-number">7.6</span> MLR inference: Parameter inferences using the t-distribution</a></li>
<li><a href="7-7-section8-7.html#section8-7"><span class="toc-section-number">7.7</span> Overall F-test in multiple linear regression</a></li>
<li><a href="7-8-section8-8.html#section8-8"><span class="toc-section-number">7.8</span> Case study: First year college GPA and SATs</a></li>
<li><a href="7-9-section8-9.html#section8-9"><span class="toc-section-number">7.9</span> Different intercepts for different groups: MLR with indicator variables</a></li>
<li><a href="7-10-section8-10.html#section8-10"><span class="toc-section-number">7.10</span> Additive MLR with more than two groups: Headache example</a></li>
<li><a href="7-11-section8-11.html#section8-11"><span class="toc-section-number">7.11</span> Different slopes and different intercepts</a></li>
<li><a href="7-12-section8-12.html#section8-12"><span class="toc-section-number">7.12</span> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li><a href="7-13-section8-13.html#section8-13"><span class="toc-section-number">7.13</span> AICs for model selection</a></li>
<li><a href="7-14-section8-14.html#section8-14"><span class="toc-section-number">7.14</span> Case study: Forced expiratory volume model selection using AICs</a></li>
<li><a href="7-15-section8-15.html#section8-15"><span class="toc-section-number">7.15</span> Chapter summary</a></li>
<li><a href="7-16-section8-16.html#section8-16"><span class="toc-section-number">7.16</span> Summary of important R code</a></li>
<li><a href="7-17-section8-17.html#section8-17"><span class="toc-section-number">7.17</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="8-chapter9.html#chapter9"><span class="toc-section-number">8</span> Case studies</a><ul>
<li><a href="8-1-section9-1.html#section9-1"><span class="toc-section-number">8.1</span> Overview of material covered</a></li>
<li><a href="8-2-section9-2.html#section9-2"><span class="toc-section-number">8.2</span> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li><a href="8-3-section9-3.html#section9-3"><span class="toc-section-number">8.3</span> Ants learn to rely on more informative attributes during decision-making</a></li>
<li><a href="8-4-section9-4.html#section9-4"><span class="toc-section-number">8.4</span> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li><a href="8-5-section9-5.html#section9-5"><span class="toc-section-number">8.5</span> What do didgeridoos really do about sleepiness?</a></li>
<li><a href="8-6-section9-6.html#section9-6"><span class="toc-section-number">8.6</span> General summary</a></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="section2-5" class="section level2">
<h2><span class="header-section-number">1.5</span> Hypothesis testing (general)</h2>
<p>In hypothesis testing

(sometimes more explicitly called “Null Hypothesis
Significance Testing” or NHST), it is formulated to answer a specific question about
a population or true parameter(s) using a statistic based on a data set.
In your previous statistics course, you (hopefully) considered one-sample
hypotheses about population means and proportions and the two-sample mean
situation we are focused on here. Hypotheses relate to trying to answer
the question about whether the population mean overtake distances between the two
groups are different, with an initial assumption of no difference.</p>
<p>NHST is much like a criminal trial with a jury where you are in the role
of a jury member. Initially, the defendant
is assumed innocent. In our situation, the true means are assumed to be
equal between the groups. Then evidence is presented and, as a juror,
you analyze it. In statistical hypothesis testing,

data are collected and
analyzed. Then you have to decide if we had “enough” evidence to reject
the initial assumption (“innocence” that is initially assumed). To make
this decision, you want to have thought about and decided on the standard of
evidence required to reject the initial assumption. In criminal cases,
“beyond a reasonable doubt” is used. Wikipedia’s definition
(<a href="https://en.wikipedia.org/wiki/Reasonable_doubt" class="uri">https://en.wikipedia.org/wiki/Reasonable_doubt</a>) suggests that this
standard is that “there can still be a doubt, but only to the extent
that it would not affect a reasonable person’s belief regarding whether
or not the defendant is guilty”. In civil trials, a lower standard called
a “preponderance of evidence” is used. Based on that defined and pre-decided
(<em>a priori</em>) measure, you decide that the defendant is guilty or not guilty.
In statistics, the standard is set by choosing a significance level, <span class="math inline">\(\alpha\)</span>, and then you compare the p-value

to it. In this approach, if the p-value is less than
<span class="math inline">\(\alpha\)</span>, we reject the null hypothesis. The choice of the significance level
is like the variation in standards of evidence between criminal and civil
trials – and in all situations everyone should know the standards required
for rejecting the initial assumption before any information is “analyzed”.
Once someone is found guilty, then there is the matter of sentencing which
is related to the impacts (“size”) of the crime. In statistics, this is
similar to the estimated size of differences and the related judgments
about whether the differences are practically important or not. If the
crime is proven beyond a reasonable doubt but it is a minor crime, then
the sentence will be small. With the same level of evidence and a more
serious crime, the sentence will be more dramatic. This latter step is more critical than the p-value as it directly relates to actions to be taken based on the research but unfortunately p-values and the related decisions get most of the attention.</p>
<p>There are some important aspects of the testing process to note that inform
how we interpret statistical hypothesis test results. When someone is found
“not guilty”, it does not mean “innocent”, it just means that there was not
enough evidence to find the person guilty “beyond a reasonable doubt”.
Not finding enough evidence to reject the null hypothesis does not imply
that the true means are equal, just that there was not enough evidence to
conclude that they were different. There are many potential reasons why we
might fail to reject the null, but the most common one is that our sample
size was too small (which is related to having too little evidence). Other
reasons include simply the variation in taking a random sample  from the
population(s). This randomness in samples and the differences in the sample
means also implies that p-values are random and can easily vary if the data set
had been slightly different. This also relates to the suggestion of using a
graded interpretation of p-values instead of the fixed <span class="math inline">\(\alpha\)</span> usage – if the p-value is an estimated quantity,
is there really any difference between p-values of 0.049 and 0.051? We
probably shouldn’t think there is a big difference in results for these two
p-values even though the standard NHST reject/fail to reject the null approach
considers these as completely different results. So where does that leave us?
Interpret the p-values

using strength of evidence against the null hypothesis,
remembering that smaller (but not really small) p-values can still be
interesting. And if you think the p-value is small enough, then you can reject
the null hypothesis and conclude that the alternative hypothesis is a better characterization of the truth – and then estimate the size of the differences.</p>
<p>Throughout this material, we will continue to re-iterate the distinctions
between parameters and statistics and want you to be clear about the
distinctions between estimates based on the sample and inferences for the
population or true values of the parameters of interest. Remember that
statistics are summaries of the sample information and parameters are
characteristics of populations (which we rarely know). In the two-sample
mean situation, the sample means are always at least a little different
– that is not an interesting conclusion. What is interesting is whether
we have enough evidence to feel like we have proven that the population or true means
differ “beyond a reasonable doubt”.</p>
<p>The scope of any inferences is constrained based on whether there is a
<strong><em>random sample</em></strong> (RS) and/or <strong><em>random assignment</em></strong> (RA).   
Table <a href="1-5-section2-5.html#tab:Table2-2">1.1</a> contains the four possible combinations of these
two characteristics of a given study. Random assignment of treatment levels to
subjects allows for causal
inferences for differences that are observed – the difference in treatment
levels is said to cause differences in the mean responses. Random sampling (or
at least some sort of representative sample) allows inferences to be made to
the population of interest. If we do not have RA, then causal inferences cannot
be made. If we do not have a representative sample, then our inferences are
limited to the sampled subjects.
</p>


<table>
<caption><span id="tab:Table2-2">Table 1.1: </span> Scope of inference summary.</caption>
<colgroup>
<col width="30%" />
<col width="34%" />
<col width="34%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>Random<br />
Sampling/Random<br />
Assignment</strong></th>
<th align="left"><strong>Random Assignment (RA)<br />
– Yes (controlled
experiment)</strong></th>
<th align="left"><strong>Random Assignment (RA)<br />
– No (observational study)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Random Sampling (RS)<br />
– Yes (or some method<br />
that results in a<br />
representative sample of<br />
population of<br />
interest)</strong></td>
<td align="left">Because we have RS, we can<br />
generalize inferences to the<br />
population the RS was taken<br />
from. Because we have<br />
RA we can assume the groups<br />
were equivalent on all
aspects<br />
except for the treatment<br />
and can establish causal
inference.<br />
</td>
<td align="left">Can generalize inference to<br />
population the RS was taken<br />
from but cannot establish<br />
causal inference (no RA<br />
– cannot isolate treatment<br />
variable as only difference<br />
among groups, could be<br />
confounding variables).</td>
</tr>
<tr class="even">
<td align="left"><strong>Random Sampling (RS)<br />
– No (usually a<br />
convenience sample)</strong></td>
<td align="left">Cannot generalize inference
to<br />
the population of interest<br />
because the sample was<br />
not random and could be<br />
biased – may not be<br />
“representative” of the<br />
population of interest.<br />
Can establish causal<br />
inference due to RA
<span class="math inline">\(\rightarrow\)</span><br />
the inference from this type
of<br />
study applies only to the
sample.</td>
<td align="left">Cannot generalize inference
to<br />
the population of interest<br />
because the sample was<br />
not random and could be<br />
biased – may not be<br />
“representative” of the<br />
population of interest.<br />
Cannot establish causal<br />
inference due to lack of RA
of<br />
the treatment.</td>
</tr>
</tbody>
</table>

<p>A simple example helps to clarify how the scope of inference can change
based on the study design.  Suppose
we are interested in studying the GPA of students. If we had taken a random
sample from, say, Intermediate Statistics students in a given semester at a university, our scope of
inference would be the population of students in that semester taking that course. If we had
taken a random sample  from the entire population of students at that school, then the inferences would
be to the entire population of students in that semester. These are similar types of
problems but the two populations are very different and the group you are trying
to make conclusions about should be noted carefully in your results – it does
matter! If we did not have a representative sample, say the students could
choose to provide this information or not and some chose not to, then we can only make inferences to
volunteers. These volunteers might differ in systematic ways from the entire
population of Intermediate Statistics students (for example, they are proud of their GPA) so we cannot safely extend our inferences beyond the group that volunteered.</p>
<p>To consider the impacts of RA versus results from purely observational
studies, we need to be
comparing groups. Suppose that we are interested in differences in the mean
GPAs for different sections of Intermediate Statistics and that we take a random sample  of
students from each section and compare the results and find evidence of some
difference. In this scenario, we can conclude that there is some difference in
the population of these statistics students but we can’t say that being in different
sections caused the differences in the mean GPAs. Now suppose that we randomly
assigned every student to get extra training in one of three different
study techniques and found evidence of differences among the training methods.
We could conclude that the training methods caused the differences in these
students. These conclusions would only apply to Intermediate Statistics students at this university in this semester and could
not be generalized to a larger population of students. If we took a random
sample of Intermediate Statistics students (say only 10 from each section) and then randomly
assigned them to one of three training programs and found evidence of
differences, then we can say that the training programs caused the differences.
But we can also say that we have evidence that those differences pertain to the
population of Intermediate Statistics students in that semester at this university. This seems similar to the scenario where all
the students participated in the training programs except that by using random
sampling, only a fraction of the population needs to actually be studied to
make inferences to the entire population of interest – saving time and money.</p>
<p>A quick summary of the terminology of hypothesis testing

is useful at this
point. The <strong><em>null hypothesis</em></strong> (<span class="math inline">\(H_0\)</span>) states that there is no difference
or no relationship in the population. This is the statement of no effect or
no difference and the claim that we are trying to find evidence against in NHST. In
this chapter, <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mu_1=\mu_2\)</span>. When doing two-group problems, you
always need to specify which group is 1 and which one is 2 because the order
does matter. The <strong><em>alternative hypothesis</em></strong> (<span class="math inline">\(H_1\)</span> or <span class="math inline">\(H_A\)</span>) states a
specific difference between parameters. This is the research hypothesis and
the claim about the population that we often hope to demonstrate is more reasonable
to conclude than the null hypothesis. In the two-group situation, we can have
<strong><em>one-sided alternatives</em></strong> <span class="math inline">\(H_A: \mu_1 &gt; \mu_2\)</span> (greater than) or
<span class="math inline">\(H_A: \mu_1 &lt; \mu_2\)</span> (less than) or, the more common, <strong><em>two-sided
alternative</em></strong> <span class="math inline">\(H_A: \mu_1 \ne \mu_2\)</span> (not equal to). We usually default to
using two-sided tests because we often do not know enough to know the
direction of a difference <em>a priori</em>, especially in more complicated
situations. The <strong><em>sampling distribution under the null</em></strong> is the
distribution of all possible values of a statistic under the assumption that
<span class="math inline">\(H_0\)</span> is true. It
is used to calculate the <strong><em>p-value</em></strong>,

the probability of obtaining a
result as extreme or more extreme (defined by the alternative) than what we observed given that the null
hypothesis is true. We will find sampling distributions

using
<strong><em>nonparametric</em></strong>

approaches (like the permutation  approach used previously)
and <strong><em>parametric</em></strong>

methods (using “named” distributions like the
<span class="math inline">\(t\)</span>, F, and <span class="math inline">\(\chi^2\)</span>).</p>
<p>Small p-values are evidence against the null hypothesis

because the observed
result is unlikely due to chance if <span class="math inline">\(H_0\)</span> is true. Large p-values provide
little to no evidence against <span class="math inline">\(H_0\)</span> but do not allow us to conclude that the null
hypothesis is correct – just that we didn’t find enough evidence to think it
was wrong. The <strong><em>level of significance</em></strong> is an <em>a priori</em> definition of
how small the p-value needs to be to provide “enough” (sufficient) evidence
against <span class="math inline">\(H_0\)</span>. This is most useful to prevent sliding the standards after
the results are found but you can interpret p-values as strength of evidence against the null hypothesis without employing the fixed significance level. If using a fixed significance level, we can compare the p-value to the level of significance to
decide if the p-value is small enough to constitute sufficient evidence to
reject the null hypothesis. We use <span class="math inline">\(\alpha\)</span> to denote the level of
significance and most typically use 0.05 which we refer to as the 5%
significance level. We can compare the p-value to this level and make a
decision, focusing our interpretation on the strength of evidence we found
based on the p-value from very strong to little to none.
If we are using the strict version of NHST, the two options for <em>decisions</em> are
to either <em>reject the null hypothesis</em>
if the p-value <span class="math inline">\(\le \alpha\)</span> or <em>fail to reject the null hypothesis</em> if the
p-value <span class="math inline">\(&gt; \alpha\)</span>. When interpreting hypothesis testing results, remember
that the p-value is a measure of how unlikely the observed outcome was,
assuming that the null hypothesis is true. It is <strong>NOT</strong> the probability of
the data or the probability of either hypothesis being true. The p-value,
simply, is a measure of evidence against the null hypothesis.</p>
<p>Although we want to use graded evidence to interpret p-values, there
is one situation where thinking about comparisons to fixed <span class="math inline">\(\alpha\)</span> levels is
useful for understanding and studying statistical hypothesis testing. The
specific definition of <span class="math inline">\(\alpha\)</span> is that it is the probability of rejecting
<span class="math inline">\(H_0\)</span> when <span class="math inline">\(H_0\)</span> is true, the probability of what is called a
<strong><em>Type I error</em></strong>.  Type I errors are also called <strong><em>false rejections</em></strong> or
<strong><em>false detections</em></strong>. In the two-group mean situation, a Type I error would
be concluding that there
is a difference in the true means between the groups when none really exists
in the population. In the courtroom setting, this is like falsely finding
someone guilty. We don’t want to do this very often, so we use small values
of the significance level, allowing us to control the rate of Type I errors
at <span class="math inline">\(\alpha\)</span>.  We also have to worry about <strong>Type II errors</strong>,  which are failing
to reject the null hypothesis when it’s false. In a courtroom, this is the same
as failing to convict a truly guilty person. This most often occurs due to a
lack of evidence that could be due to a small sample size or merely just an
unusual sample from the population. You can use the Table <a href="1-5-section2-5.html#tab:Table2-3">1.2</a>
to help you remember all the possibilities.</p>
<p>
</p>
<p>(ref:tab2-3) Table of decisions and truth scenarios in a hypothesis
testing situation. But we never know the truth in a real situation.</p>
<table>
<caption><span id="tab:Table2-3">Table 1.2: </span> (ref:tab2-3)</caption>
<colgroup>
<col width="34%" />
<col width="32%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"> </th>
<th align="left"><span class="math inline">\(\mathbf{H_0}\)</span> <strong>True</strong></th>
<th align="left"><span class="math inline">\(\mathbf{H_0}\)</span> <strong>False</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>FTR</strong> <span class="math inline">\(\mathbf{H_0}\)</span></td>
<td align="left">Correct decision</td>
<td align="left">Type II error</td>
</tr>
<tr class="even">
<td align="left"><strong>Reject</strong> <span class="math inline">\(\mathbf{H_0}\)</span></td>
<td align="left">Type I error</td>
<td align="left">Correct decision</td>
</tr>
</tbody>
</table>
<p>In comparing different procedures or in planning studies, there is an
interest in studying the rate or
probability of Type I and II errors. The probability of a Type I error was
defined previously as <span class="math inline">\(\alpha\)</span>, the significance level. The <strong><em>power</em></strong> of a
procedure is the probability of rejecting the null hypothesis when it is false.   
Power is defined as</p>
<p><span class="math display">\[\text{Power} = 1 - \text{Probability(Type II error) } = 
\text{Probability(Reject } H_0 | H_0 \text{ is false),}\]</span></p>
<p>
</p>
<p>or, in words, the probability of detecting a difference when it actually
exists. We want to use a statistical procedure that controls the Type I error
rate at the pre-specified level and has high power to detect false null
hypotheses. Increasing the sample size is one of the most commonly used
methods for increasing the power  in a given situation. Sometimes we can choose
among different procedures and use the power of the procedures to help us make
that selection. Note that there are many ways <span class="math inline">\(H_0\)</span> can be false and the power changes
based on how false the null hypothesis actually is. To make this concrete,
suppose that the true mean overtake distances differed by either 1 or 30 cm in
previous example. The chances of rejecting the null hypothesis are much larger
when the groups actually differ by 30 cm than if they differ by just 1 cm,
given the same sample size. The null hypothesis is false in both cases. Similarly, for a given difference in the true means, the larger
the sample, the higher the power  of the study to actually find evidence of a
difference in the groups. We will see this difference when we return to using the entire overtake data set instead of the sample of <span class="math inline">\(n=30\)</span> used to illustrate the permutation procedures.</p>
<p>After making a decision (was there enough evidence to reject the null
or not), we want to make the conclusions specific to the problem of interest.
If we reject <span class="math inline">\(H_0\)</span>, then we can conclude that there was sufficient evidence at
the <span class="math inline">\(\alpha\)</span>-level that the null hypothesis is wrong (and the results point in
the direction of the alternative). If we fail to reject <span class="math inline">\(H_0\)</span> (FTR <span class="math inline">\(H_0\)</span>), then
we can conclude that there was insufficient evidence at the <span class="math inline">\(\alpha\)</span>-level to say
that the null hypothesis is wrong. We are <strong>NOT</strong> saying that the null is
correct and we <strong>NEVER</strong> accept the null hypothesis. We just failed to find
enough evidence to say it’s wrong. If we find sufficient evidence to reject the
null, then we need to revisit the method of data collection and design of the
study to discuss the scope of inference.  Can we discuss causality (due to RA) and/or
make inferences to a larger group than those in the sample (due to RS)?</p>
<p>To perform a hypothesis test, there are some steps to remember to
complete to make sure you have thought through and reported all aspects of the results.</p>
<table>
<colgroup>
<col width="100%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="left"><strong>Outline of 6+ steps to perform a Hypothesis Test</strong></td>
</tr>
<tr class="even">
<td align="left">Preliminary steps:</td>
</tr>
<tr class="odd">
<td align="left">* Define RQ and consider study design - what question can the data collected address?</td>
</tr>
<tr class="even">
<td align="left">* What graphs are appropriate to visualize the data?</td>
</tr>
<tr class="odd">
<td align="left">* What model/statistic (T) is needed to address RQ?</td>
</tr>
<tr class="even">
<td align="left">1. Write the null and alternative hypotheses,</td>
</tr>
<tr class="odd">
<td align="left">2. Plot the data and assess the “Validity Conditions” for the procedure being used (discussed below),</td>
</tr>
<tr class="even">
<td align="left">3. Find the value of the appropriate test statistic and p-value for your hypotheses, </td>
</tr>
<tr class="odd">
<td align="left">4. Write a conclusion specific to the problem based on the p-value, reporting the strength of evidence  against the null hypothesis (include test statistic, its distribution under the null hypothesis, and p-value).</td>
</tr>
<tr class="even">
<td align="left">5. Report and discuss an estimate of the size of the differences, with confidence interval(s) if appropriate. </td>
</tr>
<tr class="odd">
<td align="left">6. Scope of inference discussion for results. </td>
</tr>
</tbody>
</table>
</div>
<p style="text-align: center;">
<a href="1-4-section2-4.html"><button class="btn btn-default">Previous</button></a>
<a href="1-6-section2-6.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
